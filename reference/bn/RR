asm/alpha-mont.pl:# Copyright 2006-2020 The OpenSSL Project Authors. All Rights Reserved.
asm/alpha-mont.pl:# instructed to '-tune host' code with in-line assembler. Other
asm/alpha-mont.pl:# benchmarks improve by 15-20%. To anchor it to something else, the
asm/alpha-mont.pl:	lda	sp,-48(sp)
asm/alpha-mont.pl:	.mask	0x0400f000,-48
asm/alpha-mont.pl:	lda	AT,-4096(zero)	# mov	-4096,AT
asm/alpha-mont.pl:	stq	$lo1,-8($tp)
asm/alpha-mont.pl:	stq	$lo1,-8($tp)	#L1
asm/alpha-mont.pl:	subq	$lo0,$lo1,$lo1	# tp[i]-np[i]
asm/alpha-mont.pl:	stq	zero,-8($tp)	# zap tp
asm/alpha-mont.pl:	stq	$aj,-8($rp)
asm/armv4-gf2m.pl:# Copyright 2011-2020 The OpenSSL Project Authors. All Rights Reserved.
asm/armv4-gf2m.pl:# used in bn_gf2m.c. It's kind of low-hanging mechanical port from
asm/armv4-gf2m.pl:# in ~45 cycles on dual-issue core such as Cortex A8, which is ~50%
asm/armv4-gf2m.pl:# faster than compiler-generated code. For ECDH and ECDSA verify (but
asm/armv4-gf2m.pl:# not for ECDSA sign) it means 25%-45% improvement depending on key
asm/armv4-gf2m.pl:# by 18-40%.
asm/armv4-gf2m.pl:    ( $xlate="${dir}arm-xlate.pl" and -f $xlate ) or
asm/armv4-gf2m.pl:    ( $xlate="${dir}../../perlasm/arm-xlate.pl" and -f $xlate) or
asm/armv4-gf2m.pl:    die "can't locate arm-xlate.pl";
asm/armv4-gf2m.pl:.size	mul_1x1_ialu,.-mul_1x1_ialu
asm/armv4-gf2m.pl:	stmdb	sp!,{r4-r9}
asm/armv4-gf2m.pl:	stmdb	sp!,{r4-r10,lr}
asm/armv4-gf2m.pl:	and	r7,r7,#-32
asm/armv4-gf2m.pl:	ldmia	$ret,{@r[0]-@r[3]}
asm/armv4-gf2m.pl:	ldmia	sp!,{r4-r10,pc}
asm/armv4-gf2m.pl:	ldmia	sp!,{r4-r10,lr}
asm/armv4-gf2m.pl:	bx	lr			@ interoperable with Thumb ISA:-)
asm/armv4-gf2m.pl:.arch	armv7-a
asm/armv4-gf2m.pl:.size	bn_GF2m_mul_2x2,.-bn_GF2m_mul_2x2
asm/armv4-gf2m.pl:.word	OPENSSL_armcap_P-.
asm/armv4-gf2m.pl:	s/\bq([0-9]+)#(lo|hi)/sprintf "d%d",2*$1+($2 eq "hi")/geo	or
asm/armv4-gf2m.pl:	s/\bbx\s+lr\b/.word\t0xe12fff1e/go;    # make it possible to compile with -march=armv4
asm/armv4-mont.pl:# Copyright 2007-2020 The OpenSSL Project Authors. All Rights Reserved.
asm/armv4-mont.pl:# and compilers. The code was observed to provide +65-35% improvement
asm/armv4-mont.pl:# +115-80% on Intel IXP425. This is compared to pre-bn_mul_mont code
asm/armv4-mont.pl:# base and compiler generated code with in-lined umull and even umlal
asm/armv4-mont.pl:# performance improvement on Cortex-A8 is ~45-100% depending on key
asm/armv4-mont.pl:# length, more for longer keys. On Cortex-A15 the span is ~10-105%.
asm/armv4-mont.pl:# rather because original integer-only code seems to perform
asm/armv4-mont.pl:# suboptimally on S4. Situation on Cortex-A9 is unfortunately
asm/armv4-mont.pl:# of percent worse than for integer-only code. The code is chosen
asm/armv4-mont.pl:# for execution on all NEON-capable processors, because gain on
asm/armv4-mont.pl:# others outweighs the marginal loss on Cortex-A9.
asm/armv4-mont.pl:# Align Cortex-A9 performance with November 2013 improvements, i.e.
asm/armv4-mont.pl:# NEON code is now ~20-105% faster than integer-only one on this
asm/armv4-mont.pl:# on other processors: NEON code path is ~45-180% faster than original
asm/armv4-mont.pl:# integer-only on Cortex-A8, ~10-210% on Cortex-A15, ~70-450% on
asm/armv4-mont.pl:    ( $xlate="${dir}arm-xlate.pl" and -f $xlate ) or
asm/armv4-mont.pl:    ( $xlate="${dir}../../perlasm/arm-xlate.pl" and -f $xlate) or
asm/armv4-mont.pl:    die "can't locate arm-xlate.pl";
asm/armv4-mont.pl:$num="r0";	# starts as num argument, but holds &tp[num-1]
asm/armv4-mont.pl:#### argument block layout relative to &tp[num-1], a.k.a. $num
asm/armv4-mont.pl:.word	OPENSSL_armcap_P-.Lbn_mul_mont
asm/armv4-mont.pl:	stmdb	sp!,{r4-r12,lr}		@ save 10 registers
asm/armv4-mont.pl:	sub	$num,$num,#4		@ "num=num-1"
asm/armv4-mont.pl:	add	$tp,$bp,$num		@ &bp[num-1]
asm/armv4-mont.pl:	add	$num,sp,$num		@ $num to point at &tp[num-1]
asm/armv4-mont.pl:	str	$nlo,[$tp],#4		@ tp[j-1]=,tp++
asm/armv4-mont.pl:	str	$nlo,[$num]		@ tp[num-1]=
asm/armv4-mont.pl:	sub	$tj,$num,$tj		@ "original" $num-1 value
asm/armv4-mont.pl:	ldr	$aj,[$ap,#-4]		@ ap[0]
asm/armv4-mont.pl:	ldr	$nj,[$np,#-4]		@ np[0]
asm/armv4-mont.pl:	str	$nlo,[$tp],#4		@ tp[j-1]=,tp++
asm/armv4-mont.pl:	str	$nlo,[$num]		@ tp[num-1]=
asm/armv4-mont.pl:	sbcs	$tj,$tj,$nj		@ tp[j]-np[j]
asm/armv4-mont.pl:	ldmia	sp!,{r4-r12,lr}		@ restore registers
asm/armv4-mont.pl:	bx	lr			@ interoperable with Thumb ISA:-)
asm/armv4-mont.pl:.size	bn_mul_mont,.-bn_mul_mont
asm/armv4-mont.pl:.arch	armv7-a
asm/armv4-mont.pl:	stmdb	sp!,{r4-r11}
asm/armv4-mont.pl:	vstmdb	sp!,{d8-d15}		@ ABI specification says so
asm/armv4-mont.pl:	ldmia	ip,{r4-r5}		@ load rest of parameter block
asm/armv4-mont.pl:	vld1.32		{$A0-$A3},  [$aptr]!		@ can't specify :32 :-(
asm/armv4-mont.pl:	and		$toutptr,$toutptr,#-64
asm/armv4-mont.pl:	 vld1.32	{$N0-$N3}, [$nptr]!
asm/armv4-mont.pl:	 and		$toutptr,$toutptr,#-64
asm/armv4-mont.pl:	vst1.64		{@ACC[0]-@ACC[1]},[$toutptr,:256]!
asm/armv4-mont.pl:	vst1.64		{@ACC[2]-@ACC[3]},[$toutptr,:256]!
asm/armv4-mont.pl:	vst1.64		{@ACC[4]-@ACC[5]},[$toutptr,:256]!
asm/armv4-mont.pl:	vst1.64		{@ACC[6]-@ACC[7]},[$toutptr,:256]!
asm/armv4-mont.pl:	vld1.32		{$A0-$A3},[$aptr]!
asm/armv4-mont.pl:	vld1.32		{$N0-$N3},[$nptr]!
asm/armv4-mont.pl:	vld1.32		{$A0-$A3},[$aptr]!
asm/armv4-mont.pl:	vld1.32		{$N0-$N3},[$nptr]!
asm/armv4-mont.pl:	vld1.32		{$A0-$A3},[$aptr]!
asm/armv4-mont.pl:	vst1.64		{@ACC[0]-@ACC[1]},[$toutptr,:256]!
asm/armv4-mont.pl:	veor		q2,q2,q2		@ $N0-$N1
asm/armv4-mont.pl:	vst1.64		{@ACC[2]-@ACC[3]},[$toutptr,:256]!
asm/armv4-mont.pl:	veor		q3,q3,q3		@ $N2-$N3
asm/armv4-mont.pl:	vst1.64		{@ACC[4]-@ACC[5]},[$toutptr,:256]!
asm/armv4-mont.pl:	vld1.64		{@ACC[0]-@ACC[1]},[$tinptr,:256]!
asm/armv4-mont.pl:	vld1.64		{@ACC[2]-@ACC[3]},[$tinptr,:256]!
asm/armv4-mont.pl:	vld1.64		{@ACC[4]-@ACC[5]},[$tinptr,:256]!
asm/armv4-mont.pl:	vld1.64		{@ACC[6]-@ACC[7]},[$tinptr,:256]!
asm/armv4-mont.pl:	vst1.64		{q2-q3}, [sp,:256]!	@ start wiping stack frame
asm/armv4-mont.pl:	vst1.64		{q2-q3},[sp,:256]!
asm/armv4-mont.pl:	vst1.64		{q2-q3}, [sp,:256]!
asm/armv4-mont.pl:	vst1.64		{q2-q3}, [sp,:256]!
asm/armv4-mont.pl:	vld1.64		{@ACC[2]-@ACC[3]}, [$tinptr, :256]!
asm/armv4-mont.pl:	vld1.64		{@ACC[4]-@ACC[5]}, [$tinptr, :256]!
asm/armv4-mont.pl:	vld1.64		{@ACC[6]-@ACC[7]}, [$tinptr, :256]!
asm/armv4-mont.pl:	vld1.64		{@ACC[0]-@ACC[1]}, [$tinptr, :256]!
asm/armv4-mont.pl:	vst1.32	{${temp}[0]}, [$toutptr, :32]		@ top-most bit
asm/armv4-mont.pl:	ldmia	$aptr!, {r4-r7}
asm/armv4-mont.pl:	ldmia	$nptr!, {r8-r11}
asm/armv4-mont.pl:	stmia	$rptr!, {r8-r11}
asm/armv4-mont.pl:	ldr	r10, [$aptr]				@ load top-most bit
asm/armv4-mont.pl:	ldmia	$aptr!, {r4-r7}
asm/armv4-mont.pl:	ldmia	$rptr,  {r8-r11}
asm/armv4-mont.pl:	vst1.64	{q0-q1}, [$nptr,:256]!			@ wipe
asm/armv4-mont.pl:	vst1.64	{q0-q1}, [$nptr,:256]!			@ wipe
asm/armv4-mont.pl:	ldmia	$aptr, {r4-r7}
asm/armv4-mont.pl:	stmia	$rptr!, {r8-r11}
asm/armv4-mont.pl:	ldmia	$rptr, {r8-r11}
asm/armv4-mont.pl:	vst1.64	{q0-q1}, [$aptr,:256]!			@ wipe
asm/armv4-mont.pl:	vst1.64	{q0-q1}, [$nptr,:256]!			@ wipe
asm/armv4-mont.pl:	stmia	$rptr!, {r8-r11}
asm/armv4-mont.pl:        vldmia  sp!,{d8-d15}
asm/armv4-mont.pl:        ldmia   sp!,{r4-r11}
asm/armv4-mont.pl:.size	bn_mul8x_mont_neon,.-bn_mul8x_mont_neon
asm/armv4-mont.pl:	s/\bq([0-9]+)#(lo|hi)/sprintf "d%d",2*$1+($2 eq "hi")/ge	or
asm/armv4-mont.pl:	s/\bbx\s+lr\b/.word\t0xe12fff1e/g;	# make it possible to compile with -march=armv4
asm/armv8-mont.pl:# Copyright 2015-2021 The OpenSSL Project Authors. All Rights Reserved.
asm/armv8-mont.pl:# work. While it does improve RSA sign performance by 20-30% (less for
asm/armv8-mont.pl:# faster and RSA4096 goes 15-20% slower on Cortex-A57. Multiplication
asm/armv8-mont.pl:# compiler-generated code. Recall that compiler is instructed to use
asm/armv8-mont.pl:# RSA/DSA performance by 25-40-60% depending on processor and key
asm/armv8-mont.pl:# comparison to compiler-generated code. On Cortex-A57 improvement
asm/armv8-mont.pl:# 50-70% improvement for RSA4096 sign. RSA2048 sign is ~25% faster
asm/armv8-mont.pl:# on Cortex-A57 and ~60-100% faster on others.
asm/armv8-mont.pl:( $xlate="${dir}arm-xlate.pl" and -f $xlate ) or
asm/armv8-mont.pl:( $xlate="${dir}../../perlasm/arm-xlate.pl" and -f $xlate) or
asm/armv8-mont.pl:die "can't locate arm-xlate.pl";
asm/armv8-mont.pl:	stp	x29,x30,[sp,#-64]!
asm/armv8-mont.pl:	and	$tp,$tp,#-16		// ABI says so
asm/armv8-mont.pl:	sub	$j,$num,#16		// j=num-2
asm/armv8-mont.pl:	//	$lo0 being non-zero. So that carry can be calculated
asm/armv8-mont.pl:	//	by adding -1 to $lo0. That's what next instruction does.
asm/armv8-mont.pl:	sub	$j,$j,#8		// j--
asm/armv8-mont.pl:	str	$lo1,[$tp],#8		// tp[j-1]
asm/armv8-mont.pl:	sub	$i,$num,#8		// i=num-1
asm/armv8-mont.pl:	sub	$j,$num,#16		// j=num-2
asm/armv8-mont.pl:	sub	$i,$i,#8		// i--
asm/armv8-mont.pl:	sub	$j,$j,#8		// j--
asm/armv8-mont.pl:	stur	$lo1,[$tp,#-16]		// tp[j-1]
asm/armv8-mont.pl:	stp	$lo1,$hi1,[$tp,#-16]
asm/armv8-mont.pl:	subs	$j,$num,#8		// j=num-1 and clear borrow
asm/armv8-mont.pl:	sbcs	$aj,$tj,$nj		// tp[j]-np[j]
asm/armv8-mont.pl:	sub	$j,$j,#8		// j--
asm/armv8-mont.pl:	str	$aj,[$ap],#8		// rp[j]=tp[j]-np[j]
asm/armv8-mont.pl:	str	$aj,[$ap],#8		// rp[num-1]
asm/armv8-mont.pl:	sub	$num,$num,#8		// num--
asm/armv8-mont.pl:	sub	$num,$num,#8		// num--
asm/armv8-mont.pl:	stur	xzr,[$tp,#-16]		// wipe tp
asm/armv8-mont.pl:	stur	$nj,[$rp,#-16]
asm/armv8-mont.pl:	stur	xzr,[$tp,#-8]		// wipe tp
asm/armv8-mont.pl:	stur	$nj,[$rp,#-8]
asm/armv8-mont.pl:.size	bn_mul_mont,.-bn_mul_mont
asm/armv8-mont.pl:	stp	x29,x30,[sp,#-80]!
asm/armv8-mont.pl:	and	$toutptr,$toutptr,#-64
asm/armv8-mont.pl:	st1	{$temp.s}[0], [$toutptr],#4	// top-most bit
asm/armv8-mont.pl:	ldr	w10, [$aptr]		// load top-most bit
asm/armv8-mont.pl:.size	bn_mul8x_mont_neon,.-bn_mul8x_mont_neon
asm/armv8-mont.pl:# Following is ARMv8 adaptation of sqrx8x_mont from x86_64-mont5 module.
asm/armv8-mont.pl:	stp	x29,x30,[sp,#-128]!
asm/armv8-mont.pl:	mov	$cnt,#-8*8
asm/armv8-mont.pl:	adc	$carry,xzr,xzr		// carry bit, modulo-scheduled
asm/armv8-mont.pl:	ldur	$n0,[$rp,#-8*8]
asm/armv8-mont.pl:	mov	$cnt,#-8*8
asm/armv8-mont.pl:	// Now multiply above result by 2 and add a[n-1]*a[n-1]|...|a[0]*a[0]
asm/armv8-mont.pl:	mov	$topmost,xzr		// initial top-most carry
asm/armv8-mont.pl:	// (*)	mul	$t0,$a0,$na0	// lo(n[0-7])*lo(t[0]*n0)
asm/armv8-mont.pl:	umulh	$t0,$a0,$na0		// hi(n[0-7])*lo(t[0]*n0)
asm/armv8-mont.pl:	ldur	$n0,[$tp,#-8*8]
asm/armv8-mont.pl:	mov	$cnt,#-8*8
asm/armv8-mont.pl:	adc	$carry,xzr,xzr		// carry bit, modulo-scheduled
asm/armv8-mont.pl:	ldur	$n0,[$rp,#-8*8]
asm/armv8-mont.pl:	mov	$cnt,#-8*8
asm/armv8-mont.pl:	subs	xzr,$topmost,#1		// "move" top-most carry to carry bit
asm/armv8-mont.pl:	adc	$topmost,xzr,xzr	// top-most carry
asm/armv8-mont.pl:	// $acc0-7,$carry hold result, $a0-7 hold modulus
asm/armv8-mont.pl:	// $a0-7 hold result-modulus
asm/armv8-mont.pl:.size	__bn_sqr8x_mont,.-__bn_sqr8x_mont
asm/armv8-mont.pl:# x86_64-mont5 module, it's different in sense that it performs
asm/armv8-mont.pl:	stp	x29,x30,[sp,#-128]!
asm/armv8-mont.pl:	adc	$carry,$carry,xzr	// modulo-scheduled
asm/armv8-mont.pl:	adc	$carry,$carry,xzr	// modulo-scheduled
asm/armv8-mont.pl:	adc	$carry,$carry,xzr	// modulo-scheduled
asm/armv8-mont.pl:	adc	$carry,$carry,xzr	// modulo-scheduled
asm/armv8-mont.pl:	// $acc0-3,$carry hold result, $m0-7 hold modulus
asm/armv8-mont.pl:	// $a0-3 hold result-modulus
asm/armv8-mont.pl:.size	__bn_mul4x_mont,.-__bn_mul4x_mont
asm/bn-586.pl:# Copyright 1995-2020 The OpenSSL Project Authors. All Rights Reserved.
asm/bn-586.pl:for (@ARGV) { $sse2=1 if (/-DOPENSSL_IA32_SSE2/); }
asm/bn-586.pl:		 &dec("ecx") if ($i != 7-1);
asm/bn-586.pl:		&jz(&label("maw_end")) if ($i != 7-1);
asm/bn-586.pl:		 &dec($num) if ($i != 7-1);
asm/bn-586.pl:		&jz(&label("mw_end")) if ($i != 7-1);
asm/bn-586.pl:		 &dec($num) if ($i != 7-1);
asm/bn-586.pl:		 &jz(&label("sw_end")) if ($i != 7-1);
asm/bn-c64xplus.asm:;; Copyright 2012-2016 The OpenSSL Project Authors. All Rights Reserved.
asm/bn-c64xplus.asm:;; Compiler-generated multiply-n-add SPLOOP runs at 12*n cycles, n
asm/bn-c64xplus.asm:;; being the number of 32-bit words, addition - 8*n. Corresponding 4x
asm/bn-c64xplus.asm:;; unrolled SPLOOP-free loops - at ~8*n and ~5*n. Below assembler
asm/bn-c64xplus.asm:||[ A2]	MVK	-1,A4		; return overflow
asm/bn-c64xplus.asm:  [!A1]	SUB	A3,A6,A3	; hi-=dv
asm/bn-c64xplus.asm:  [!A1]	SUB	A3,A6,A3	; hi-=dv
asm/bn-c64xplus.asm:||	SUB	B0,2,B1		; N-2, initial ILC
asm/bn-c64xplus.asm:||	SUB	B0,1,B2		; const B2=N-1
asm/bn-c64xplus.asm:||	LDW	*A5++,A9	; pre-fetch ap[1]
asm/bn-c64xplus.asm:||	CMPGT	A0,1,A2		; done pre-fetching ap[i+1]?
asm/bn-c64xplus.asm:   [A2]	LDW	*A5++,A9	; pre-fetch ap[i+1]
asm/bn-c64xplus.asm:	STW	B19,*B4--[B2]	; rewind rp tp rp[1]
asm/bn-c64xplus.asm:	;; because of low-counter effect, when prologue phase finishes
asm/bn-c64xplus.asm:||	SUB	B0,1,B2		; const B2=N-1
asm/bn-c64xplus.asm:	ADDU	B0,A9:A8,A9:A8	; removed || to avoid cross-path stall below
asm/c64xplus-gf2m.pl:# Copyright 2012-2020 The OpenSSL Project Authors. All Rights Reserved.
asm/c64xplus-gf2m.pl:# used in bn_gf2m.c. It's kind of low-hanging mechanical port from
asm/c64xplus-gf2m.pl:# 4.5x faster than compiler-generated code. Though comparison is
asm/c64xplus-gf2m.pl:||	XOR	B29,B30,B30			; (a0+a1)·(b0+b1)-a0·b0-a1·b1
asm/co-586.pl:# Copyright 1995-2020 The OpenSSL Project Authors. All Rights Reserved.
asm/co-586.pl:	# pos == -1 if eax and edx are pre-loaded, 0 to load from next
asm/co-586.pl:	# "eax" and "edx" will always be pre-loaded.
asm/co-586.pl:	# pos == -1 if eax and edx are pre-loaded, 0 to load from next
asm/co-586.pl:	# "eax" and "edx" will always be pre-loaded.
asm/co-586.pl:	# pos == -1 if eax and edx are pre-loaded, 0 to load from next
asm/co-586.pl:	# "eax" and "edx" will always be pre-loaded.
asm/co-586.pl:	$tot=$num+$num-1;
asm/co-586.pl:				$na=($ai-1);
asm/co-586.pl:				$na=$as+($i < ($num-1));
asm/co-586.pl:				$nb=$bs+($i >= ($num-1));
asm/co-586.pl:#printf STDERR "[$ai,$bi] -> [$na,$nb]\n";
asm/co-586.pl:			$ai--;
asm/co-586.pl:		$as++ if ($i < ($num-1));
asm/co-586.pl:		$ae++ if ($i >= ($num-1));
asm/co-586.pl:		$bs++ if ($i >= ($num-1));
asm/co-586.pl:		$be++ if ($i < ($num-1));
asm/co-586.pl:	$tot=$num+$num-1;
asm/co-586.pl:			if (($ai-1) < ($bi+1))
asm/co-586.pl:				$na=$ai-1;
asm/co-586.pl:				$na=$as+($i < ($num-1));
asm/co-586.pl:				$nb=$bs+($i >= ($num-1));
asm/co-586.pl:			$ai--;
asm/co-586.pl:		$as++ if ($i < ($num-1));
asm/co-586.pl:		$ae++ if ($i >= ($num-1));
asm/co-586.pl:		$bs++ if ($i >= ($num-1));
asm/co-586.pl:		$be++ if ($i < ($num-1));
asm/ia64-mont.pl:# Copyright 2010-2020 The OpenSSL Project Authors. All Rights Reserved.
asm/ia64-mont.pl:# "Teaser" Montgomery multiplication module for IA-64. There are
asm/ia64-mont.pl:# - modulo-scheduling outer loop would eliminate quite a number of
asm/ia64-mont.pl:# - shorter vector support [with input vectors being fetched only
asm/ia64-mont.pl:# - 2x unroll with help of n0[1] would make the code scalable on
asm/ia64-mont.pl:#   "wider" IA-64, "wider" than Itanium 2 that is, which is not of
asm/ia64-mont.pl:# - dedicated squaring procedure(?);
asm/ia64-mont.pl:# Shorter vector support is implemented by zero-padding ap and np
asm/ia64-mont.pl:# vectors up to 8 elements, or 512 bits. This means that 256-bit
asm/ia64-mont.pl:# inputs will be processed only 2 times faster than 512-bit inputs,
asm/ia64-mont.pl:# core loop happens to match 512-bit timing. Either way, it resulted
asm/ia64-mont.pl:# in >100% improvement of 512-bit RSA sign benchmark and 50% - of
asm/ia64-mont.pl:# 1024-bit one [in comparison to original version of *this* module].
asm/ia64-mont.pl:# As it can be seen, RSA sign performance improves by 130-30%,
asm/ia64-mont.pl:# hereafter less for longer keys, while verify - by 74-13%.
asm/ia64-mont.pl:# DSA performance improves by 115-30%.
asm/ia64-mont.pl:    for (@ARGV) { $ADDP="add" if (/[\+DD|\-mlp]64/); }
asm/ia64-mont.pl:tp_1=r17;	// &tp[-1]
asm/ia64-mont.pl:	add		lc=-5,num
asm/ia64-mont.pl:{ .mfb;	and		sp=-16,r31		// alloca
asm/ia64-mont.pl:	brp.loop.imp	.L1st_ctop,.L1st_cend-16
asm/ia64-mont.pl:			// ------^----- (p40) at first (p23)
asm/ia64-mont.pl:			// ----------^^ p[16:20]=1
asm/ia64-mont.pl:	add		num=-1,num	};;	// num--
asm/ia64-mont.pl:	brp.loop.imp	.Linner_ctop,.Linner_cend-16
asm/ia64-mont.pl:			// ------^----- (p40) at first (p23)
asm/ia64-mont.pl:			// --------^--- (p30) at first (p22)
asm/ia64-mont.pl:			// ----------^^ p[16:20]=1
asm/ia64-mont.pl:// in latter case accounts for two-tick pipeline stall, which means
asm/ia64-mont.pl:	add		num=-1,num		// num--
asm/ia64-mont.pl:	brp.loop.imp	.Lsub_ctop,.Lsub_cend-16
asm/ia64-mont.pl:			// ------^---- (p33) at first (p17)
asm/ia64-mont.pl:	brp.loop.imp	.Lcopy_ctop,.Lcopy_cend-16
asm/ia64-mont.pl:{ .mmi;	add		r17=-6*16,sp
asm/ia64-mont.pl:	add		sp=-7*16,sp
asm/ia64-mont.pl:	stf.spill	[sp]=f16,-16
asm/ia64-mont.pl:	add		r16=-5*16,prevsp};;
asm/ia64-mont.pl:	addp4		r28=-1,in5	}
asm/ia64-mont.pl:	brp.loop.imp	.Louter_8_ctop,.Louter_8_cend-16
asm/ia64-mont.pl:// to platform-specific instruction-level profiler. On Itanium it
asm/ia64-mont.pl:	(p0)	add		r16=-7*16,prevsp
asm/ia64-mont.pl:	(p0)	add		r17=-6*16,prevsp	};;
asm/ia64-mont.pl:	add		r18=-5*16,prevsp
asm/ia64-mont.pl:// save the result, either tmp[num] or tmp[num]-np[num]
asm/ia64-mont.pl:	add		r19=-4*16,prevsp};;
asm/ia64-mont.pl:stringz	"Montgomery multiplication for IA-64, CRYPTOGAMS by <appro\@openssl.org>"
asm/ia64.S:.ident	"IA-64 ISA artwork by Andy Polyakov <appro@openssl.org>"
asm/ia64.S:// Copyright 2001-2018 The OpenSSL Project Authors. All Rights Reserved.
asm/ia64.S:// Version 2.x is Itanium2 re-tune. Few words about how Itanium2 is
asm/ia64.S:// discussed in commentary sections? Not really:-( Itanium2 has 6
asm/ia64.S:// essentially different in respect to this module, and a re-tune was
asm/ia64.S://	xma[->getf]	7[+1]	4[+0]
asm/ia64.S://	add[->st8]	1[+1]	1[+0]
asm/ia64.S:// stall condition and the code will therefore turn anti-scalable, e.g.
asm/ia64.S:// Itanium would exhibit anti-scalability. So I've chosen to reschedule
asm/ia64.S:// for worst latency for every instruction aiming for best *all-round*
asm/ia64.S://	Linux 7.1 2.96-81):
asm/ia64.S://	implementation:-)
asm/ia64.S://	My code is still way faster, huh:-) And I believe that even
asm/ia64.S://		if (((nrp[-1]+=v)&BN_MASK2) < v)
asm/ia64.S://	with SGIcc version 0.01.0-12 (keep in mind that for the moment
asm/ia64.S://	i.e. for a compiler generated one:-):
asm/ia64.S://	Oh! Benchmarks were performed on 733MHz Lion-class Itanium
asm/ia64.S://	(f32-f128) FP register bank over process context switch, thus
asm/ia64.S://	programs for that matter) with -mfixed-range=f32-f127 command
asm/ia64.S://	-Drum=nop.m in command line.
asm/ia64.S:// shall automagically spin in n+5 on "wider" IA-64 implementations:-)
asm/ia64.S:.skip	32	// makes the loop body aligned at 64-byte boundary
asm/ia64.S:	brp.loop.imp	.L_bn_add_words_ctop,.L_bn_add_words_cend-16
asm/ia64.S:	(p58)	cmp.eq.or	p57,p0=-1,r41	  // (p20)
asm/ia64.S:.skip	32	// makes the loop body aligned at 64-byte boundary
asm/ia64.S:	brp.loop.imp	.L_bn_sub_words_ctop,.L_bn_sub_words_cend-16
asm/ia64.S:	(p58)	add		r41=-1,r41	} // (p20)
asm/ia64.S:.skip	32	// makes the loop body aligned at 64-byte boundary
asm/ia64.S:			// ------^----- serves as (p50) at first (p27)
asm/ia64.S:	brp.loop.imp	.L_bn_mul_words_ctop,.L_bn_mul_words_cend-16
asm/ia64.S:// bypass L1 cache and L2 latency is actually best-case scenario for
asm/ia64.S:// "wider" IA-64 implementations. It's a trade-off here. n+24 loop
asm/ia64.S:// IA-64, but would hurt Itanium for about same because of longer
asm/ia64.S:// words at 6*(n+4) ticks:-( Compare to the "production" loop above
asm/ia64.S:// by moving the dependency to one-tick latent integer ALU. Note that
asm/ia64.S:.skip	48	// makes the loop body aligned at 64-byte boundary
asm/ia64.S:	brp.loop.imp	.L_bn_mul_add_words_ctop,.L_bn_mul_add_words_cend-16
asm/ia64.S:			// ------^----- serves as (p40) at first (p27)
asm/ia64.S:// Itanium 2. Yes, unlike previous versions it scales:-) Previous
asm/ia64.S:.skip	32	// makes the loop body aligned at 64-byte boundary
asm/ia64.S:	brp.loop.imp	.L_bn_sqr_words_ctop,.L_bn_sqr_words_cend-16
asm/ia64.S:// 2*(n+17) on Itanium, (n+17) on "wider" IA-64 implementations. It's
asm/ia64.S:// will appear larger than loss on "wider" IA-64, then the loop should
asm/ia64.S:// one 128-bit multiplication result per clock cycle multiplications
asm/ia64.S:// might become of interest for "wider" IA-64 implementation as you'll
asm/ia64.S:// drop down to bn_mul_comba8 code:-)
asm/ia64.S:// clause in Itanium µ-architecture manual? Comments are welcomed and
asm/ia64.S:// as it's over 10 (yes, ten, spelled as t-e-n) times faster than the
asm/ia64.S:// The one below is over 8 times faster than the one above:-( Even
asm/ia64.S:// -------\ Entering multiplier's heaven /-------
asm/ia64.S:// ------------\                    /------------
asm/ia64.S:// -----------------\          /-----------------
asm/ia64.S:// ----------------------\/----------------------
asm/ia64.S://	prevent "wider" IA-64 implementations from achieving the peak
asm/ia64.S://	implementation will have same latency:-). This stall will hold
asm/ia64.S://-------------------------------------------------//
asm/ia64.S://-------------------------------------------------//
asm/ia64.S://-------------------------------------------------//
asm/ia64.S://-------------------------------------------------//
asm/ia64.S://-------------------------------------------------//
asm/ia64.S://-------------------------------------------------//
asm/ia64.S://-------------------------------------------------//
asm/ia64.S://-------------------------------------------------//
asm/ia64.S://-------------------------------------------------//
asm/ia64.S://-------------------------------------------------//
asm/ia64.S:// Some preprocessors (most notably HP-UX) appear to be allergic to
asm/ia64.S:	mov		r8=-1
asm/ia64.S:.L_divw_shift:	// -vv- note signed comparison
asm/ia64.S:{ .mii;	(p0)	add		r32=-1,r33
asm/ia64.S:	(p8)	add		r31=-1,r31
asm/ia64.S:{ .mii;	(p0)	add		r32=-1,r33
asm/ia64.S:	(p8)	add		r31=-1,r31
asm/ia64.S:// any independent attempt to implement high-performance division
asm/ia64.S:// faster than Intel one (note commented splits:-), not to mention
asm/ia64.S:(pred)	fnma.s1		f9=f7,f8,f1		// [5]  e0 = 1 - b * y0
asm/ia64.S:(pred)	fnma.s1		f10=f7,f9,f6;;		// [20] r2 = a - b * q2
asm/mips-mont.pl:# Copyright 2010-2020 The OpenSSL Project Authors. All Rights Reserved.
asm/mips-mont.pl:# in-order-execution cores. While 512-bit RSA sign operations can be
asm/mips-mont.pl:# 65% faster in 64-bit mode, 1024-bit ones are only 15% faster, and
asm/mips-mont.pl:# 4096-bit ones are up to 15% slower. In 32-bit mode it varies from
asm/mips-mont.pl:# 16% improvement for 512-bit RSA sign to -33% for 4096-bit RSA
asm/mips-mont.pl:# verify:-( All comparisons are against bn_mul_mont-free assembler.
asm/mips-mont.pl:# and 75-30% [less for longer keys] on MIPS32 over compiler-generated
asm/mips-mont.pl:# - never ever touch $tp, "thread pointer", former $gp;
asm/mips-mont.pl:# - copy return value to $t0, former $v0 [or to $a0 if you're adapting
asm/mips-mont.pl:# - on O32 populate $a4-$a7 with 'lw $aN,4*N($sp)' if necessary;
asm/mips-mont.pl:	slt	$at,$num,17	# on in-order CPU
asm/mips-mont.pl:	.mask	0x40000000|$SAVED_REGS_MASK,-$SZREG
asm/mips-mont.pl:	$REG_S	$fp,($FRAMESIZE-1)*$SZREG($sp)
asm/mips-mont.pl:	$REG_S	$s11,($FRAMESIZE-2)*$SZREG($sp)
asm/mips-mont.pl:	$REG_S	$s10,($FRAMESIZE-3)*$SZREG($sp)
asm/mips-mont.pl:	$REG_S	$s9,($FRAMESIZE-4)*$SZREG($sp)
asm/mips-mont.pl:	$REG_S	$s8,($FRAMESIZE-5)*$SZREG($sp)
asm/mips-mont.pl:	$REG_S	$s7,($FRAMESIZE-6)*$SZREG($sp)
asm/mips-mont.pl:	$REG_S	$s6,($FRAMESIZE-7)*$SZREG($sp)
asm/mips-mont.pl:	$REG_S	$s5,($FRAMESIZE-8)*$SZREG($sp)
asm/mips-mont.pl:	$REG_S	$s4,($FRAMESIZE-9)*$SZREG($sp)
asm/mips-mont.pl:	$REG_S	$s3,($FRAMESIZE-10)*$SZREG($sp)
asm/mips-mont.pl:	$REG_S	$s2,($FRAMESIZE-11)*$SZREG($sp)
asm/mips-mont.pl:	$REG_S	$s1,($FRAMESIZE-12)*$SZREG($sp)
asm/mips-mont.pl:	$REG_S	$s0,($FRAMESIZE-13)*$SZREG($sp)
asm/mips-mont.pl:	li	$at,-4096
asm/mips-mont.pl:	$SUBU	$lo1,$lo0,$lo1	# tp[i]-np[i]
asm/mips-mont.pl:	$REG_L	$fp,($FRAMESIZE-1)*$SZREG($sp)
asm/mips-mont.pl:	$REG_L	$s11,($FRAMESIZE-2)*$SZREG($sp)
asm/mips-mont.pl:	$REG_L	$s10,($FRAMESIZE-3)*$SZREG($sp)
asm/mips-mont.pl:	$REG_L	$s9,($FRAMESIZE-4)*$SZREG($sp)
asm/mips-mont.pl:	$REG_L	$s8,($FRAMESIZE-5)*$SZREG($sp)
asm/mips-mont.pl:	$REG_L	$s7,($FRAMESIZE-6)*$SZREG($sp)
asm/mips-mont.pl:	$REG_L	$s6,($FRAMESIZE-7)*$SZREG($sp)
asm/mips-mont.pl:	$REG_L	$s5,($FRAMESIZE-8)*$SZREG($sp)
asm/mips-mont.pl:	$REG_L	$s4,($FRAMESIZE-9)*$SZREG($sp)
asm/mips-mont.pl:	$REG_L	$s3,($FRAMESIZE-10)*$SZREG($sp)
asm/mips-mont.pl:	$REG_L	$s2,($FRAMESIZE-11)*$SZREG($sp)
asm/mips-mont.pl:	$REG_L	$s1,($FRAMESIZE-12)*$SZREG($sp)
asm/mips-mont.pl:	$REG_L	$s0,($FRAMESIZE-13)*$SZREG($sp)
asm/mips.pl:# Copyright 2010-2020 The OpenSSL Project Authors. All Rights Reserved.
asm/mips.pl:# This is drop-in MIPS III/IV ISA replacement for crypto/bn/bn_asm.c.
asm/mips.pl:# because 5.x kernels put R4x00 CPU into 32-bit mode and all those
asm/mips.pl:# 64-bit instructions (daddu, dmultu, etc.) found below gonna only
asm/mips.pl:# cause illegal instruction exception:-(
asm/mips.pl:# I mean as long as -mmips-as is specified or is the default option,
asm/mips.pl:# Adapt the module even for 32-bit ABIs and other OSes. The former was
asm/mips.pl:# achieved by mechanical replacement of 64-bit arithmetic instructions
asm/mips.pl:# such as dmultu, daddu, etc. with their 32-bit counterparts and
asm/mips.pl:# >3x performance improvement naturally does not apply to 32-bit code
asm/mips.pl:# [because there is no instruction 32-bit compiler can't use], one
asm/mips.pl:# has to content with 40-85% improvement depending on benchmark and
asm/mips.pl:	.mask	0x8000f008,-$SZREG
asm/mips.pl:	li	$minus4,-4
asm/mips.pl:	sltu	$v0,$t1,$v0	# All manuals say it "compares 32-bit
asm/mips.pl:				# even on 64-bit registers.
asm/mips.pl:	$ST	$ta1,-2*$BNSZ($a0)
asm/mips.pl:	$ST	$ta3,-$BNSZ($a0)
asm/mips.pl:	.mask	0x8000f008,-$SZREG
asm/mips.pl:	li	$minus4,-4
asm/mips.pl:	$ST	$v0,-3*$BNSZ($a0)
asm/mips.pl:	$ST	$v0,-2*$BNSZ($a0)
asm/mips.pl:	$ST	$v0,-$BNSZ($a0)
asm/mips.pl:	.mask	0x8000f008,-$SZREG
asm/mips.pl:	li	$minus4,-4
asm/mips.pl:	$ST	$t3,-6*$BNSZ($a0)
asm/mips.pl:	$ST	$t2,-5*$BNSZ($a0)
asm/mips.pl:	$ST	$ta1,-4*$BNSZ($a0)
asm/mips.pl:	$ST	$ta0,-3*$BNSZ($a0)
asm/mips.pl:	$ST	$ta3,-2*$BNSZ($a0)
asm/mips.pl:	$ST	$ta2,-$BNSZ($a0)
asm/mips.pl:	.mask	0x8000f008,-$SZREG
asm/mips.pl:	li	$minus4,-4
asm/mips.pl:	$LD	$ta1,-3*$BNSZ($a2)
asm/mips.pl:	$LD	$ta2,-2*$BNSZ($a2)
asm/mips.pl:	$LD	$ta3,-$BNSZ($a2)
asm/mips.pl:	$ST	$t0,-4*$BNSZ($a0)
asm/mips.pl:	$ST	$t1,-3*$BNSZ($a0)
asm/mips.pl:	$ST	$t2,-2*$BNSZ($a0)
asm/mips.pl:	$ST	$t3,-$BNSZ($a0)
asm/mips.pl:	.mask	0x8000f008,-$SZREG
asm/mips.pl:	li	$minus4,-4
asm/mips.pl:	$LD	$ta1,-3*$BNSZ($a2)
asm/mips.pl:	$LD	$ta2,-2*$BNSZ($a2)
asm/mips.pl:	$LD	$ta3,-$BNSZ($a2)
asm/mips.pl:	$ST	$t0,-4*$BNSZ($a0)
asm/mips.pl:	$ST	$t1,-3*$BNSZ($a0)
asm/mips.pl:	$ST	$t2,-2*$BNSZ($a0)
asm/mips.pl:	$ST	$t3,-$BNSZ($a0)
asm/mips.pl: * The bn_div_3_words entry point is re-used for constant-time interface.
asm/mips.pl:				# instead of stack:-)
asm/mips.pl:	$LD	$a1,-$BNSZ($a3)
asm/mips.pl:	li	$v0,-1
asm/mips.pl:	.mask	0x8000f008,-$SZREG
asm/mips.pl:	$LD	$t2,-2*$BNSZ($a3)
asm/mips.pl:	li	$v0,-1		# I would rather signal div-by-zero
asm/mips.pl:	.mask	0x8000f008,-$SZREG
asm/mips.pl:	bgtz	$a2,.-4
asm/mips.pl:	li	$t2,-1
asm/mips.pl:	li	$QT,-1
asm/mips.pl:	li	$QT,-1
asm/mips.pl:	.mask	0x803ff008,-$SZREG
asm/mips.pl:	.mask	0x003f0000,-$SZREG
asm/mips.pl:	$LD	$a_0,0($a1)	# If compiled with -mips3 option on
asm/mips.pl:	.mask	0x8000f008,-$SZREG
asm/mips.pl:                # $c_[XYZ] when there is no Z-carry to accumulate yet;
asm/mips.pl:	.mask	0x8000f008,-$SZREG
asm/mips.pl:	.mask	0x8000f008,-$SZREG
asm/parisc-mont.pl:# Copyright 2009-2020 The OpenSSL Project Authors. All Rights Reserved.
asm/parisc-mont.pl:# On PA-7100LC this module performs ~90-50% better, less for longer
asm/parisc-mont.pl:# keys, than code generated by gcc 3.2 for PA-RISC 1.1. Latter means
asm/parisc-mont.pl:# that compiler utilized xmpyu instruction to perform 32x32=64-bit
asm/parisc-mont.pl:# toward 4 times 16x16=32-bit multiplications [plus complementary
asm/parisc-mont.pl:# for PA-RISC 1.1, but the "baseline" is far from optimal. The actual
asm/parisc-mont.pl:# improvement coefficient was never collected on PA-7100LC, or any
asm/parisc-mont.pl:# vendor compiler. But to give you a taste, PA-RISC 1.1 code path
asm/parisc-mont.pl:# of ~5x on PA-8600.
asm/parisc-mont.pl:# On PA-RISC 2.0 it has to compete with pa-risc2[W].s, which is
asm/parisc-mont.pl:# to comment in pa-risc2[W].s]. Here comes a catch. Execution core of
asm/parisc-mont.pl:# this implementation is actually 32-bit one, in the sense that it
asm/parisc-mont.pl:# operates on 32-bit values. But pa-risc2[W].s operates on arrays of
asm/parisc-mont.pl:# 64-bit BN_LONGs... How do they interoperate then? No problem. This
asm/parisc-mont.pl:# module picks halves of 64-bit values in reverse order and pretends
asm/parisc-mont.pl:# they were 32-bit BN_LONGs. But can 32-bit core compete with "pure"
asm/parisc-mont.pl:# 64-bit code such as pa-risc2[W].s then? Well, the thing is that
asm/parisc-mont.pl:# 32x32=64-bit multiplication is the best even PA-RISC 2.0 can do,
asm/parisc-mont.pl:# i.e. there is no "wider" multiplication like on most other 64-bit
asm/parisc-mont.pl:# platforms. This means that even being effectively 32-bit, this
asm/parisc-mont.pl:# implementation performs "64-bit" computational task in same amount
asm/parisc-mont.pl:# seem to exhaust memory port capacity. And indeed, dedicated PA-RISC
asm/parisc-mont.pl:# 2.0 code path provides virtually same performance as pa-risc2[W].s:
asm/parisc-mont.pl:# PA-RISC 1.1 and PA-RISC 2.0 ones. Latter features carry-free 64-bit
asm/parisc-mont.pl:# additions and 64-bit integer loads, not to mention specific
asm/parisc-mont.pl:# instruction scheduling. In 64-bit build naturally only 2.0 code path
asm/parisc-mont.pl:# is assembled. In 32-bit application context both code paths are
asm/parisc-mont.pl:# assembled, PA-RISC 2.0 CPU is detected at run-time and proper path
asm/parisc-mont.pl:# is taken automatically. Also, in 32-bit build the module imposes
asm/parisc-mont.pl:# addresses has to be 64-bit aligned. Normally neither is a problem:
asm/parisc-mont.pl:# most common key lengths are even and vectors are commonly malloc-ed,
asm/parisc-mont.pl:# Special thanks to polarhome.com for providing HP-UX account on
asm/parisc-mont.pl:# PA-RISC 1.1 machine, and to correspondent who chose to remain
asm/parisc-mont.pl:# anonymous for testing the code on PA-RISC 2.0 machine.
asm/parisc-mont.pl:$LOCALS=$FRAME-$FRAME_MARKER;
asm/parisc-mont.pl:$n0="%r22";	# passed through stack in 32-bit
asm/parisc-mont.pl:$num="%r21";	# passed through stack in 32-bit
asm/parisc-mont.pl:$xfer=$n0;	# accommodates [-16..15] offset in fld[dw]s
asm/parisc-mont.pl:	.CALLINFO	FRAME=`$FRAME-8*$SIZE_T`,NO_CALLS,SAVE_RP,SAVE_SP,ENTRY_GR=6
asm/parisc-mont.pl:	$PUSH	%r2,-$SAVED_RP(%sp)		; standard prologue
asm/parisc-mont.pl:	$PUSH	%r4,`-$FRAME+1*$SIZE_T`(%sp)
asm/parisc-mont.pl:	$PUSH	%r5,`-$FRAME+2*$SIZE_T`(%sp)
asm/parisc-mont.pl:	$PUSH	%r6,`-$FRAME+3*$SIZE_T`(%sp)
asm/parisc-mont.pl:	$PUSH	%r7,`-$FRAME+4*$SIZE_T`(%sp)
asm/parisc-mont.pl:	$PUSH	%r8,`-$FRAME+5*$SIZE_T`(%sp)
asm/parisc-mont.pl:	$PUSH	%r9,`-$FRAME+6*$SIZE_T`(%sp)
asm/parisc-mont.pl:	$PUSH	%r10,`-$FRAME+7*$SIZE_T`(%sp)
asm/parisc-mont.pl:	ldo	-$FRAME(%sp),$fp
asm/parisc-mont.pl:	ldw	`-$FRAME_MARKER-4`($fp),$n0
asm/parisc-mont.pl:	ldw	`-$FRAME_MARKER-8`($fp),$num
asm/parisc-mont.pl:	extru,=		$ti1,31,3,%r0		; are ap and np 64-bit aligned?
asm/parisc-mont.pl:	addl		$num,$num,$num		; I operate on 32-bit values
asm/parisc-mont.pl:	$PUSH		$fp,-$SIZE_T(%sp)
asm/parisc-mont.pl:	fstds		${fab0},-16($xfer)
asm/parisc-mont.pl:	fstds		${fnm0},-8($xfer)
asm/parisc-mont.pl:	extrd,u,*=	$hi0,%sar,1,$hi0	; executes on PA-RISC 1.0
asm/parisc-mont.pl:$code.=<<___;					# PA-RISC 2.0 code-path
asm/parisc-mont.pl:	ldd		-16($xfer),$ab0
asm/parisc-mont.pl:	fstds		${fab0},-16($xfer)
asm/parisc-mont.pl:	ldd		-8($xfer),$nm0
asm/parisc-mont.pl:	fstds		${fnm0},-8($xfer)
asm/parisc-mont.pl:	ldd		-16($xfer),$ab0
asm/parisc-mont.pl:	fstds		${fab0},-16($xfer)
asm/parisc-mont.pl:	ldd		-8($xfer),$nm0
asm/parisc-mont.pl:	fstds		${fnm0},-8($xfer)
asm/parisc-mont.pl:	stw		$nm1,-4($tp)		; tp[j-1]
asm/parisc-mont.pl:	 stw,ma		$nm0,8($tp)		; tp[j-1]
asm/parisc-mont.pl:	ldd		-16($xfer),$ab0
asm/parisc-mont.pl:	ldd		-8($xfer),$nm0
asm/parisc-mont.pl:	stw		$nm1,-4($tp)		; tp[j-1]
asm/parisc-mont.pl:	stw,ma		$nm0,8($tp)		; tp[j-1]
asm/parisc-mont.pl:	ldo		-1($num),$num		; i--
asm/parisc-mont.pl:	 fstws,mb	${fab0}L,-8($xfer)	; save high part
asm/parisc-mont.pl:	stw		$nm1,-4($tp)		; tp[j-1]
asm/parisc-mont.pl:	 fcnvxf,dbl,dbl	${fti},${fti}		; 32-bit unsigned int -> double
asm/parisc-mont.pl:	fcnvfx,dbl,dbl	${fab0},${fab0}		; double -> 33-bit unsigned int
asm/parisc-mont.pl:	fstds		${fab0},-16($xfer)	; 33-bit value
asm/parisc-mont.pl:	fstds		${fnm0},-8($xfer)
asm/parisc-mont.pl:	ldd		-16($xfer),$ab0		; 33-bit value
asm/parisc-mont.pl:	ldd		-8($xfer),$nm0
asm/parisc-mont.pl:	fstds		${fab0},-16($xfer)
asm/parisc-mont.pl:	fstds		${fnm0},-8($xfer)
asm/parisc-mont.pl:	stw		$nm1,-4($tp)		; tp[j-1]
asm/parisc-mont.pl:	ldd		-16($xfer),$ab0
asm/parisc-mont.pl:	fstds		${fab0},-16($xfer)
asm/parisc-mont.pl:	ldd		-8($xfer),$nm0
asm/parisc-mont.pl:	fstds		${fnm0},-8($xfer)
asm/parisc-mont.pl:	 stw,ma		$nm0,8($tp)		; tp[j-1]
asm/parisc-mont.pl:	ldd		-16($xfer),$ab0
asm/parisc-mont.pl:	ldd		-8($xfer),$nm0
asm/parisc-mont.pl:	 stw		$nm1,-4($tp)		; tp[j-1]
asm/parisc-mont.pl:	 stw,ma		$nm0,8($tp)		; tp[j-1]
asm/parisc-mont.pl:	addib,=		-1,$num,L\$outerdone	; i--
asm/parisc-mont.pl:	ldi		-4,$ti0
asm/parisc-mont.pl:	 fstws,mb	${fab0}L,-8($xfer)	; save high part
asm/parisc-mont.pl:	stw		$nm1,-4($tp)		; tp[j-1]
asm/parisc-mont.pl:	 fcnvxf,dbl,dbl	${fti},${fti}		; 32-bit unsigned int -> double
asm/parisc-mont.pl:	 fcnvfx,dbl,dbl	${fab0},${fab0}		; double -> 33-bit unsigned int
asm/parisc-mont.pl:	stw		$nm1,-4($tp)		; tp[j-1]
asm/parisc-mont.pl:	extru,=		$rp,31,3,%r0		; is rp 64-bit aligned?
asm/parisc-mont.pl:	std		$ti0,-8($tp)		; save flipped value
asm/parisc-mont.pl:if ($BN_SZ==4) {				# PA-RISC 1.1 code-path
asm/parisc-mont.pl:	ldw		-12($xfer),$ablo
asm/parisc-mont.pl:	ldw		-16($xfer),$hi0
asm/parisc-mont.pl:	ldw		-4($xfer),$nmlo0
asm/parisc-mont.pl:	ldw		-8($xfer),$nmhi0
asm/parisc-mont.pl:	fstds		${fab0},-16($xfer)
asm/parisc-mont.pl:	fstds		${fnm0},-8($xfer)
asm/parisc-mont.pl:	ldw		-12($xfer),$ablo
asm/parisc-mont.pl:	ldw		-16($xfer),$abhi
asm/parisc-mont.pl:	ldw		-4($xfer),$nmlo0
asm/parisc-mont.pl:	ldw		-8($xfer),$nmhi0
asm/parisc-mont.pl:	stw		$nmlo1,-4($tp)		; tp[j-1]
asm/parisc-mont.pl:	fstds		${fab0},-16($xfer)
asm/parisc-mont.pl:	fstds		${fnm0},-8($xfer)
asm/parisc-mont.pl:	 stws,ma	$nmlo0,8($tp)		; tp[j-1]
asm/parisc-mont.pl:	ldw		-16($xfer),$abhi
asm/parisc-mont.pl:	ldw		-12($xfer),$ablo
asm/parisc-mont.pl:	ldw		-8($xfer),$nmhi0
asm/parisc-mont.pl:	ldw		-4($xfer),$nmlo0
asm/parisc-mont.pl:	stw		$nmlo1,-4($tp)		; tp[j-1]
asm/parisc-mont.pl:	stws,ma		$nmlo0,8($tp)		; tp[j-1]
asm/parisc-mont.pl:	ldo		-1($num),$num		; i--
asm/parisc-mont.pl:	 fstws,mb	${fab0}L,-8($xfer)	; save high part
asm/parisc-mont.pl:	stw		$nmlo1,-4($tp)		; tp[j-1]
asm/parisc-mont.pl:	 fcnvxf,dbl,dbl	${fti},${fti}		; 32-bit unsigned int -> double
asm/parisc-mont.pl:	fcnvfx,dbl,dbl	${fab0},${fab0}		; double -> 33-bit unsigned int
asm/parisc-mont.pl:	fstds		${fab0},-16($xfer)	; 33-bit value
asm/parisc-mont.pl:	fstds		${fnm0},-8($xfer)
asm/parisc-mont.pl:	ldw		-16($xfer),$abhi	; carry bit actually
asm/parisc-mont.pl:	ldw		-12($xfer),$ablo
asm/parisc-mont.pl:	ldw		-8($xfer),$nmhi0
asm/parisc-mont.pl:	ldw		-4($xfer),$nmlo0
asm/parisc-mont.pl:	fstds		${fab0},-16($xfer)
asm/parisc-mont.pl:	fstds		${fnm0},-8($xfer)
asm/parisc-mont.pl:	ldw		-12($xfer),$ablo
asm/parisc-mont.pl:	ldw		-16($xfer),$abhi
asm/parisc-mont.pl:	ldw		-4($xfer),$nmlo0
asm/parisc-mont.pl:	ldw		-8($xfer),$nmhi0
asm/parisc-mont.pl:	stw		$nmlo1,-4($tp)		; tp[j-1]
asm/parisc-mont.pl:	fstds		${fab0},-16($xfer)
asm/parisc-mont.pl:	fstds		${fnm0},-8($xfer)
asm/parisc-mont.pl:	 stws,ma	$nmlo0,8($tp)		; tp[j-1]
asm/parisc-mont.pl:	ldw		-16($xfer),$abhi
asm/parisc-mont.pl:	ldw		-12($xfer),$ablo
asm/parisc-mont.pl:	ldw		-8($xfer),$nmhi0
asm/parisc-mont.pl:	ldw		-4($xfer),$nmlo0
asm/parisc-mont.pl:	 stw		$nmlo1,-4($tp)		; tp[j-1]
asm/parisc-mont.pl:	 stws,ma	$nmlo0,8($tp)		; tp[j-1]
asm/parisc-mont.pl:	addib,=		-1,$num,L\$outerdone_pa11; i--
asm/parisc-mont.pl:	 fstws,mb	${fab0}L,-8($xfer)	; save high part
asm/parisc-mont.pl:	stw		$nmlo1,-4($tp)		; tp[j-1]
asm/parisc-mont.pl:	 fcnvxf,dbl,dbl	${fti},${fti}		; 32-bit unsigned int -> double
asm/parisc-mont.pl:	 fcnvfx,dbl,dbl	${fab0},${fab0}		; double -> 33-bit unsigned int
asm/parisc-mont.pl:	stw		$nmlo1,-4($tp)		; tp[j-1]
asm/parisc-mont.pl:	ldw		-4($tp),$ti0
asm/parisc-mont.pl:	$POP	`-$FRAME-$SAVED_RP`(%sp),%r2	; standard epilogue
asm/parisc-mont.pl:	$POP	`-$FRAME+1*$SIZE_T`(%sp),%r4
asm/parisc-mont.pl:	$POP	`-$FRAME+2*$SIZE_T`(%sp),%r5
asm/parisc-mont.pl:	$POP	`-$FRAME+3*$SIZE_T`(%sp),%r6
asm/parisc-mont.pl:	$POP	`-$FRAME+4*$SIZE_T`(%sp),%r7
asm/parisc-mont.pl:	$POP	`-$FRAME+5*$SIZE_T`(%sp),%r8
asm/parisc-mont.pl:	$POP	`-$FRAME+6*$SIZE_T`(%sp),%r9
asm/parisc-mont.pl:	$POP	`-$FRAME+7*$SIZE_T`(%sp),%r10
asm/parisc-mont.pl:	$POPMB	-$FRAME(%sp),%r3
asm/parisc-mont.pl:	.STRINGZ "Montgomery Multiplication for PA-RISC, CRYPTOGAMS by <appro\@openssl.org>"
asm/parisc-mont.pl:# Explicitly encode PA-RISC 2.0 instructions used in this module, so
asm/parisc-mont.pl:    if ($args =~ /%r([0-9]+)\(%r([0-9]+)\),%r([0-9]+)/)		# format 4
asm/parisc-mont.pl:    elsif ($args =~ /(\-?[0-9]+)\(%r([0-9]+)\),%r([0-9]+)/)	# format 5
asm/parisc-mont.pl:    if ($args =~ /%r([0-9]+),(\-?[0-9]+)\(%r([0-9]+)\)/)	# format 6
asm/parisc-mont.pl:    if ($args =~ /%r([0-9]+),([0-9]+),([0-9]+),%r([0-9]+)/)	# format 15
asm/parisc-mont.pl:	my $len=32-$3;
asm/parisc-mont.pl:    elsif ($args =~ /%r([0-9]+),%sar,([0-9]+),%r([0-9]+)/)	# format 12
asm/parisc-mont.pl:	my $len=32-$2;
asm/parisc-mont.pl:    if ($args =~ /%r([0-9]+),%r([0-9]+),([0-9]+),%r([0-9]+)/)	# format 14
asm/parisc-mont.pl:	my $cpos=63-$3;
asm/parisc-mont.pl:    if ($mod eq ",db" && $args =~ /%r([0-9]+),%r([0-9]+),%r([0-9]+)/) {
asm/parisc-mont.pl:if (`$ENV{CC} -Wa,-v -c -o /dev/null -x assembler /dev/null 2>&1`
asm/parisc-mont.pl:	# flip word order in 64-bit mode...
asm/parisc-mont.pl:	# assemble 2.0 instructions in 32-bit mode...
asm/parisc-mont.pl:	s/^\s+([a-z]+)([\S]*)\s+([\S]*)/&assemble($1,$2,$3)/e if ($BN_SZ==4);
asm/ppc-mont.pl:# Copyright 2006-2020 The OpenSSL Project Authors. All Rights Reserved.
asm/ppc-mont.pl:# to gain a bit more by modulo-scheduling outer loop, then dedicated
asm/ppc-mont.pl:# for 32-bit application running on 64-bit CPU. As for the latter.
asm/ppc-mont.pl:# It won't be able to achieve "native" 64-bit performance, because in
asm/ppc-mont.pl:# 32-bit application context every addc instruction will have to be
asm/ppc-mont.pl:# So far RSA *sign* performance improvement over pre-bn_mul_mont asm
asm/ppc-mont.pl:# for 64-bit application running on PPC970/G5 is:
asm/ppc-mont.pl:# 512-bit	+65%
asm/ppc-mont.pl:# 1024-bit	+35%
asm/ppc-mont.pl:# 2048-bit	+18%
asm/ppc-mont.pl:# 4096-bit	+4%
asm/ppc-mont.pl:# ~35-50% faster (more for longer keys) on contemporary high-end POWER
asm/ppc-mont.pl:# processors in 64-bit builds, [mysteriously enough] more in 32-bit
asm/ppc-mont.pl:# builds. On low-end 32-bit processors performance improvement turned
asm/ppc-mont.pl:	# same as above, but 64-bit mnemonics...
asm/ppc-mont.pl:( $xlate="${dir}ppc-xlate.pl" and -f $xlate ) or
asm/ppc-mont.pl:( $xlate="${dir}../../perlasm/ppc-xlate.pl" and -f $xlate) or
asm/ppc-mont.pl:die "can't locate ppc-xlate.pl";
asm/ppc-mont.pl:# non-volatile registers
asm/ppc-mont.pl:	li	$tj,-4096
asm/ppc-mont.pl:	subf	$ovf,$ovf,$sp	; $sp-$ovf
asm/ppc-mont.pl:	subf	$ovf,$sp,$ovf	; $ovf-$sp
asm/ppc-mont.pl:	$PUSH	r20,`-12*$SIZE_T`($tj)
asm/ppc-mont.pl:	$PUSH	r21,`-11*$SIZE_T`($tj)
asm/ppc-mont.pl:	$PUSH	r22,`-10*$SIZE_T`($tj)
asm/ppc-mont.pl:	$PUSH	r23,`-9*$SIZE_T`($tj)
asm/ppc-mont.pl:	$PUSH	r24,`-8*$SIZE_T`($tj)
asm/ppc-mont.pl:	$PUSH	r25,`-7*$SIZE_T`($tj)
asm/ppc-mont.pl:	$PUSH	r26,`-6*$SIZE_T`($tj)
asm/ppc-mont.pl:	$PUSH	r27,`-5*$SIZE_T`($tj)
asm/ppc-mont.pl:	$PUSH	r28,`-4*$SIZE_T`($tj)
asm/ppc-mont.pl:	$PUSH	r29,`-3*$SIZE_T`($tj)
asm/ppc-mont.pl:	$PUSH	r30,`-2*$SIZE_T`($tj)
asm/ppc-mont.pl:	$PUSH	r31,`-1*$SIZE_T`($tj)
asm/ppc-mont.pl:	addi	$num,$num,-2	; adjust $num for counter register
asm/ppc-mont.pl:	$ST	$lo1,0($tp)	; tp[j-1]
asm/ppc-mont.pl:	$ST	$lo1,0($tp)	; tp[j-1]
asm/ppc-mont.pl:	$ST	$lo1,0($tp)	; tp[j-1]
asm/ppc-mont.pl:	$ST	$lo1,0($tp)	; tp[j-1]
asm/ppc-mont.pl:	addic	$ovf,$ovf,-1	; move upmost overflow to XER[CA]
asm/ppc-mont.pl:	subfe	$aj,$nj,$tj	; tp[j]-np[j]
asm/ppc-mont.pl:	$POP	r20,`-12*$SIZE_T`($tj)
asm/ppc-mont.pl:	$POP	r21,`-11*$SIZE_T`($tj)
asm/ppc-mont.pl:	$POP	r22,`-10*$SIZE_T`($tj)
asm/ppc-mont.pl:	$POP	r23,`-9*$SIZE_T`($tj)
asm/ppc-mont.pl:	$POP	r24,`-8*$SIZE_T`($tj)
asm/ppc-mont.pl:	$POP	r25,`-7*$SIZE_T`($tj)
asm/ppc-mont.pl:	$POP	r26,`-6*$SIZE_T`($tj)
asm/ppc-mont.pl:	$POP	r27,`-5*$SIZE_T`($tj)
asm/ppc-mont.pl:	$POP	r28,`-4*$SIZE_T`($tj)
asm/ppc-mont.pl:	$POP	r29,`-3*$SIZE_T`($tj)
asm/ppc-mont.pl:	$POP	r30,`-2*$SIZE_T`($tj)
asm/ppc-mont.pl:	$POP	r31,`-1*$SIZE_T`($tj)
asm/ppc-mont.pl:.size	.bn_mul_mont_int,.-.bn_mul_mont_int
asm/ppc-mont.pl:# sp----------->+-------------------------------+
asm/ppc-mont.pl:#		+-------------------------------+
asm/ppc-mont.pl:# +8*size_t	+-------------------------------+
asm/ppc-mont.pl:# +12*size_t	+-------------------------------+
asm/ppc-mont.pl:#		+-------------------------------+
asm/ppc-mont.pl:# -18*size_t	+-------------------------------+
asm/ppc-mont.pl:#		| 18 saved gpr, r14-r31		|
asm/ppc-mont.pl:#		+-------------------------------+
asm/ppc-mont.pl:	li	$a1,-32*$SIZE_T
asm/ppc-mont.pl:	$PUSH	r14,-$SIZE_T*18($a0)
asm/ppc-mont.pl:	$PUSH	r15,-$SIZE_T*17($a0)
asm/ppc-mont.pl:	$PUSH	r16,-$SIZE_T*16($a0)
asm/ppc-mont.pl:	$PUSH	r17,-$SIZE_T*15($a0)
asm/ppc-mont.pl:	$PUSH	r18,-$SIZE_T*14($a0)
asm/ppc-mont.pl:	$PUSH	r19,-$SIZE_T*13($a0)
asm/ppc-mont.pl:	$PUSH	r20,-$SIZE_T*12($a0)
asm/ppc-mont.pl:	$PUSH	r21,-$SIZE_T*11($a0)
asm/ppc-mont.pl:	$PUSH	r22,-$SIZE_T*10($a0)
asm/ppc-mont.pl:	$PUSH	r23,-$SIZE_T*9($a0)
asm/ppc-mont.pl:	$PUSH	r24,-$SIZE_T*8($a0)
asm/ppc-mont.pl:	$PUSH	r25,-$SIZE_T*7($a0)
asm/ppc-mont.pl:	$PUSH	r26,-$SIZE_T*6($a0)
asm/ppc-mont.pl:	$PUSH	r27,-$SIZE_T*5($a0)
asm/ppc-mont.pl:	$PUSH	r28,-$SIZE_T*4($a0)
asm/ppc-mont.pl:	$PUSH	r29,-$SIZE_T*3($a0)
asm/ppc-mont.pl:	$PUSH	r30,-$SIZE_T*2($a0)
asm/ppc-mont.pl:	$PUSH	r31,-$SIZE_T*1($a0)
asm/ppc-mont.pl:	subi	$ap,$ap,$SIZE_T		# bias by -1
asm/ppc-mont.pl:	subi	$np,$np,$SIZE_T		# bias by -1
asm/ppc-mont.pl:	subi	$rp,$rp,$SIZE_T		# bias by -1
asm/ppc-mont.pl:	subi	$t0,$t0,$SIZE_T*4	# &b[num-4]
asm/ppc-mont.pl:	$PUSH	$rp,$SIZE_T*6($sp)	# offload rp and &b[num-4]
asm/ppc-mont.pl:	addic	$tp,$sp,$SIZE_T*7	# &t[-1], clear carry bit
asm/ppc-mont.pl:	addze	$carry,$carry		# modulo-scheduled
asm/ppc-mont.pl:	andi.	$cnt,$cnt,$SIZE_T*4-1
asm/ppc-mont.pl:	#	$acc0 being non-zero. So that carry can be calculated
asm/ppc-mont.pl:	#	by adding -1 to $acc0. That's what next instruction does.
asm/ppc-mont.pl:	addic	$acc0,$acc0,-1		# (*), discarded
asm/ppc-mont.pl:	addze	$carry,$carry		# modulo-scheduled
asm/ppc-mont.pl:	andi.	$cnt,$cnt,$SIZE_T*4-1
asm/ppc-mont.pl:	addic	$tp,$sp,$SIZE_T*7	# &t[-1], clear carry bit
asm/ppc-mont.pl:	addze	$carry,$carry		# modulo-scheduled
asm/ppc-mont.pl:	andi.	$cnt,$cnt,$SIZE_T*4-1
asm/ppc-mont.pl:	addic	$acc0,$acc0,-1		# (*), discarded
asm/ppc-mont.pl:	addze	$carry,$carry		# modulo-scheduled
asm/ppc-mont.pl:	andi.	$cnt,$cnt,$SIZE_T*4-1
asm/ppc-mont.pl:	$POP	$t2,$SIZE_T*6($sp)	# pull rp and &b[num-4]
asm/ppc-mont.pl:	addic	$tp,$sp,$SIZE_T*7	# &t[-1], clear carry bit
asm/ppc-mont.pl:	mr	$bp,$t2			# &rp[-1]
asm/ppc-mont.pl:	mr	$ap_end,$t2		# &rp[-1] copy
asm/ppc-mont.pl:	$POP	$ap,$SIZE_T*6($sp)	# pull &rp[-1]
asm/ppc-mont.pl:	addze	$carry,$carry		# modulo-scheduled
asm/ppc-mont.pl:	# $acc0-3,$carry hold result, $m0-3 hold modulus
asm/ppc-mont.pl:	$POP	r14,-$SIZE_T*18($bp)
asm/ppc-mont.pl:	$POP	r15,-$SIZE_T*17($bp)
asm/ppc-mont.pl:	$POP	r16,-$SIZE_T*16($bp)
asm/ppc-mont.pl:	$POP	r17,-$SIZE_T*15($bp)
asm/ppc-mont.pl:	$POP	r18,-$SIZE_T*14($bp)
asm/ppc-mont.pl:	$POP	r19,-$SIZE_T*13($bp)
asm/ppc-mont.pl:	$POP	r20,-$SIZE_T*12($bp)
asm/ppc-mont.pl:	$POP	r21,-$SIZE_T*11($bp)
asm/ppc-mont.pl:	$POP	r22,-$SIZE_T*10($bp)
asm/ppc-mont.pl:	$POP	r23,-$SIZE_T*9($bp)
asm/ppc-mont.pl:	$POP	r24,-$SIZE_T*8($bp)
asm/ppc-mont.pl:	$POP	r25,-$SIZE_T*7($bp)
asm/ppc-mont.pl:	$POP	r26,-$SIZE_T*6($bp)
asm/ppc-mont.pl:	$POP	r27,-$SIZE_T*5($bp)
asm/ppc-mont.pl:	$POP	r28,-$SIZE_T*4($bp)
asm/ppc-mont.pl:	$POP	r29,-$SIZE_T*3($bp)
asm/ppc-mont.pl:	$POP	r30,-$SIZE_T*2($bp)
asm/ppc-mont.pl:	$POP	r31,-$SIZE_T*1($bp)
asm/ppc-mont.pl:.size	.bn_mul4x_mont_int,.-.bn_mul4x_mont_int
asm/ppc-mont.pl:# Following is PPC adaptation of sqrx8x_mont from x86_64-mont5 module.
asm/ppc-mont.pl:# sp----------->+-------------------------------+
asm/ppc-mont.pl:#		+-------------------------------+
asm/ppc-mont.pl:# +12*size_t	+-------------------------------+
asm/ppc-mont.pl:#		+-------------------------------+
asm/ppc-mont.pl:# -18*size_t	+-------------------------------+
asm/ppc-mont.pl:#		| 18 saved gpr, r14-r31		|
asm/ppc-mont.pl:#		+-------------------------------+
asm/ppc-mont.pl:	li	$a2,-32*$SIZE_T
asm/ppc-mont.pl:	$PUSH	r14,-$SIZE_T*18($a0)
asm/ppc-mont.pl:	$PUSH	r15,-$SIZE_T*17($a0)
asm/ppc-mont.pl:	$PUSH	r16,-$SIZE_T*16($a0)
asm/ppc-mont.pl:	$PUSH	r17,-$SIZE_T*15($a0)
asm/ppc-mont.pl:	$PUSH	r18,-$SIZE_T*14($a0)
asm/ppc-mont.pl:	$PUSH	r19,-$SIZE_T*13($a0)
asm/ppc-mont.pl:	$PUSH	r20,-$SIZE_T*12($a0)
asm/ppc-mont.pl:	$PUSH	r21,-$SIZE_T*11($a0)
asm/ppc-mont.pl:	$PUSH	r22,-$SIZE_T*10($a0)
asm/ppc-mont.pl:	$PUSH	r23,-$SIZE_T*9($a0)
asm/ppc-mont.pl:	$PUSH	r24,-$SIZE_T*8($a0)
asm/ppc-mont.pl:	$PUSH	r25,-$SIZE_T*7($a0)
asm/ppc-mont.pl:	$PUSH	r26,-$SIZE_T*6($a0)
asm/ppc-mont.pl:	$PUSH	r27,-$SIZE_T*5($a0)
asm/ppc-mont.pl:	$PUSH	r28,-$SIZE_T*4($a0)
asm/ppc-mont.pl:	$PUSH	r29,-$SIZE_T*3($a0)
asm/ppc-mont.pl:	$PUSH	r30,-$SIZE_T*2($a0)
asm/ppc-mont.pl:	$PUSH	r31,-$SIZE_T*1($a0)
asm/ppc-mont.pl:	subi	$ap,$ap,$SIZE_T		# bias by -1
asm/ppc-mont.pl:	subi	$t0,$np,$SIZE_T		# bias by -1
asm/ppc-mont.pl:	subi	$rp,$rp,$SIZE_T		# bias by -1
asm/ppc-mont.pl:	addi	$tp,$sp,$SIZE_T*11	# &tp[-1]
asm/ppc-mont.pl:	$PUSH	$rp,$SIZE_T*6($sp)	# offload &rp[-1]
asm/ppc-mont.pl:	$PUSH	$t0,$SIZE_T*7($sp)	# offload &np[-1]
asm/ppc-mont.pl:	$PUSH	$tp,$SIZE_T*9($sp)	# &tp[2*num-1]
asm/ppc-mont.pl:	$PUSH	$zero,$SIZE_T*10($sp)	# initial top-most carry
asm/ppc-mont.pl:	addi	$tp,$sp,$SIZE_T*11	# &tp[-1]
asm/ppc-mont.pl:	addze	$carry,$zero		# carry bit, modulo-scheduled
asm/ppc-mont.pl:	andi.	$cnt,$cnt,$SIZE_T*8-1
asm/ppc-mont.pl:	# Now multiply above result by 2 and add a[n-1]*a[n-1]|...|a[0]*a[0]
asm/ppc-mont.pl:	$LD	$a1,$SIZE_T*1($t0)	# recall that $t0 is &a[-1]
asm/ppc-mont.pl:	addi	$tp,$sp,$SIZE_T*11	# &tp[-1]
asm/ppc-mont.pl:	$SHRI	$t1,$t1,$BITS-1
asm/ppc-mont.pl:	$SHRI	$t2,$t2,$BITS-1
asm/ppc-mont.pl:	$SHRI	$t3,$t3,$BITS-1
asm/ppc-mont.pl:	$SHRI	$t0,$t0,$BITS-1
asm/ppc-mont.pl:	$SHRI	$t1,$t1,$BITS-1
asm/ppc-mont.pl:	$SHRI	$t2,$t2,$BITS-1
asm/ppc-mont.pl:	$SHRI	$t3,$t3,$BITS-1
asm/ppc-mont.pl:	$SHRI	$t0,$t0,$BITS-1
asm/ppc-mont.pl:	$SHRI	$t1,$t1,$BITS-1
asm/ppc-mont.pl:	$SHRI	$t2,$t2,$BITS-1
asm/ppc-mont.pl:	$SHRI	$t3,$t3,$BITS-1
asm/ppc-mont.pl:	 $POP	$np,$SIZE_T*7($sp)	# pull &np[-1] and n0
asm/ppc-mont.pl:	$SHRI	$t0,$t0,$BITS-1
asm/ppc-mont.pl:	$SHRI	$t1,$t1,$BITS-1
asm/ppc-mont.pl:	$SHRI	$t2,$t2,$BITS-1
asm/ppc-mont.pl:	addi	$tp,$sp,$SIZE_T*11	# &tp[-1]
asm/ppc-mont.pl:	# (*)	$UMULL	$t0,$a0,$na0	# lo(n[0-7])*lo(t[0]*n0)
asm/ppc-mont.pl:	addic	$acc0,$acc0,-1		# (*)
asm/ppc-mont.pl:	$UMULH	$t0,$a0,$na0		# hi(n[0-7])*lo(t[0]*n0)
asm/ppc-mont.pl:	addze	$carry,$zero		# carry bit, modulo-scheduled
asm/ppc-mont.pl:	andi.	$cnt,$cnt,$SIZE_T*8-1
asm/ppc-mont.pl:	$POP	$carry,$SIZE_T*10($sp)	# pull top-most carry in case we break
asm/ppc-mont.pl:	$POP	$t3,$SIZE_T*9($sp)	# &tp[2*num-1]
asm/ppc-mont.pl:	addic	$carry,$carry,-1	# "move" top-most carry to carry bit
asm/ppc-mont.pl:	$LD	$a0,$SIZE_T*1($t2)	# recall that $t2 is &n[-1]
asm/ppc-mont.pl:	addze	$t2,$zero		# top-most carry
asm/ppc-mont.pl:	$PUSH	$t2,$SIZE_T*10($sp)	# off-load top-most carry
asm/ppc-mont.pl:	$POP	$rp,$SIZE_T*6($sp)	# pull &rp[-1]
asm/ppc-mont.pl:	 $ST	$zero,-$SIZE_T*3($n0)	# wipe stack clean
asm/ppc-mont.pl:	 $ST	$zero,-$SIZE_T*2($n0)
asm/ppc-mont.pl:	 $ST	$zero,-$SIZE_T*1($n0)
asm/ppc-mont.pl:	 $ST	$zero,-$SIZE_T*0($n0)
asm/ppc-mont.pl:	# $acc0-7,$carry hold result, $a0-7 hold modulus
asm/ppc-mont.pl:	$POP	r14,-$SIZE_T*18($ap)
asm/ppc-mont.pl:	$POP	r15,-$SIZE_T*17($ap)
asm/ppc-mont.pl:	$POP	r16,-$SIZE_T*16($ap)
asm/ppc-mont.pl:	$POP	r17,-$SIZE_T*15($ap)
asm/ppc-mont.pl:	$POP	r18,-$SIZE_T*14($ap)
asm/ppc-mont.pl:	$POP	r19,-$SIZE_T*13($ap)
asm/ppc-mont.pl:	$POP	r20,-$SIZE_T*12($ap)
asm/ppc-mont.pl:	$POP	r21,-$SIZE_T*11($ap)
asm/ppc-mont.pl:	$POP	r22,-$SIZE_T*10($ap)
asm/ppc-mont.pl:	$POP	r23,-$SIZE_T*9($ap)
asm/ppc-mont.pl:	$POP	r24,-$SIZE_T*8($ap)
asm/ppc-mont.pl:	$POP	r25,-$SIZE_T*7($ap)
asm/ppc-mont.pl:	$POP	r26,-$SIZE_T*6($ap)
asm/ppc-mont.pl:	$POP	r27,-$SIZE_T*5($ap)
asm/ppc-mont.pl:	$POP	r28,-$SIZE_T*4($ap)
asm/ppc-mont.pl:	$POP	r29,-$SIZE_T*3($ap)
asm/ppc-mont.pl:	$POP	r30,-$SIZE_T*2($ap)
asm/ppc-mont.pl:	$POP	r31,-$SIZE_T*1($ap)
asm/ppc-mont.pl:.size	__bn_sqr8x_mont,.-__bn_sqr8x_mont
asm/ppc.pl:# Copyright 2004-2020 The OpenSSL Project Authors. All Rights Reserved.
asm/ppc.pl:# as pre-processor to cover for platform differences in name decoration,
asm/ppc.pl:# linker tables, 32-/64-bit instruction sets...
asm/ppc.pl:# Linux and AIX use different 32-bit ABIs. Good news are that these ABIs
asm/ppc.pl:#	The following is the performance of 32-bit compiler
asm/ppc.pl:#compiler: cc -DTHREADS  -DAIX -DB_ENDIAN -DBN_LLONG -O3
asm/ppc.pl:#	Here are performance numbers for 64-bit compiler
asm/ppc.pl:#	compiler: cc -DTHREADS -D_REENTRANT -q64 -DB_ENDIAN -O3
asm/ppc.pl:	# same as above, but 64-bit mnemonics...
asm/ppc.pl:( $xlate="${dir}ppc-xlate.pl" and -f $xlate ) or
asm/ppc.pl:( $xlate="${dir}../../perlasm/ppc-xlate.pl" and -f $xlate) or
asm/ppc.pl:die "can't locate ppc-xlate.pl";
asm/ppc.pl:#--------------------------------------------------------------------
asm/ppc.pl:#--------------------------------------------------------------------------
asm/ppc.pl:.size	.bn_sqr_comba4,.-.bn_sqr_comba4
asm/ppc.pl:.size	.bn_sqr_comba8,.-.bn_sqr_comba8
asm/ppc.pl:.size	.bn_mul_comba4,.-.bn_mul_comba4
asm/ppc.pl:.size	.bn_mul_comba8,.-.bn_mul_comba8
asm/ppc.pl:	addi	r4,r4,-$BNSZ
asm/ppc.pl:	addi	r3,r3,-$BNSZ
asm/ppc.pl:	addi	r5,r5,-$BNSZ
asm/ppc.pl:				# if carry = 1 this is r7-r8. Else it
asm/ppc.pl:				# is r7-r8 -1 as we need.
asm/ppc.pl:	subfze	r3,r0		# if carry bit is set then r3 = 0 else -1
asm/ppc.pl:.size	.bn_sub_words,.-.bn_sub_words
asm/ppc.pl:	addi	r4,r4,-$BNSZ
asm/ppc.pl:	addi	r3,r3,-$BNSZ
asm/ppc.pl:	addi	r5,r5,-$BNSZ
asm/ppc.pl:.size	.bn_add_words,.-.bn_add_words
asm/ppc.pl:#	only at level -O2 we can possibly squeeze it more?
asm/ppc.pl:	li	r3,-1			# d=0 return -1
asm/ppc.pl:	subf	r3,r5,r3		#h-=d ;
asm/ppc.pl:Lppcasm_div3:				#r7 = BN_BITS2-i. so r7=i
asm/ppc.pl:	$SHR	r8,r4,r8		# r8 = (l >> BN_BITS2 -i)
asm/ppc.pl:	or	r3,r3,r8		# h = (h<<i)|(l>>(BN_BITS2-i))
asm/ppc.pl:	li	r8,-1
asm/ppc.pl:	subf	r10,r12,r3		#t = h -th
asm/ppc.pl:	addi	r8,r8,-1		#q--
asm/ppc.pl:	subf	r12,r9,r12		#th -=dh
asm/ppc.pl:	subf	r6,r10,r6		#tl -=dl
asm/ppc.pl:	subf	r11,r11,r4		#r11=l-tl
asm/ppc.pl:	addi	r8,r8,-1		# q--
asm/ppc.pl:	subf	r12,r12,r3		#r12 = h-th
asm/ppc.pl:.size	.bn_div_words,.-.bn_div_words
asm/ppc.pl:	addi	r4,r4,-$BNSZ
asm/ppc.pl:	addi	r3,r3,-$BNSZ
asm/ppc.pl:.size	.bn_sqr_words,.-.bn_sqr_words
asm/ppc.pl:	addi	r5,r5,-1
asm/ppc.pl:	addi	r5,r5,-1
asm/ppc.pl:.size	.bn_mul_words,.-.bn_mul_words
asm/ppc.pl:	addi	r3,r3,-$BNSZ
asm/ppc.pl:	addi	r4,r4,-$BNSZ
asm/ppc.pl:.size	.bn_mul_add_words,.-.bn_mul_add_words
asm/ppc64-mont.pl:# Copyright 2007-2020 The OpenSSL Project Authors. All Rights Reserved.
asm/ppc64-mont.pl:# something wrong, but in the lack of assembler level micro-profiling
asm/ppc64-mont.pl:# *worse* performance on other PowerPC implementations, ~40-15% slower
asm/ppc64-mont.pl:# key lengths. As it's obviously inappropriate as "best all-round"
asm/ppc64-mont.pl:# alternative, it has to be complemented with run-time CPU family
asm/ppc64-mont.pl:# implementation IALU ppc-mont.pl module performs *suboptimally* on
asm/ppc64-mont.pl:# >=1024-bit key lengths on Power 6. It should also be noted that
asm/ppc64-mont.pl:# *everything* said so far applies to 64-bit builds! As far as 32-bit
asm/ppc64-mont.pl:# application executed on 64-bit CPU goes, this module is likely to
asm/ppc64-mont.pl:# case and *is* faster than 32-bit ppc-mont.pl on *all* processors.
asm/ppc64-mont.pl:# Micro-profiling assisted optimization results in ~15% improvement
asm/ppc64-mont.pl:# over original ppc64-mont.pl version, or overall ~50% improvement
asm/ppc64-mont.pl:# over ppc.pl module on Power 6. If compared to ppc-mont.pl on same
asm/ppc64-mont.pl:# Power 6 CPU, this module is 5-150% faster depending on key length,
asm/ppc64-mont.pl:# [hereafter] more for longer keys. But if compared to ppc-mont.pl
asm/ppc64-mont.pl:# on 1.8GHz PPC970, it's only 5-55% faster. Still far from impressive
asm/ppc64-mont.pl:# Adapted for 32-bit build this module delivers 25-120%, yes, more
asm/ppc64-mont.pl:# than *twice* for longer keys, performance improvement over 32-bit
asm/ppc64-mont.pl:# ppc-mont.pl on 1.8GHz PPC970. However! This implementation utilizes
asm/ppc64-mont.pl:# even 64-bit integer operations and the trouble is that most PPC
asm/ppc64-mont.pl:# registers upon 32-bit signal delivery. They do preserve them upon
asm/ppc64-mont.pl:# context switch, but not signalling:-( This means that asynchronous
asm/ppc64-mont.pl:# that 512-bit key performance can be as low as 1/3 of expected one.
asm/ppc64-mont.pl:# GPRs by sticking to 32-bit integer operations...
asm/ppc64-mont.pl:# Remove above mentioned dependence on GPRs' upper halves in 32-bit
asm/ppc64-mont.pl:# *more* numerous... It's still "universally" faster than 32-bit
asm/ppc64-mont.pl:# ppc-mont.pl, but improvement coefficient is not as impressive
asm/ppc64-mont.pl:	# same as above, but 64-bit mnemonics...
asm/ppc64-mont.pl:( $xlate="${dir}ppc-xlate.pl" and -f $xlate ) or
asm/ppc64-mont.pl:( $xlate="${dir}../../perlasm/ppc-xlate.pl" and -f $xlate) or
asm/ppc64-mont.pl:die "can't locate ppc-xlate.pl";
asm/ppc64-mont.pl:# non-volatile registers
asm/ppc64-mont.pl:#    -----------
asm/ppc64-mont.pl:# sp----------->+-------------------------------+
asm/ppc64-mont.pl:#		+-------------------------------+
asm/ppc64-mont.pl:#   +64		+-------------------------------+
asm/ppc64-mont.pl:#		| 16 gpr<->fpr transfer zone	|
asm/ppc64-mont.pl:#   +16*8	+-------------------------------+
asm/ppc64-mont.pl:#		| __int64 tmp[-1]		|
asm/ppc64-mont.pl:#		+-------------------------------+
asm/ppc64-mont.pl:#   +(num+1)*8	+-------------------------------+
asm/ppc64-mont.pl:#   +X		+-------------------------------+
asm/ppc64-mont.pl:#		+-------------------------------+
asm/ppc64-mont.pl:#   -13*size_t	+-------------------------------+
asm/ppc64-mont.pl:#		| 13 saved gpr, r19-r31		|
asm/ppc64-mont.pl:#   -12*8	+-------------------------------+
asm/ppc64-mont.pl:#		| 12 saved fpr, f20-f31		|
asm/ppc64-mont.pl:#		+-------------------------------+
asm/ppc64-mont.pl:	bltlr-
asm/ppc64-mont.pl:	andi.	r0,$num,`16/$SIZE_T-1`		; $num has to be "even"
asm/ppc64-mont.pl:	bnelr-
asm/ppc64-mont.pl:	li	$i,-4096
asm/ppc64-mont.pl:	subf	$tp,$tp,$sp	; $sp-$tp
asm/ppc64-mont.pl:	subf	$tp,$sp,$tp	; $tp-$sp
asm/ppc64-mont.pl:	$PUSH	r19,`-12*8-13*$SIZE_T`($i)
asm/ppc64-mont.pl:	$PUSH	r20,`-12*8-12*$SIZE_T`($i)
asm/ppc64-mont.pl:	$PUSH	r21,`-12*8-11*$SIZE_T`($i)
asm/ppc64-mont.pl:	$PUSH	r22,`-12*8-10*$SIZE_T`($i)
asm/ppc64-mont.pl:	$PUSH	r23,`-12*8-9*$SIZE_T`($i)
asm/ppc64-mont.pl:	$PUSH	r24,`-12*8-8*$SIZE_T`($i)
asm/ppc64-mont.pl:	$PUSH	r25,`-12*8-7*$SIZE_T`($i)
asm/ppc64-mont.pl:	$PUSH	r26,`-12*8-6*$SIZE_T`($i)
asm/ppc64-mont.pl:	$PUSH	r27,`-12*8-5*$SIZE_T`($i)
asm/ppc64-mont.pl:	$PUSH	r28,`-12*8-4*$SIZE_T`($i)
asm/ppc64-mont.pl:	$PUSH	r29,`-12*8-3*$SIZE_T`($i)
asm/ppc64-mont.pl:	$PUSH	r30,`-12*8-2*$SIZE_T`($i)
asm/ppc64-mont.pl:	$PUSH	r31,`-12*8-1*$SIZE_T`($i)
asm/ppc64-mont.pl:	stfd	f20,`-12*8`($i)
asm/ppc64-mont.pl:	stfd	f21,`-11*8`($i)
asm/ppc64-mont.pl:	stfd	f22,`-10*8`($i)
asm/ppc64-mont.pl:	stfd	f23,`-9*8`($i)
asm/ppc64-mont.pl:	stfd	f24,`-8*8`($i)
asm/ppc64-mont.pl:	stfd	f25,`-7*8`($i)
asm/ppc64-mont.pl:	stfd	f26,`-6*8`($i)
asm/ppc64-mont.pl:	stfd	f27,`-5*8`($i)
asm/ppc64-mont.pl:	stfd	f28,`-4*8`($i)
asm/ppc64-mont.pl:	stfd	f29,`-3*8`($i)
asm/ppc64-mont.pl:	stfd	f30,`-2*8`($i)
asm/ppc64-mont.pl:	stfd	f31,`-1*8`($i)
asm/ppc64-mont.pl:	li	$i,-64
asm/ppc64-mont.pl:	addi	$nap_d,$nap_d,-8
asm/ppc64-mont.pl:	addi	$j,$j,-1
asm/ppc64-mont.pl:	addi	$tp,$sp,`$FRAME+$TRANSFER-8`
asm/ppc64-mont.pl:	; transfer bp[0] to FPU as 4x16-bit values
asm/ppc64-mont.pl:	; transfer (ap[0]*bp[0])*n0 to FPU as 4x16-bit values
asm/ppc64-mont.pl:	lwz	$t2,`12^$LITTLE_ENDIAN`($ap)	; load a[1] as 32-bit word pair
asm/ppc64-mont.pl:	lwz	$t4,`4^$LITTLE_ENDIAN`($np)	; load n[0] as 32-bit word pair
asm/ppc64-mont.pl:	lwz	$t6,`12^$LITTLE_ENDIAN`($np)	; load n[1] as 32-bit word pair
asm/ppc64-mont.pl:	; transfer bp[0] to FPU as 4x16-bit values
asm/ppc64-mont.pl:	std	$t0,`$FRAME+0`($sp)	; yes, std in 32-bit build
asm/ppc64-mont.pl:	; transfer (ap[0]*bp[0])*n0 to FPU as 4x16-bit values
asm/ppc64-mont.pl:	std	$t4,`$FRAME+32`($sp)	; yes, std in 32-bit build
asm/ppc64-mont.pl:	lwz	$t2,8($ap)		; load a[j..j+3] as 32-bit word pairs
asm/ppc64-mont.pl:	lwz	$t4,0($np)		; load n[j..j+3] as 32-bit word pairs
asm/ppc64-mont.pl:	std	$t0,`$FRAME+64`($sp)	; yes, std even in 32-bit build
asm/ppc64-mont.pl:	lwz	$t0,`4^$LITTLE_ENDIAN`($ap)	; load a[j] as 32-bit word pair
asm/ppc64-mont.pl:	lwz	$t2,`12^$LITTLE_ENDIAN`($ap)	; load a[j+1] as 32-bit word pair
asm/ppc64-mont.pl:	lwz	$t4,`4^$LITTLE_ENDIAN`($np)	; load n[j] as 32-bit word pair
asm/ppc64-mont.pl:	lwz	$t6,`12^$LITTLE_ENDIAN`($np)	; load n[j+1] as 32-bit word pair
asm/ppc64-mont.pl:	lwz	$t0,0($ap)		; load a[j..j+3] as 32-bit word pairs
asm/ppc64-mont.pl:	lwz	$t4,0($np)		; load n[j..j+3] as 32-bit word pairs
asm/ppc64-mont.pl:	std	$t0,`$FRAME+64`($sp)	; yes, std even in 32-bit build
asm/ppc64-mont.pl:	 std	$t0,8($tp)		; tp[j-1]
asm/ppc64-mont.pl:	 stw	$t0,12($tp)		; tp[j-1]
asm/ppc64-mont.pl:	std	$t0,8($tp)		; tp[j-1]
asm/ppc64-mont.pl:	std	$t6,8($tp)		; tp[num-1]
asm/ppc64-mont.pl:	 stw	$t0,12($tp)		; tp[j-1]
asm/ppc64-mont.pl:	stw	$t6,12($tp)		; tp[num-1]
asm/ppc64-mont.pl:	; transfer bp[i] to FPU as 4x16-bit values
asm/ppc64-mont.pl:	; transfer (ap[0]*bp[i]+tp[0])*n0 to FPU as 4x16-bit values
asm/ppc64-mont.pl:	; transfer bp[i] to FPU as 4x16-bit values
asm/ppc64-mont.pl:	std	$t0,`$FRAME+0`($sp)	; yes, std in 32-bit build
asm/ppc64-mont.pl:	; transfer (ap[0]*bp[i]+tp[0])*n0 to FPU as 4x16-bit values
asm/ppc64-mont.pl:	std	$t4,`$FRAME+32`($sp)	; yes, std in 32-bit build
asm/ppc64-mont.pl:	 std	$t3,-16($tp)		; tp[j-1]
asm/ppc64-mont.pl:	 std	$t5,-8($tp)		; tp[j]
asm/ppc64-mont.pl:	 stw	$t0,4($tp)		; tp[j-1]
asm/ppc64-mont.pl:	 stw	$t2,-4($tp)		; tp[j]
asm/ppc64-mont.pl:	 stw	$t0,-8($tp)
asm/ppc64-mont.pl:	std	$t3,-16($tp)		; tp[j-1]
asm/ppc64-mont.pl:	std	$t5,-8($tp)		; tp[j]
asm/ppc64-mont.pl:	std	$t6,0($tp)		; tp[num-1]
asm/ppc64-mont.pl:	 stw	$t0,4($tp)		; tp[j-1]
asm/ppc64-mont.pl:	 stw	$t2,-4($tp)		; tp[j]
asm/ppc64-mont.pl:	 stw	$t0,-8($tp)
asm/ppc64-mont.pl:	stw	$t6,4($tp)		; tp[num-1]
asm/ppc64-mont.pl:	blt-	Louter
asm/ppc64-mont.pl:	subfe	$t0,$t1,$t0	; tp[j]-np[j]
asm/ppc64-mont.pl:	subfe	$t2,$t3,$t2	; tp[j+1]-np[j+1]
asm/ppc64-mont.pl:	addi	$np,$np,-4
asm/ppc64-mont.pl:	addi	$rp,$rp,-4
asm/ppc64-mont.pl:Lsub:	lwz	$t0,12($tp)	; load tp[j..j+3] in 64-bit word order
asm/ppc64-mont.pl:	lwz	$t4,4($np)	; load np[j..j+3] in 32-bit word order
asm/ppc64-mont.pl:	subfe	$t4,$t4,$t0	; tp[j]-np[j]
asm/ppc64-mont.pl:	 stw	$t0,4($ap)	; save tp[j..j+3] in 32-bit word order
asm/ppc64-mont.pl:	subfe	$t5,$t5,$t1	; tp[j+1]-np[j+1]
asm/ppc64-mont.pl:	subfe	$t6,$t6,$t2	; tp[j+2]-np[j+2]
asm/ppc64-mont.pl:	subfe	$t7,$t7,$t3	; tp[j+3]-np[j+3]
asm/ppc64-mont.pl:	$POP	r19,`-12*8-13*$SIZE_T`($i)
asm/ppc64-mont.pl:	$POP	r20,`-12*8-12*$SIZE_T`($i)
asm/ppc64-mont.pl:	$POP	r21,`-12*8-11*$SIZE_T`($i)
asm/ppc64-mont.pl:	$POP	r22,`-12*8-10*$SIZE_T`($i)
asm/ppc64-mont.pl:	$POP	r23,`-12*8-9*$SIZE_T`($i)
asm/ppc64-mont.pl:	$POP	r24,`-12*8-8*$SIZE_T`($i)
asm/ppc64-mont.pl:	$POP	r25,`-12*8-7*$SIZE_T`($i)
asm/ppc64-mont.pl:	$POP	r26,`-12*8-6*$SIZE_T`($i)
asm/ppc64-mont.pl:	$POP	r27,`-12*8-5*$SIZE_T`($i)
asm/ppc64-mont.pl:	$POP	r28,`-12*8-4*$SIZE_T`($i)
asm/ppc64-mont.pl:	$POP	r29,`-12*8-3*$SIZE_T`($i)
asm/ppc64-mont.pl:	$POP	r30,`-12*8-2*$SIZE_T`($i)
asm/ppc64-mont.pl:	$POP	r31,`-12*8-1*$SIZE_T`($i)
asm/ppc64-mont.pl:	lfd	f20,`-12*8`($i)
asm/ppc64-mont.pl:	lfd	f21,`-11*8`($i)
asm/ppc64-mont.pl:	lfd	f22,`-10*8`($i)
asm/ppc64-mont.pl:	lfd	f23,`-9*8`($i)
asm/ppc64-mont.pl:	lfd	f24,`-8*8`($i)
asm/ppc64-mont.pl:	lfd	f25,`-7*8`($i)
asm/ppc64-mont.pl:	lfd	f26,`-6*8`($i)
asm/ppc64-mont.pl:	lfd	f27,`-5*8`($i)
asm/ppc64-mont.pl:	lfd	f28,`-4*8`($i)
asm/ppc64-mont.pl:	lfd	f29,`-3*8`($i)
asm/ppc64-mont.pl:	lfd	f30,`-2*8`($i)
asm/ppc64-mont.pl:	lfd	f31,`-1*8`($i)
asm/ppc64-mont.pl:.size	.$fname,.-.$fname
asm/rsaz-2k-avx512.pl:# Copyright 2020-2022 The OpenSSL Project Authors. All Rights Reserved.
asm/rsaz-2k-avx512.pl:# Implementation utilizes 256-bit (ymm) registers to avoid frequency scaling issues.
asm/rsaz-2k-avx512.pl:# IceLake-Client @ 1.3GHz
asm/rsaz-2k-avx512.pl:# |---------+----------------------+--------------+-------------|
asm/rsaz-2k-avx512.pl:# |         | OpenSSL 3.0.0-alpha9 | this         | Unit        |
asm/rsaz-2k-avx512.pl:# |---------+----------------------+--------------+-------------|
asm/rsaz-2k-avx512.pl:# |---------+----------------------+--------------+-------------|
asm/rsaz-2k-avx512.pl:( $xlate="${dir}x86_64-xlate.pl" and -f $xlate ) or
asm/rsaz-2k-avx512.pl:( $xlate="${dir}../../perlasm/x86_64-xlate.pl" and -f $xlate) or
asm/rsaz-2k-avx512.pl:die "can't locate x86_64-xlate.pl";
asm/rsaz-2k-avx512.pl:if (`$ENV{CC} -Wa,-v -c -o /dev/null -x assembler /dev/null 2>&1`
asm/rsaz-2k-avx512.pl:        =~ /GNU assembler version ([2-9]\.[0-9]+)/) {
asm/rsaz-2k-avx512.pl:       `nasm -v 2>&1` =~ /NASM version ([2-9]\.[0-9]+)(?:\.([0-9]+))?/) {
asm/rsaz-2k-avx512.pl:if (!$avx512 && `$ENV{CC} -v 2>&1` =~ /((?:clang|LLVM) version|.*based on LLVM) ([0-9]+\.[0-9]+)/) {
asm/rsaz-2k-avx512.pl:.type   ossl_rsaz_avx512ifma_eligible,\@abi-omnipotent
asm/rsaz-2k-avx512.pl:.size   ossl_rsaz_avx512ifma_eligible, .-ossl_rsaz_avx512ifma_eligible
asm/rsaz-2k-avx512.pl:# Almost Montgomery Multiplication (AMM) for 20-digit number in radix 2^52.
asm/rsaz-2k-avx512.pl:#   |res|, |a|, |b|, |m| are arrays of 20 64-bit qwords with 12 high bits zeroed.
asm/rsaz-2k-avx512.pl:#   |k0| is a Montgomery coefficient, which is here k0 = -1/m mod 2^64
asm/rsaz-2k-avx512.pl:# the next AMM iteration.  This post-condition is true, provided the correct
asm/rsaz-2k-avx512.pl:#     DOI: 10.1007/s13389-012-0031-5
asm/rsaz-2k-avx512.pl:#     DOI: 10.1007/3-540-36400-5_5
asm/rsaz-2k-avx512.pl:# _data_offset - offset in the |a| or |m| arrays pointing to the beginning
asm/rsaz-2k-avx512.pl:# _b_offset    - offset in the |b| array pointing to the next qword digit;
asm/rsaz-2k-avx512.pl:# Uses %r8-14,%e[bcd]x
asm/rsaz-2k-avx512.pl:    # Get mask of QWs which 52-bit parts overflow...
asm/rsaz-2k-avx512.pl:    # Merge 4-bit masks to 8-bit values to use add with carry.
asm/rsaz-2k-avx512.pl:    movq    \$0xfffffffffffff, $mask52       # 52-bit mask
asm/rsaz-2k-avx512.pl:.cfi_adjust_cfa_offset  -48
asm/rsaz-2k-avx512.pl:.size   ossl_rsaz_amm52x20_x1_ifma256, .-ossl_rsaz_amm52x20_x1_ifma256
asm/rsaz-2k-avx512.pl:# Dual Almost Montgomery Multiplication for 20-digit number in radix 2^52
asm/rsaz-2k-avx512.pl:    movq    \$0xfffffffffffff, $mask52       # 52-bit mask
asm/rsaz-2k-avx512.pl:    # 20*8 = offset of the next dimension in two-dimension array
asm/rsaz-2k-avx512.pl:.cfi_adjust_cfa_offset  -48
asm/rsaz-2k-avx512.pl:.size   ossl_rsaz_amm52x20_x2_ifma256, .-ossl_rsaz_amm52x20_x2_ifma256
asm/rsaz-2k-avx512.pl:#    i = 0..2^EXP_WIN_SIZE-1
asm/rsaz-2k-avx512.pl:.type   ossl_extract_multiplier_2x20_win5,\@abi-omnipotent
asm/rsaz-2k-avx512.pl:.size   ossl_extract_multiplier_2x20_win5, .-ossl_extract_multiplier_2x20_win5
asm/rsaz-2k-avx512.pl:.type   rsaz_def_handler,\@abi-omnipotent
asm/rsaz-2k-avx512.pl:    mov     120($context),%rax # pull context->Rax
asm/rsaz-2k-avx512.pl:    mov     248($context),%rbx # pull context->Rip
asm/rsaz-2k-avx512.pl:    mov     8($disp),%rsi      # disp->ImageBase
asm/rsaz-2k-avx512.pl:    mov     56($disp),%r11     # disp->HandlerData
asm/rsaz-2k-avx512.pl:    cmp     %r10,%rbx          # context->Rip<.Lprologue
asm/rsaz-2k-avx512.pl:    mov     152($context),%rax # pull context->Rsp
asm/rsaz-2k-avx512.pl:    cmp     %r10,%rbx          # context->Rip>=.Lepilogue
asm/rsaz-2k-avx512.pl:    mov     -8(%rax),%rbx
asm/rsaz-2k-avx512.pl:    mov     -16(%rax),%rbp
asm/rsaz-2k-avx512.pl:    mov     -24(%rax),%r12
asm/rsaz-2k-avx512.pl:    mov     -32(%rax),%r13
asm/rsaz-2k-avx512.pl:    mov     -40(%rax),%r14
asm/rsaz-2k-avx512.pl:    mov     -48(%rax),%r15
asm/rsaz-2k-avx512.pl:    mov     %rbx,144($context) # restore context->Rbx
asm/rsaz-2k-avx512.pl:    mov     %rbp,160($context) # restore context->Rbp
asm/rsaz-2k-avx512.pl:    mov     %r12,216($context) # restore context->R12
asm/rsaz-2k-avx512.pl:    mov     %r13,224($context) # restore context->R13
asm/rsaz-2k-avx512.pl:    mov     %r14,232($context) # restore context->R14
asm/rsaz-2k-avx512.pl:    mov     %r15,240($context) # restore context->R14
asm/rsaz-2k-avx512.pl:    mov     %rax,152($context) # restore context->Rsp
asm/rsaz-2k-avx512.pl:    mov     %rsi,168($context) # restore context->Rsi
asm/rsaz-2k-avx512.pl:    mov     %rdi,176($context) # restore context->Rdi
asm/rsaz-2k-avx512.pl:    mov     40($disp),%rdi     # disp->ContextRecord
asm/rsaz-2k-avx512.pl:    mov     8(%rsi),%rdx       # arg2, disp->ImageBase
asm/rsaz-2k-avx512.pl:    mov     0(%rsi),%r8        # arg3, disp->ControlPc
asm/rsaz-2k-avx512.pl:    mov     16(%rsi),%r9       # arg4, disp->FunctionEntry
asm/rsaz-2k-avx512.pl:    mov     40(%rsi),%r10      # disp->ContextRecord
asm/rsaz-2k-avx512.pl:    lea     56(%rsi),%r11      # &disp->HandlerData
asm/rsaz-2k-avx512.pl:    lea     24(%rsi),%r12      # &disp->EstablisherFrame
asm/rsaz-2k-avx512.pl:.size   rsaz_def_handler,.-rsaz_def_handler
asm/rsaz-2k-avx512.pl:.type   ossl_rsaz_avx512ifma_eligible,\@abi-omnipotent
asm/rsaz-2k-avx512.pl:.size   ossl_rsaz_avx512ifma_eligible, .-ossl_rsaz_avx512ifma_eligible
asm/rsaz-2k-avx512.pl:.type   ossl_rsaz_amm52x20_x1_ifma256,\@abi-omnipotent
asm/rsaz-2k-avx512.pl:.size   ossl_rsaz_amm52x20_x1_ifma256, .-ossl_rsaz_amm52x20_x1_ifma256
asm/rsaz-3k-avx512.pl:# Copyright 2021-2022 The OpenSSL Project Authors. All Rights Reserved.
asm/rsaz-3k-avx512.pl:# Implementation utilizes 256-bit (ymm) registers to avoid frequency scaling issues.
asm/rsaz-3k-avx512.pl:# IceLake-Client @ 1.3GHz
asm/rsaz-3k-avx512.pl:# |---------+-----------------------+---------------+-------------|
asm/rsaz-3k-avx512.pl:# |         | OpenSSL 3.0.0-alpha15 | this          | Unit        |
asm/rsaz-3k-avx512.pl:# |---------+-----------------------+---------------+-------------|
asm/rsaz-3k-avx512.pl:# |---------+-----------------------+---------------+-------------|
asm/rsaz-3k-avx512.pl:( $xlate="${dir}x86_64-xlate.pl" and -f $xlate ) or
asm/rsaz-3k-avx512.pl:( $xlate="${dir}../../perlasm/x86_64-xlate.pl" and -f $xlate) or
asm/rsaz-3k-avx512.pl:die "can't locate x86_64-xlate.pl";
asm/rsaz-3k-avx512.pl:if (`$ENV{CC} -Wa,-v -c -o /dev/null -x assembler /dev/null 2>&1`
asm/rsaz-3k-avx512.pl:        =~ /GNU assembler version ([2-9]\.[0-9]+)/) {
asm/rsaz-3k-avx512.pl:       `nasm -v 2>&1` =~ /NASM version ([2-9]\.[0-9]+)(?:\.([0-9]+))?/) {
asm/rsaz-3k-avx512.pl:if (!$avx512 && `$ENV{CC} -v 2>&1` =~ /((?:clang|LLVM) version|.*based on LLVM) ([0-9]+\.[0-9]+)/) {
asm/rsaz-3k-avx512.pl:# Almost Montgomery Multiplication (AMM) for 30-digit number in radix 2^52.
asm/rsaz-3k-avx512.pl:#   |res|, |a|, |b|, |m| are arrays of 32 64-bit qwords with 12 high bits zeroed
asm/rsaz-3k-avx512.pl:#   NOTE: the function uses zero-padded data - 2 high QWs is a padding.
asm/rsaz-3k-avx512.pl:#   |k0| is a Montgomery coefficient, which is here k0 = -1/m mod 2^64
asm/rsaz-3k-avx512.pl:# the next AMM iteration.  This post-condition is true, provided the correct
asm/rsaz-3k-avx512.pl:#     DOI: 10.1007/s13389-012-0031-5
asm/rsaz-3k-avx512.pl:#     DOI: 10.1007/3-540-36400-5_5
asm/rsaz-3k-avx512.pl:# _data_offset - offset in the |a| or |m| arrays pointing to the beginning
asm/rsaz-3k-avx512.pl:# _b_offset    - offset in the |b| array pointing to the next qword digit;
asm/rsaz-3k-avx512.pl:# Uses %r8-14,%e[abcd]x
asm/rsaz-3k-avx512.pl:    # Get mask of QWs whose 52-bit parts overflow
asm/rsaz-3k-avx512.pl:    # Get mask of QWs whose 52-bit parts saturated
asm/rsaz-3k-avx512.pl:    lea     -168(%rsp),%rsp                 # 16*10 + (8 bytes to get correct 16-byte SIMD alignment)
asm/rsaz-3k-avx512.pl:    vmovdqa64   %xmm6, `0*16`(%rsp)         # save non-volatile registers
asm/rsaz-3k-avx512.pl:    movq    \$0xfffffffffffff, $mask52       # 52-bit mask
asm/rsaz-3k-avx512.pl:.size   ossl_rsaz_amm52x30_x1_ifma256, .-ossl_rsaz_amm52x30_x1_ifma256
asm/rsaz-3k-avx512.pl:# Dual Almost Montgomery Multiplication for 30-digit number in radix 2^52
asm/rsaz-3k-avx512.pl:# NOTE: the function uses zero-padded data - 2 high QWs is a padding.
asm/rsaz-3k-avx512.pl:    lea     -168(%rsp),%rsp
asm/rsaz-3k-avx512.pl:    vmovdqa64   %xmm6, `0*16`(%rsp)        # save non-volatile registers
asm/rsaz-3k-avx512.pl:    movq    \$0xfffffffffffff, $mask52       # 52-bit mask
asm/rsaz-3k-avx512.pl:    # 32*8 = offset of the next dimension in two-dimension array
asm/rsaz-3k-avx512.pl:.size   ossl_rsaz_amm52x30_x2_ifma256, .-ossl_rsaz_amm52x30_x2_ifma256
asm/rsaz-3k-avx512.pl:#    i = 0..2^EXP_WIN_SIZE-1
asm/rsaz-3k-avx512.pl:.type   ossl_extract_multiplier_2x30_win5,\@abi-omnipotent
asm/rsaz-3k-avx512.pl:.size   ossl_extract_multiplier_2x30_win5, .-ossl_extract_multiplier_2x30_win5
asm/rsaz-3k-avx512.pl:.type   rsaz_avx_handler,\@abi-omnipotent
asm/rsaz-3k-avx512.pl:    mov     120($context),%rax # pull context->Rax
asm/rsaz-3k-avx512.pl:    mov     248($context),%rbx # pull context->Rip
asm/rsaz-3k-avx512.pl:    mov     8($disp),%rsi      # disp->ImageBase
asm/rsaz-3k-avx512.pl:    mov     56($disp),%r11     # disp->HandlerData
asm/rsaz-3k-avx512.pl:    cmp     %r10,%rbx          # context->Rip<.Lprologue
asm/rsaz-3k-avx512.pl:    cmp     %r10,%rbx          # context->Rip>=.Lepilogue
asm/rsaz-3k-avx512.pl:    mov     152($context),%rax # pull context->Rsp
asm/rsaz-3k-avx512.pl:    mov     -8(%rax),%rbx
asm/rsaz-3k-avx512.pl:    mov     -16(%rax),%rbp
asm/rsaz-3k-avx512.pl:    mov     -24(%rax),%r12
asm/rsaz-3k-avx512.pl:    mov     -32(%rax),%r13
asm/rsaz-3k-avx512.pl:    mov     -40(%rax),%r14
asm/rsaz-3k-avx512.pl:    mov     -48(%rax),%r15
asm/rsaz-3k-avx512.pl:    mov     %rbx,144($context) # restore context->Rbx
asm/rsaz-3k-avx512.pl:    mov     %rbp,160($context) # restore context->Rbp
asm/rsaz-3k-avx512.pl:    mov     %r12,216($context) # restore context->R12
asm/rsaz-3k-avx512.pl:    mov     %r13,224($context) # restore context->R13
asm/rsaz-3k-avx512.pl:    mov     %r14,232($context) # restore context->R14
asm/rsaz-3k-avx512.pl:    mov     %r15,240($context) # restore context->R14
asm/rsaz-3k-avx512.pl:    mov     %rax,152($context) # restore context->Rsp
asm/rsaz-3k-avx512.pl:    mov     %rsi,168($context) # restore context->Rsi
asm/rsaz-3k-avx512.pl:    mov     %rdi,176($context) # restore context->Rdi
asm/rsaz-3k-avx512.pl:    mov     40($disp),%rdi     # disp->ContextRecord
asm/rsaz-3k-avx512.pl:    mov     8(%rsi),%rdx       # arg2, disp->ImageBase
asm/rsaz-3k-avx512.pl:    mov     0(%rsi),%r8        # arg3, disp->ControlPc
asm/rsaz-3k-avx512.pl:    mov     16(%rsi),%r9       # arg4, disp->FunctionEntry
asm/rsaz-3k-avx512.pl:    mov     40(%rsi),%r10      # disp->ContextRecord
asm/rsaz-3k-avx512.pl:    lea     56(%rsi),%r11      # &disp->HandlerData
asm/rsaz-3k-avx512.pl:    lea     24(%rsi),%r12      # &disp->EstablisherFrame
asm/rsaz-3k-avx512.pl:.size   rsaz_avx_handler,.-rsaz_avx_handler
asm/rsaz-3k-avx512.pl:.type   ossl_rsaz_amm52x30_x1_ifma256,\@abi-omnipotent
asm/rsaz-3k-avx512.pl:.size   ossl_rsaz_amm52x30_x1_ifma256, .-ossl_rsaz_amm52x30_x1_ifma256
asm/rsaz-4k-avx512.pl:# Copyright 2021-2022 The OpenSSL Project Authors. All Rights Reserved.
asm/rsaz-4k-avx512.pl:# Implementation utilizes 256-bit (ymm) registers to avoid frequency scaling issues.
asm/rsaz-4k-avx512.pl:# IceLake-Client @ 1.3GHz
asm/rsaz-4k-avx512.pl:# |---------+-----------------------+---------------+-------------|
asm/rsaz-4k-avx512.pl:# |         | OpenSSL 3.0.0-alpha15 | this          | Unit        |
asm/rsaz-4k-avx512.pl:# |---------+-----------------------+---------------+-------------|
asm/rsaz-4k-avx512.pl:# |---------+-----------------------+---------------+-------------|
asm/rsaz-4k-avx512.pl:( $xlate="${dir}x86_64-xlate.pl" and -f $xlate ) or
asm/rsaz-4k-avx512.pl:( $xlate="${dir}../../perlasm/x86_64-xlate.pl" and -f $xlate) or
asm/rsaz-4k-avx512.pl:die "can't locate x86_64-xlate.pl";
asm/rsaz-4k-avx512.pl:if (`$ENV{CC} -Wa,-v -c -o /dev/null -x assembler /dev/null 2>&1`
asm/rsaz-4k-avx512.pl:        =~ /GNU assembler version ([2-9]\.[0-9]+)/) {
asm/rsaz-4k-avx512.pl:       `nasm -v 2>&1` =~ /NASM version ([2-9]\.[0-9]+)(?:\.([0-9]+))?/) {
asm/rsaz-4k-avx512.pl:if (!$avx512 && `$ENV{CC} -v 2>&1` =~ /((?:clang|LLVM) version|.*based on LLVM) ([0-9]+\.[0-9]+)/) {
asm/rsaz-4k-avx512.pl:# Almost Montgomery Multiplication (AMM) for 40-digit number in radix 2^52.
asm/rsaz-4k-avx512.pl:#   |res|, |a|, |b|, |m| are arrays of 40 64-bit qwords with 12 high bits zeroed.
asm/rsaz-4k-avx512.pl:#   |k0| is a Montgomery coefficient, which is here k0 = -1/m mod 2^64
asm/rsaz-4k-avx512.pl:# the next AMM iteration.  This post-condition is true, provided the correct
asm/rsaz-4k-avx512.pl:#     DOI: 10.1007/s13389-012-0031-5
asm/rsaz-4k-avx512.pl:#     DOI: 10.1007/3-540-36400-5_5
asm/rsaz-4k-avx512.pl:# _data_offset - offset in the |a| or |m| arrays pointing to the beginning
asm/rsaz-4k-avx512.pl:# _b_offset    - offset in the |b| array pointing to the next qword digit;
asm/rsaz-4k-avx512.pl:# Uses %r8-14,%e[abcd]x
asm/rsaz-4k-avx512.pl:    # Get mask of QWs whose 52-bit parts overflow
asm/rsaz-4k-avx512.pl:    # Get mask of QWs whose 52-bit parts saturated
asm/rsaz-4k-avx512.pl:    lea     -168(%rsp),%rsp                 # 16*10 + (8 bytes to get correct 16-byte SIMD alignment)
asm/rsaz-4k-avx512.pl:    vmovdqa64   %xmm6, `0*16`(%rsp)         # save non-volatile registers
asm/rsaz-4k-avx512.pl:    movq    \$0xfffffffffffff, $mask52       # 52-bit mask
asm/rsaz-4k-avx512.pl:.size   ossl_rsaz_amm52x40_x1_ifma256, .-ossl_rsaz_amm52x40_x1_ifma256
asm/rsaz-4k-avx512.pl:# Dual Almost Montgomery Multiplication for 40-digit number in radix 2^52
asm/rsaz-4k-avx512.pl:    lea     -168(%rsp),%rsp
asm/rsaz-4k-avx512.pl:    vmovdqa64   %xmm6, `0*16`(%rsp)        # save non-volatile registers
asm/rsaz-4k-avx512.pl:    movq    \$0xfffffffffffff, $mask52       # 52-bit mask
asm/rsaz-4k-avx512.pl:    # 40*8 = offset of the next dimension in two-dimension array
asm/rsaz-4k-avx512.pl:.size   ossl_rsaz_amm52x40_x2_ifma256, .-ossl_rsaz_amm52x40_x2_ifma256
asm/rsaz-4k-avx512.pl:#    i = 0..2^EXP_WIN_SIZE-1
asm/rsaz-4k-avx512.pl:.type   ossl_extract_multiplier_2x40_win5,\@abi-omnipotent
asm/rsaz-4k-avx512.pl:.size   ossl_extract_multiplier_2x40_win5, .-ossl_extract_multiplier_2x40_win5
asm/rsaz-4k-avx512.pl:.type   rsaz_avx_handler,\@abi-omnipotent
asm/rsaz-4k-avx512.pl:    mov     120($context),%rax # pull context->Rax
asm/rsaz-4k-avx512.pl:    mov     248($context),%rbx # pull context->Rip
asm/rsaz-4k-avx512.pl:    mov     8($disp),%rsi      # disp->ImageBase
asm/rsaz-4k-avx512.pl:    mov     56($disp),%r11     # disp->HandlerData
asm/rsaz-4k-avx512.pl:    cmp     %r10,%rbx          # context->Rip<.Lprologue
asm/rsaz-4k-avx512.pl:    cmp     %r10,%rbx          # context->Rip>=.Lepilogue
asm/rsaz-4k-avx512.pl:    mov     152($context),%rax # pull context->Rsp
asm/rsaz-4k-avx512.pl:    mov     -8(%rax),%rbx
asm/rsaz-4k-avx512.pl:    mov     -16(%rax),%rbp
asm/rsaz-4k-avx512.pl:    mov     -24(%rax),%r12
asm/rsaz-4k-avx512.pl:    mov     -32(%rax),%r13
asm/rsaz-4k-avx512.pl:    mov     -40(%rax),%r14
asm/rsaz-4k-avx512.pl:    mov     -48(%rax),%r15
asm/rsaz-4k-avx512.pl:    mov     %rbx,144($context) # restore context->Rbx
asm/rsaz-4k-avx512.pl:    mov     %rbp,160($context) # restore context->Rbp
asm/rsaz-4k-avx512.pl:    mov     %r12,216($context) # restore context->R12
asm/rsaz-4k-avx512.pl:    mov     %r13,224($context) # restore context->R13
asm/rsaz-4k-avx512.pl:    mov     %r14,232($context) # restore context->R14
asm/rsaz-4k-avx512.pl:    mov     %r15,240($context) # restore context->R14
asm/rsaz-4k-avx512.pl:    mov     %rax,152($context) # restore context->Rsp
asm/rsaz-4k-avx512.pl:    mov     %rsi,168($context) # restore context->Rsi
asm/rsaz-4k-avx512.pl:    mov     %rdi,176($context) # restore context->Rdi
asm/rsaz-4k-avx512.pl:    mov     40($disp),%rdi     # disp->ContextRecord
asm/rsaz-4k-avx512.pl:    mov     8(%rsi),%rdx       # arg2, disp->ImageBase
asm/rsaz-4k-avx512.pl:    mov     0(%rsi),%r8        # arg3, disp->ControlPc
asm/rsaz-4k-avx512.pl:    mov     16(%rsi),%r9       # arg4, disp->FunctionEntry
asm/rsaz-4k-avx512.pl:    mov     40(%rsi),%r10      # disp->ContextRecord
asm/rsaz-4k-avx512.pl:    lea     56(%rsi),%r11      # &disp->HandlerData
asm/rsaz-4k-avx512.pl:    lea     24(%rsi),%r12      # &disp->EstablisherFrame
asm/rsaz-4k-avx512.pl:.size   rsaz_avx_handler,.-rsaz_avx_handler
asm/rsaz-4k-avx512.pl:.type   ossl_rsaz_amm52x40_x1_ifma256,\@abi-omnipotent
asm/rsaz-4k-avx512.pl:.size   ossl_rsaz_amm52x40_x1_ifma256, .-ossl_rsaz_amm52x40_x1_ifma256
asm/rsaz-avx2.pl:# Copyright 2013-2020 The OpenSSL Project Authors. All Rights Reserved.
asm/rsaz-avx2.pl:#     F. Ozbudak and F. Rodriguez-Henriquez (Eds.): WAIFI 2012, LNCS 7369,
asm/rsaz-avx2.pl:#     pp. 119?135, 2012. Springer-Verlag Berlin Heidelberg 2012
asm/rsaz-avx2.pl:#     Exponentiation", Journal of Cryptographic Engineering 2:31-43 (2012).
asm/rsaz-avx2.pl:# [3] S. Gueron, V. Krasnov: "Speeding up Big-numbers Squaring",IEEE
asm/rsaz-avx2.pl:#     New Generations (ITNG 2012), pp.821-823 (2012)
asm/rsaz-avx2.pl:#     resistant 1024-bit modular exponentiation, for optimizing RSA2048
asm/rsaz-avx2.pl:( $xlate="${dir}x86_64-xlate.pl" and -f $xlate ) or
asm/rsaz-avx2.pl:( $xlate="${dir}../../perlasm/x86_64-xlate.pl" and -f $xlate) or
asm/rsaz-avx2.pl:die "can't locate x86_64-xlate.pl";
asm/rsaz-avx2.pl:if (`$ENV{CC} -Wa,-v -c -o /dev/null -x assembler /dev/null 2>&1`
asm/rsaz-avx2.pl:		=~ /GNU assembler version ([2-9]\.[0-9]+)/) {
asm/rsaz-avx2.pl:	    `nasm -v 2>&1` =~ /NASM version ([2-9]\.[0-9]+)/) {
asm/rsaz-avx2.pl:	    `ml64 2>&1` =~ /Version ([0-9]+)\./) {
asm/rsaz-avx2.pl:if (!$avx && `$ENV{CC} -v 2>&1` =~ /((?:clang|LLVM) version|based on LLVM) ([0-9]+)\.([0-9]+)/) {
asm/rsaz-avx2.pl:	my $ver = $2 + $3/100.0;	# 3.1->3.01, 3.10->3.10
asm/rsaz-avx2.pl:	lea	-0xa8(%rsp),%rsp
asm/rsaz-avx2.pl:	vmovaps	%xmm6,-0xd8(%rax)
asm/rsaz-avx2.pl:	vmovaps	%xmm7,-0xc8(%rax)
asm/rsaz-avx2.pl:	vmovaps	%xmm8,-0xb8(%rax)
asm/rsaz-avx2.pl:	vmovaps	%xmm9,-0xa8(%rax)
asm/rsaz-avx2.pl:	vmovaps	%xmm10,-0x98(%rax)
asm/rsaz-avx2.pl:	vmovaps	%xmm11,-0x88(%rax)
asm/rsaz-avx2.pl:	vmovaps	%xmm12,-0x78(%rax)
asm/rsaz-avx2.pl:	vmovaps	%xmm13,-0x68(%rax)
asm/rsaz-avx2.pl:	vmovaps	%xmm14,-0x58(%rax)
asm/rsaz-avx2.pl:	vmovaps	%xmm15,-0x48(%rax)
asm/rsaz-avx2.pl:	sub	\$-128, $rp			# size optimization
asm/rsaz-avx2.pl:	sub	\$-128, $ap
asm/rsaz-avx2.pl:	sub	\$-128, $np
asm/rsaz-avx2.pl:	# unaligned 256-bit load that crosses page boundary can
asm/rsaz-avx2.pl:	vmovdqu		32*0-128($np), $ACC0
asm/rsaz-avx2.pl:	and		\$-2048, %rsp
asm/rsaz-avx2.pl:	vmovdqu		32*1-128($np), $ACC1
asm/rsaz-avx2.pl:	vmovdqu		32*2-128($np), $ACC2
asm/rsaz-avx2.pl:	vmovdqu		32*3-128($np), $ACC3
asm/rsaz-avx2.pl:	vmovdqu		32*4-128($np), $ACC4
asm/rsaz-avx2.pl:	vmovdqu		32*5-128($np), $ACC5
asm/rsaz-avx2.pl:	vmovdqu		32*6-128($np), $ACC6
asm/rsaz-avx2.pl:	vmovdqu		32*7-128($np), $ACC7
asm/rsaz-avx2.pl:	vmovdqu		32*8-128($np), $ACC8
asm/rsaz-avx2.pl:	vmovdqu		$ACC0, 32*0-128($np)
asm/rsaz-avx2.pl:	vmovdqu		$ACC1, 32*1-128($np)
asm/rsaz-avx2.pl:	vmovdqu		$ACC2, 32*2-128($np)
asm/rsaz-avx2.pl:	vmovdqu		$ACC3, 32*3-128($np)
asm/rsaz-avx2.pl:	vmovdqu		$ACC4, 32*4-128($np)
asm/rsaz-avx2.pl:	vmovdqu		$ACC5, 32*5-128($np)
asm/rsaz-avx2.pl:	vmovdqu		$ACC6, 32*6-128($np)
asm/rsaz-avx2.pl:	vmovdqu		$ACC7, 32*7-128($np)
asm/rsaz-avx2.pl:	vmovdqu		$ACC8, 32*8-128($np)
asm/rsaz-avx2.pl:	vmovdqu		$ACC9, 32*9-128($np)	# $ACC9 is zero
asm/rsaz-avx2.pl:	and		\$-1024, %rsp
asm/rsaz-avx2.pl:	vmovdqu		32*1-128($ap), $ACC1
asm/rsaz-avx2.pl:	vmovdqu		32*2-128($ap), $ACC2
asm/rsaz-avx2.pl:	vmovdqu		32*3-128($ap), $ACC3
asm/rsaz-avx2.pl:	vmovdqu		32*4-128($ap), $ACC4
asm/rsaz-avx2.pl:	vmovdqu		32*5-128($ap), $ACC5
asm/rsaz-avx2.pl:	vmovdqu		32*6-128($ap), $ACC6
asm/rsaz-avx2.pl:	vmovdqu		32*7-128($ap), $ACC7
asm/rsaz-avx2.pl:	vmovdqu		32*8-128($ap), $ACC8
asm/rsaz-avx2.pl:	# "Speeding up Big-Number Squaring", so start by calculating
asm/rsaz-avx2.pl:	 vpbroadcastq	32*0-128($ap), $B1
asm/rsaz-avx2.pl:	vmovdqa		$ACC1, 32*0-128($aap)
asm/rsaz-avx2.pl:	vmovdqa		$ACC2, 32*1-128($aap)
asm/rsaz-avx2.pl:	vmovdqa		$ACC3, 32*2-128($aap)
asm/rsaz-avx2.pl:	vmovdqa		$ACC4, 32*3-128($aap)
asm/rsaz-avx2.pl:	vmovdqa		$ACC5, 32*4-128($aap)
asm/rsaz-avx2.pl:	vmovdqa		$ACC6, 32*5-128($aap)
asm/rsaz-avx2.pl:	vmovdqa		$ACC7, 32*6-128($aap)
asm/rsaz-avx2.pl:	vmovdqa		$ACC8, 32*7-128($aap)
asm/rsaz-avx2.pl:	vpmuludq	32*0-128($ap), $B1, $ACC0
asm/rsaz-avx2.pl:	 vpbroadcastq	32*1-128($ap), $B2
asm/rsaz-avx2.pl:	 vmovdqu	$ACC9, 32*9-192($tp0)	# zero upper half
asm/rsaz-avx2.pl:	 vmovdqu	$ACC9, 32*10-448($tp1)
asm/rsaz-avx2.pl:	 vmovdqu	$ACC9, 32*11-448($tp1)
asm/rsaz-avx2.pl:	 vmovdqu	$ACC9, 32*12-448($tp1)
asm/rsaz-avx2.pl:	 vmovdqu	$ACC9, 32*13-448($tp1)
asm/rsaz-avx2.pl:	 vmovdqu	$ACC9, 32*14-448($tp1)
asm/rsaz-avx2.pl:	 vmovdqu	$ACC9, 32*15-448($tp1)
asm/rsaz-avx2.pl:	 vmovdqu	$ACC9, 32*16-448($tp1)
asm/rsaz-avx2.pl:	 vpbroadcastq	32*2-128($ap), $B1
asm/rsaz-avx2.pl:	 vmovdqu	$ACC9, 32*17-448($tp1)
asm/rsaz-avx2.pl:	 vpbroadcastq	32*1-128($tpa), $B2
asm/rsaz-avx2.pl:	vpmuludq	32*0-128($ap), $B1, $ACC0
asm/rsaz-avx2.pl:	vpaddq		32*0-192($tp0), $ACC0, $ACC0
asm/rsaz-avx2.pl:	vpmuludq	32*0-128($aap), $B1, $ACC1
asm/rsaz-avx2.pl:	vpaddq		32*1-192($tp0), $ACC1, $ACC1
asm/rsaz-avx2.pl:	vpmuludq	32*1-128($aap), $B1, $ACC2
asm/rsaz-avx2.pl:	vpaddq		32*2-192($tp0), $ACC2, $ACC2
asm/rsaz-avx2.pl:	vpmuludq	32*2-128($aap), $B1, $ACC3
asm/rsaz-avx2.pl:	vpaddq		32*3-192($tp0), $ACC3, $ACC3
asm/rsaz-avx2.pl:	vpmuludq	32*3-128($aap), $B1, $ACC4
asm/rsaz-avx2.pl:	vpaddq		32*4-192($tp0), $ACC4, $ACC4
asm/rsaz-avx2.pl:	vpmuludq	32*4-128($aap), $B1, $ACC5
asm/rsaz-avx2.pl:	vpaddq		32*5-192($tp0), $ACC5, $ACC5
asm/rsaz-avx2.pl:	vpmuludq	32*5-128($aap), $B1, $ACC6
asm/rsaz-avx2.pl:	vpaddq		32*6-192($tp0), $ACC6, $ACC6
asm/rsaz-avx2.pl:	vpmuludq	32*6-128($aap), $B1, $ACC7
asm/rsaz-avx2.pl:	vpaddq		32*7-192($tp0), $ACC7, $ACC7
asm/rsaz-avx2.pl:	vpmuludq	32*7-128($aap), $B1, $ACC8
asm/rsaz-avx2.pl:	 vpbroadcastq	32*2-128($tpa), $B1
asm/rsaz-avx2.pl:	vpaddq		32*8-192($tp0), $ACC8, $ACC8
asm/rsaz-avx2.pl:	vmovdqu		$ACC0, 32*0-192($tp0)
asm/rsaz-avx2.pl:	vmovdqu		$ACC1, 32*1-192($tp0)
asm/rsaz-avx2.pl:	vpmuludq	32*1-128($ap), $B2, $TEMP0
asm/rsaz-avx2.pl:	vpmuludq	32*1-128($aap), $B2, $TEMP1
asm/rsaz-avx2.pl:	vpmuludq	32*2-128($aap), $B2, $TEMP2
asm/rsaz-avx2.pl:	vpmuludq	32*3-128($aap), $B2, $TEMP0
asm/rsaz-avx2.pl:	vpmuludq	32*4-128($aap), $B2, $TEMP1
asm/rsaz-avx2.pl:	vpmuludq	32*5-128($aap), $B2, $TEMP2
asm/rsaz-avx2.pl:	vpmuludq	32*6-128($aap), $B2, $TEMP0
asm/rsaz-avx2.pl:	vpmuludq	32*7-128($aap), $B2, $ACC0
asm/rsaz-avx2.pl:	 vpbroadcastq	32*3-128($tpa), $B2
asm/rsaz-avx2.pl:	vpaddq		32*9-192($tp0), $ACC0, $ACC0
asm/rsaz-avx2.pl:	vmovdqu		$ACC2, 32*2-192($tp0)
asm/rsaz-avx2.pl:	vmovdqu		$ACC3, 32*3-192($tp0)
asm/rsaz-avx2.pl:	vpmuludq	32*2-128($ap), $B1, $TEMP2
asm/rsaz-avx2.pl:	vpmuludq	32*2-128($aap), $B1, $TEMP0
asm/rsaz-avx2.pl:	vpmuludq	32*3-128($aap), $B1, $TEMP1
asm/rsaz-avx2.pl:	vpmuludq	32*4-128($aap), $B1, $TEMP2
asm/rsaz-avx2.pl:	vpmuludq	32*5-128($aap), $B1, $TEMP0
asm/rsaz-avx2.pl:	vpmuludq	32*6-128($aap), $B1, $TEMP1
asm/rsaz-avx2.pl:	vpmuludq	32*7-128($aap), $B1, $ACC1
asm/rsaz-avx2.pl:	 vpbroadcastq	32*4-128($tpa), $B1
asm/rsaz-avx2.pl:	vpaddq		32*10-448($tp1), $ACC1, $ACC1
asm/rsaz-avx2.pl:	vmovdqu		$ACC4, 32*4-192($tp0)
asm/rsaz-avx2.pl:	vmovdqu		$ACC5, 32*5-192($tp0)
asm/rsaz-avx2.pl:	vpmuludq	32*3-128($ap), $B2, $TEMP0
asm/rsaz-avx2.pl:	vpmuludq	32*3-128($aap), $B2, $TEMP1
asm/rsaz-avx2.pl:	vpmuludq	32*4-128($aap), $B2, $TEMP2
asm/rsaz-avx2.pl:	vpmuludq	32*5-128($aap), $B2, $TEMP0
asm/rsaz-avx2.pl:	vpmuludq	32*6-128($aap), $B2, $TEMP1
asm/rsaz-avx2.pl:	vpmuludq	32*7-128($aap), $B2, $ACC2
asm/rsaz-avx2.pl:	 vpbroadcastq	32*5-128($tpa), $B2
asm/rsaz-avx2.pl:	vpaddq		32*11-448($tp1), $ACC2, $ACC2
asm/rsaz-avx2.pl:	vmovdqu		$ACC6, 32*6-192($tp0)
asm/rsaz-avx2.pl:	vmovdqu		$ACC7, 32*7-192($tp0)
asm/rsaz-avx2.pl:	vpmuludq	32*4-128($ap), $B1, $TEMP0
asm/rsaz-avx2.pl:	vpmuludq	32*4-128($aap), $B1, $TEMP1
asm/rsaz-avx2.pl:	vpmuludq	32*5-128($aap), $B1, $TEMP2
asm/rsaz-avx2.pl:	vpmuludq	32*6-128($aap), $B1, $TEMP0
asm/rsaz-avx2.pl:	vpmuludq	32*7-128($aap), $B1, $ACC3
asm/rsaz-avx2.pl:	 vpbroadcastq	32*6-128($tpa), $B1
asm/rsaz-avx2.pl:	vpaddq		32*12-448($tp1), $ACC3, $ACC3
asm/rsaz-avx2.pl:	vmovdqu		$ACC8, 32*8-192($tp0)
asm/rsaz-avx2.pl:	vmovdqu		$ACC0, 32*9-192($tp0)
asm/rsaz-avx2.pl:	vpmuludq	32*5-128($ap), $B2, $TEMP2
asm/rsaz-avx2.pl:	vpmuludq	32*5-128($aap), $B2, $TEMP0
asm/rsaz-avx2.pl:	vpmuludq	32*6-128($aap), $B2, $TEMP1
asm/rsaz-avx2.pl:	vpmuludq	32*7-128($aap), $B2, $ACC4
asm/rsaz-avx2.pl:	 vpbroadcastq	32*7-128($tpa), $B2
asm/rsaz-avx2.pl:	vpaddq		32*13-448($tp1), $ACC4, $ACC4
asm/rsaz-avx2.pl:	vmovdqu		$ACC1, 32*10-448($tp1)
asm/rsaz-avx2.pl:	vmovdqu		$ACC2, 32*11-448($tp1)
asm/rsaz-avx2.pl:	vpmuludq	32*6-128($ap), $B1, $TEMP0
asm/rsaz-avx2.pl:	vpmuludq	32*6-128($aap), $B1, $TEMP1
asm/rsaz-avx2.pl:	 vpbroadcastq	32*8-128($tpa), $ACC0		# borrow $ACC0 for $B1
asm/rsaz-avx2.pl:	vpmuludq	32*7-128($aap), $B1, $ACC5
asm/rsaz-avx2.pl:	 vpbroadcastq	32*0+8-128($tpa), $B1		# for next iteration
asm/rsaz-avx2.pl:	vpaddq		32*14-448($tp1), $ACC5, $ACC5
asm/rsaz-avx2.pl:	vmovdqu		$ACC3, 32*12-448($tp1)
asm/rsaz-avx2.pl:	vmovdqu		$ACC4, 32*13-448($tp1)
asm/rsaz-avx2.pl:	vpmuludq	32*7-128($ap), $B2, $TEMP0
asm/rsaz-avx2.pl:	vpmuludq	32*7-128($aap), $B2, $ACC6
asm/rsaz-avx2.pl:	vpaddq		32*15-448($tp1), $ACC6, $ACC6
asm/rsaz-avx2.pl:	vpmuludq	32*8-128($ap), $ACC0, $ACC7
asm/rsaz-avx2.pl:	vmovdqu		$ACC5, 32*14-448($tp1)
asm/rsaz-avx2.pl:	vpaddq		32*16-448($tp1), $ACC7, $ACC7
asm/rsaz-avx2.pl:	vmovdqu		$ACC6, 32*15-448($tp1)
asm/rsaz-avx2.pl:	vmovdqu		$ACC7, 32*16-448($tp1)
asm/rsaz-avx2.pl:	# we need to fix indices 32-39 to avoid overflow
asm/rsaz-avx2.pl:	vmovdqu		32*8(%rsp), $ACC8		# 32*8-192($tp0),
asm/rsaz-avx2.pl:	vmovdqu		32*9(%rsp), $ACC1		# 32*9-192($tp0)
asm/rsaz-avx2.pl:	vmovdqu		32*10(%rsp), $ACC2		# 32*10-192($tp0)
asm/rsaz-avx2.pl:	vmovdqu		$ACC1, 32*9-192($tp0)
asm/rsaz-avx2.pl:	vmovdqu		$ACC2, 32*10-192($tp0)
asm/rsaz-avx2.pl:	vmovdqu	32*2-192($tp0), $ACC2
asm/rsaz-avx2.pl:	vmovdqu	32*3-192($tp0), $ACC3
asm/rsaz-avx2.pl:	vmovdqu	32*4-192($tp0), $ACC4
asm/rsaz-avx2.pl:	vmovdqu	32*5-192($tp0), $ACC5
asm/rsaz-avx2.pl:	vmovdqu	32*6-192($tp0), $ACC6
asm/rsaz-avx2.pl:	vmovdqu	32*7-192($tp0), $ACC7
asm/rsaz-avx2.pl:	imulq	-128($np), %rax
asm/rsaz-avx2.pl:	imulq	8-128($np), %rax
asm/rsaz-avx2.pl:	imulq	16-128($np), %rax
asm/rsaz-avx2.pl:	imulq	24-128($np), %rdx
asm/rsaz-avx2.pl:	vpmuludq	32*1-128($np), $Y1, $TEMP0
asm/rsaz-avx2.pl:	 imulq	-128($np), %rax
asm/rsaz-avx2.pl:	vpmuludq	32*2-128($np), $Y1, $TEMP1
asm/rsaz-avx2.pl:	 imulq	8-128($np), %rax
asm/rsaz-avx2.pl:	vpmuludq	32*3-128($np), $Y1, $TEMP2
asm/rsaz-avx2.pl:	 imulq	16-128($np), %rax
asm/rsaz-avx2.pl:	vpmuludq	32*4-128($np), $Y1, $TEMP0
asm/rsaz-avx2.pl:	vpmuludq	32*5-128($np), $Y1, $TEMP1
asm/rsaz-avx2.pl:	vpmuludq	32*6-128($np), $Y1, $TEMP2
asm/rsaz-avx2.pl:	vpmuludq	32*7-128($np), $Y1, $TEMP0
asm/rsaz-avx2.pl:	vpmuludq	32*8-128($np), $Y1, $TEMP1
asm/rsaz-avx2.pl:	 #vmovdqu	32*1-8-128($np), $TEMP2		# moved below
asm/rsaz-avx2.pl:	 #vmovdqu	32*2-8-128($np), $TEMP0		# moved below
asm/rsaz-avx2.pl:	vpmuludq	32*1-8-128($np), $Y2, $TEMP2	# see above
asm/rsaz-avx2.pl:	vmovdqu		32*3-8-128($np), $TEMP1
asm/rsaz-avx2.pl:	 imulq	-128($np), %rax
asm/rsaz-avx2.pl:	vpmuludq	32*2-8-128($np), $Y2, $TEMP0	# see above
asm/rsaz-avx2.pl:	vmovdqu		32*4-8-128($np), $TEMP2
asm/rsaz-avx2.pl:	 imulq	8-128($np), %rax
asm/rsaz-avx2.pl:	vmovdqu		32*5-8-128($np), $TEMP0
asm/rsaz-avx2.pl:	vmovdqu		32*6-8-128($np), $TEMP1
asm/rsaz-avx2.pl:	.byte	0xc4,0x41,0x7e,0x6f,0x9d,0x58,0x00,0x00,0x00	# vmovdqu		32*7-8-128($np), $TEMP2
asm/rsaz-avx2.pl:	vmovdqu		32*8-8-128($np), $TEMP0
asm/rsaz-avx2.pl:	vmovdqu		32*9-8-128($np), $ACC9
asm/rsaz-avx2.pl:	 imulq	-128($np), %rax
asm/rsaz-avx2.pl:	 vmovdqu	32*1-16-128($np), $TEMP1
asm/rsaz-avx2.pl:	 vmovdqu	32*2-16-128($np), $TEMP2
asm/rsaz-avx2.pl:	 vmovdqu	32*1-24-128($np), $ACC0
asm/rsaz-avx2.pl:	vmovdqu		32*3-16-128($np), $TEMP0
asm/rsaz-avx2.pl:	.byte	0xc4,0x41,0x7e,0x6f,0xb5,0xf0,0xff,0xff,0xff	# vmovdqu		32*4-16-128($np), $TEMP1
asm/rsaz-avx2.pl:	vmovdqu		32*5-16-128($np), $TEMP2
asm/rsaz-avx2.pl:	 vmovdqu	$ACC0, (%rsp)		# transfer $r0-$r3
asm/rsaz-avx2.pl:	vmovdqu		32*6-16-128($np), $TEMP0
asm/rsaz-avx2.pl:	vmovdqu		32*7-16-128($np), $TEMP1
asm/rsaz-avx2.pl:	vmovdqu		32*8-16-128($np), $TEMP2
asm/rsaz-avx2.pl:	vmovdqu		32*9-16-128($np), $TEMP0
asm/rsaz-avx2.pl:	 #vmovdqu	32*2-24-128($np), $TEMP1	# moved below
asm/rsaz-avx2.pl:	 vmovdqu	32*3-24-128($np), $TEMP2
asm/rsaz-avx2.pl:	vpmuludq	32*2-24-128($np), $Y2, $TEMP1	# see above
asm/rsaz-avx2.pl:	vmovdqu		32*4-24-128($np), $TEMP0
asm/rsaz-avx2.pl:	 imulq	-128($np), %rax
asm/rsaz-avx2.pl:	vmovdqu		32*5-24-128($np), $TEMP1
asm/rsaz-avx2.pl:	 imulq	8-128($np), %rax
asm/rsaz-avx2.pl:	vmovdqu		32*6-24-128($np), $TEMP2
asm/rsaz-avx2.pl:	 imulq	16-128($np), %rax
asm/rsaz-avx2.pl:	vmovdqu		32*7-24-128($np), $TEMP0
asm/rsaz-avx2.pl:	 imulq	24-128($np), %rdx		# future $r3
asm/rsaz-avx2.pl:	vmovdqu		32*8-24-128($np), $TEMP1
asm/rsaz-avx2.pl:	vmovdqu		32*9-24-128($np), $TEMP2
asm/rsaz-avx2.pl:	vpaddq		32*9-192($tp0), $ACC0, $ACC0
asm/rsaz-avx2.pl:	vpaddq		32*10-448($tp1), $ACC1, $ACC1
asm/rsaz-avx2.pl:	vpaddq		32*11-448($tp1), $ACC2, $ACC2
asm/rsaz-avx2.pl:	vpaddq		32*12-448($tp1), $ACC3, $ACC3
asm/rsaz-avx2.pl:	vpaddq		32*13-448($tp1), $ACC4, $ACC4
asm/rsaz-avx2.pl:	vpaddq		32*14-448($tp1), $ACC5, $ACC5
asm/rsaz-avx2.pl:	vpaddq		32*15-448($tp1), $ACC6, $ACC6
asm/rsaz-avx2.pl:	vpaddq		32*16-448($tp1), $ACC7, $ACC7
asm/rsaz-avx2.pl:	vpaddq		32*17-448($tp1), $ACC8, $ACC8
asm/rsaz-avx2.pl:	vmovdqu		$ACC0, 32*0-128($rp)
asm/rsaz-avx2.pl:	vmovdqu		$ACC1, 32*1-128($rp)
asm/rsaz-avx2.pl:	vmovdqu		$ACC2, 32*2-128($rp)
asm/rsaz-avx2.pl:	vmovdqu		$ACC3, 32*3-128($rp)
asm/rsaz-avx2.pl:	vmovdqu		$ACC4, 32*4-128($rp)
asm/rsaz-avx2.pl:	vmovdqu		$ACC5, 32*5-128($rp)
asm/rsaz-avx2.pl:	vmovdqu		$ACC6, 32*6-128($rp)
asm/rsaz-avx2.pl:	vmovdqu		$ACC7, 32*7-128($rp)
asm/rsaz-avx2.pl:	vmovdqu		$ACC8, 32*8-128($rp)
asm/rsaz-avx2.pl:	movaps	-0xd8(%rax),%xmm6
asm/rsaz-avx2.pl:	movaps	-0xc8(%rax),%xmm7
asm/rsaz-avx2.pl:	movaps	-0xb8(%rax),%xmm8
asm/rsaz-avx2.pl:	movaps	-0xa8(%rax),%xmm9
asm/rsaz-avx2.pl:	movaps	-0x98(%rax),%xmm10
asm/rsaz-avx2.pl:	movaps	-0x88(%rax),%xmm11
asm/rsaz-avx2.pl:	movaps	-0x78(%rax),%xmm12
asm/rsaz-avx2.pl:	movaps	-0x68(%rax),%xmm13
asm/rsaz-avx2.pl:	movaps	-0x58(%rax),%xmm14
asm/rsaz-avx2.pl:	movaps	-0x48(%rax),%xmm15
asm/rsaz-avx2.pl:	mov	-48(%rax),%r15
asm/rsaz-avx2.pl:	mov	-40(%rax),%r14
asm/rsaz-avx2.pl:	mov	-32(%rax),%r13
asm/rsaz-avx2.pl:	mov	-24(%rax),%r12
asm/rsaz-avx2.pl:	mov	-16(%rax),%rbp
asm/rsaz-avx2.pl:	mov	-8(%rax),%rbx
asm/rsaz-avx2.pl:.size	rsaz_1024_sqr_avx2,.-rsaz_1024_sqr_avx2
asm/rsaz-avx2.pl:	lea	-0xa8(%rsp),%rsp
asm/rsaz-avx2.pl:	vmovaps	%xmm6,-0xd8(%rax)
asm/rsaz-avx2.pl:	vmovaps	%xmm7,-0xc8(%rax)
asm/rsaz-avx2.pl:	vmovaps	%xmm8,-0xb8(%rax)
asm/rsaz-avx2.pl:	vmovaps	%xmm9,-0xa8(%rax)
asm/rsaz-avx2.pl:	vmovaps	%xmm10,-0x98(%rax)
asm/rsaz-avx2.pl:	vmovaps	%xmm11,-0x88(%rax)
asm/rsaz-avx2.pl:	vmovaps	%xmm12,-0x78(%rax)
asm/rsaz-avx2.pl:	vmovaps	%xmm13,-0x68(%rax)
asm/rsaz-avx2.pl:	vmovaps	%xmm14,-0x58(%rax)
asm/rsaz-avx2.pl:	vmovaps	%xmm15,-0x48(%rax)
asm/rsaz-avx2.pl:	# unaligned 256-bit load that crosses page boundary can
asm/rsaz-avx2.pl:	sub	\$-128,$ap	# size optimization
asm/rsaz-avx2.pl:	sub	\$-128,$np
asm/rsaz-avx2.pl:	sub	\$-128,$rp
asm/rsaz-avx2.pl:	# unaligned 256-bit load that crosses page boundary can
asm/rsaz-avx2.pl:	vmovdqu		32*0-128($np), $ACC0
asm/rsaz-avx2.pl:	and		\$-512, %rsp
asm/rsaz-avx2.pl:	vmovdqu		32*1-128($np), $ACC1
asm/rsaz-avx2.pl:	vmovdqu		32*2-128($np), $ACC2
asm/rsaz-avx2.pl:	vmovdqu		32*3-128($np), $ACC3
asm/rsaz-avx2.pl:	vmovdqu		32*4-128($np), $ACC4
asm/rsaz-avx2.pl:	vmovdqu		32*5-128($np), $ACC5
asm/rsaz-avx2.pl:	vmovdqu		32*6-128($np), $ACC6
asm/rsaz-avx2.pl:	vmovdqu		32*7-128($np), $ACC7
asm/rsaz-avx2.pl:	vmovdqu		32*8-128($np), $ACC8
asm/rsaz-avx2.pl:	vmovdqu		$ACC0, 32*0-128($np)
asm/rsaz-avx2.pl:	vmovdqu		$ACC1, 32*1-128($np)
asm/rsaz-avx2.pl:	vmovdqu		$ACC2, 32*2-128($np)
asm/rsaz-avx2.pl:	vmovdqu		$ACC3, 32*3-128($np)
asm/rsaz-avx2.pl:	vmovdqu		$ACC4, 32*4-128($np)
asm/rsaz-avx2.pl:	vmovdqu		$ACC5, 32*5-128($np)
asm/rsaz-avx2.pl:	vmovdqu		$ACC6, 32*6-128($np)
asm/rsaz-avx2.pl:	vmovdqu		$ACC7, 32*7-128($np)
asm/rsaz-avx2.pl:	vmovdqu		$ACC8, 32*8-128($np)
asm/rsaz-avx2.pl:	vmovdqu		$ACC9, 32*9-128($np)	# $ACC9 is zero after vzeroall
asm/rsaz-avx2.pl:	and	\$-64,%rsp
asm/rsaz-avx2.pl:	vmovdqu	$ACC9, 32*9-128($rp)		# $ACC9 is zero after vzeroall
asm/rsaz-avx2.pl:	imulq	-128($ap), %rax
asm/rsaz-avx2.pl:	imulq	8-128($ap), $r1
asm/rsaz-avx2.pl:	 imulq	16-128($ap), $r2
asm/rsaz-avx2.pl:	 imulq	24-128($ap), $r3
asm/rsaz-avx2.pl:	vpmuludq	32*1-128($ap),$Bi,$TEMP0
asm/rsaz-avx2.pl:	vpmuludq	32*2-128($ap),$Bi,$TEMP1
asm/rsaz-avx2.pl:	vpmuludq	32*3-128($ap),$Bi,$TEMP2
asm/rsaz-avx2.pl:	vpmuludq	32*4-128($ap),$Bi,$TEMP0
asm/rsaz-avx2.pl:	vpmuludq	32*5-128($ap),$Bi,$TEMP1
asm/rsaz-avx2.pl:	vpmuludq	32*6-128($ap),$Bi,$TEMP2
asm/rsaz-avx2.pl:	vpmuludq	32*7-128($ap),$Bi,$TEMP0
asm/rsaz-avx2.pl:	vpmuludq	32*8-128($ap),$Bi,$TEMP1
asm/rsaz-avx2.pl:	imulq	-128($np),%rax
asm/rsaz-avx2.pl:	imulq	8-128($np),%rax
asm/rsaz-avx2.pl:	imulq	16-128($np),%rax
asm/rsaz-avx2.pl:	imulq	24-128($np),%rdx
asm/rsaz-avx2.pl:	vpmuludq	32*1-128($np),$Yi,$TEMP2
asm/rsaz-avx2.pl:	vpmuludq	32*2-128($np),$Yi,$TEMP0
asm/rsaz-avx2.pl:	vpmuludq	32*3-128($np),$Yi,$TEMP1
asm/rsaz-avx2.pl:	vpmuludq	32*4-128($np),$Yi,$TEMP2
asm/rsaz-avx2.pl:	vpmuludq	32*5-128($np),$Yi,$TEMP0
asm/rsaz-avx2.pl:	vpmuludq	32*6-128($np),$Yi,$TEMP1
asm/rsaz-avx2.pl:	vpmuludq	32*7-128($np),$Yi,$TEMP2
asm/rsaz-avx2.pl:	vpmuludq	32*8-128($np),$Yi,$TEMP0
asm/rsaz-avx2.pl:	imulq	-128($ap),%rax
asm/rsaz-avx2.pl:	 vmovdqu	-8+32*1-128($ap),$TEMP1
asm/rsaz-avx2.pl:	imulq	8-128($ap),%rax
asm/rsaz-avx2.pl:	 vmovdqu	-8+32*2-128($ap),$TEMP2
asm/rsaz-avx2.pl:	 imulq	16-128($ap),%rbx
asm/rsaz-avx2.pl:	vmovdqu		-8+32*3-128($ap),$TEMP0
asm/rsaz-avx2.pl:	vmovdqu		-8+32*4-128($ap),$TEMP1
asm/rsaz-avx2.pl:	vmovdqu		-8+32*5-128($ap),$TEMP2
asm/rsaz-avx2.pl:	vmovdqu		-8+32*6-128($ap),$TEMP0
asm/rsaz-avx2.pl:	vmovdqu		-8+32*7-128($ap),$TEMP1
asm/rsaz-avx2.pl:	vmovdqu		-8+32*8-128($ap),$TEMP2
asm/rsaz-avx2.pl:	vmovdqu		-8+32*9-128($ap),$ACC9
asm/rsaz-avx2.pl:	imulq	-128($np),%rax
asm/rsaz-avx2.pl:	 vmovdqu	-8+32*1-128($np),$TEMP0
asm/rsaz-avx2.pl:	imulq	8-128($np),%rax
asm/rsaz-avx2.pl:	 vmovdqu	-8+32*2-128($np),$TEMP1
asm/rsaz-avx2.pl:	imulq	16-128($np),%rdx
asm/rsaz-avx2.pl:	vmovdqu		-8+32*3-128($np),$TEMP2
asm/rsaz-avx2.pl:	vmovdqu		-8+32*4-128($np),$TEMP0
asm/rsaz-avx2.pl:	vmovdqu		-8+32*5-128($np),$TEMP1
asm/rsaz-avx2.pl:	vmovdqu		-8+32*6-128($np),$TEMP2
asm/rsaz-avx2.pl:	vmovdqu		-8+32*7-128($np),$TEMP0
asm/rsaz-avx2.pl:	vmovdqu		-8+32*8-128($np),$TEMP1
asm/rsaz-avx2.pl:	vmovdqu		-8+32*9-128($np),$TEMP2
asm/rsaz-avx2.pl:	 vmovdqu	-16+32*1-128($ap),$TEMP0
asm/rsaz-avx2.pl:	imulq	-128($ap),%rax
asm/rsaz-avx2.pl:	 vmovdqu	-16+32*2-128($ap),$TEMP1
asm/rsaz-avx2.pl:	 imulq	8-128($ap),%rbx
asm/rsaz-avx2.pl:	vmovdqu		-16+32*3-128($ap),$TEMP2
asm/rsaz-avx2.pl:	vmovdqu		-16+32*4-128($ap),$TEMP0
asm/rsaz-avx2.pl:	vmovdqu		-16+32*5-128($ap),$TEMP1
asm/rsaz-avx2.pl:	vmovdqu		-16+32*6-128($ap),$TEMP2
asm/rsaz-avx2.pl:	vmovdqu		-16+32*7-128($ap),$TEMP0
asm/rsaz-avx2.pl:	vmovdqu		-16+32*8-128($ap),$TEMP1
asm/rsaz-avx2.pl:	vmovdqu		-16+32*9-128($ap),$TEMP2
asm/rsaz-avx2.pl:	 vmovdqu	-16+32*1-128($np),$TEMP0
asm/rsaz-avx2.pl:	imulq	-128($np),%rax
asm/rsaz-avx2.pl:	 vmovdqu	-16+32*2-128($np),$TEMP1
asm/rsaz-avx2.pl:	imulq	8-128($np),%rdx
asm/rsaz-avx2.pl:	vmovdqu		-16+32*3-128($np),$TEMP2
asm/rsaz-avx2.pl:	vmovdqu		-16+32*4-128($np),$TEMP0
asm/rsaz-avx2.pl:	vmovdqu		-16+32*5-128($np),$TEMP1
asm/rsaz-avx2.pl:	vmovdqu		-16+32*6-128($np),$TEMP2
asm/rsaz-avx2.pl:	vmovdqu		-16+32*7-128($np),$TEMP0
asm/rsaz-avx2.pl:	vmovdqu		-16+32*8-128($np),$TEMP1
asm/rsaz-avx2.pl:	vmovdqu		-16+32*9-128($np),$TEMP2
asm/rsaz-avx2.pl:	 vmovdqu	-24+32*1-128($ap),$TEMP0
asm/rsaz-avx2.pl:	 vmovdqu	-24+32*2-128($ap),$TEMP1
asm/rsaz-avx2.pl:	imulq	-128($ap),%rbx
asm/rsaz-avx2.pl:	vmovdqu		-24+32*3-128($ap),$TEMP2
asm/rsaz-avx2.pl:	vmovdqu		-24+32*4-128($ap),$TEMP0
asm/rsaz-avx2.pl:	vmovdqu		-24+32*5-128($ap),$TEMP1
asm/rsaz-avx2.pl:	vmovdqu		-24+32*6-128($ap),$TEMP2
asm/rsaz-avx2.pl:	vmovdqu		-24+32*7-128($ap),$TEMP0
asm/rsaz-avx2.pl:	vmovdqu		-24+32*8-128($ap),$TEMP1
asm/rsaz-avx2.pl:	vmovdqu		-24+32*9-128($ap),$TEMP2
asm/rsaz-avx2.pl:	vmovdqu		-24+32*1-128($np),$TEMP0
asm/rsaz-avx2.pl:	imulq	-128($np),%rax
asm/rsaz-avx2.pl:	vmovdqu		-24+32*2-128($np),$TEMP1
asm/rsaz-avx2.pl:	vmovdqu		-24+32*3-128($np),$TEMP2
asm/rsaz-avx2.pl:	 vmovdqu	$ACC0, (%rsp)			# transfer $r0-$r3
asm/rsaz-avx2.pl:	vmovdqu		-24+32*4-128($np),$TEMP0
asm/rsaz-avx2.pl:	vmovdqu		-24+32*5-128($np),$TEMP1
asm/rsaz-avx2.pl:	vmovdqu		-24+32*6-128($np),$TEMP2
asm/rsaz-avx2.pl:	vmovdqu		-24+32*7-128($np),$TEMP0
asm/rsaz-avx2.pl:	vmovdqu		-24+32*8-128($np),$TEMP1
asm/rsaz-avx2.pl:	vmovdqu		-24+32*9-128($np),$TEMP2
asm/rsaz-avx2.pl:# (*)	Original implementation was correcting ACC1-ACC3 for overflow
asm/rsaz-avx2.pl:	vmovdqu		$ACC0, 0-128($rp)
asm/rsaz-avx2.pl:	vmovdqu		$ACC1, 32-128($rp)
asm/rsaz-avx2.pl:	vmovdqu		$ACC2, 64-128($rp)
asm/rsaz-avx2.pl:	vmovdqu		$ACC3, 96-128($rp)
asm/rsaz-avx2.pl:	vmovdqu		$ACC4, 128-128($rp)
asm/rsaz-avx2.pl:	vmovdqu		$ACC5, 160-128($rp)
asm/rsaz-avx2.pl:	vmovdqu		$ACC6, 192-128($rp)
asm/rsaz-avx2.pl:	vmovdqu		$ACC7, 224-128($rp)
asm/rsaz-avx2.pl:	vmovdqu		$ACC8, 256-128($rp)
asm/rsaz-avx2.pl:	movaps	-0xd8(%rax),%xmm6
asm/rsaz-avx2.pl:	movaps	-0xc8(%rax),%xmm7
asm/rsaz-avx2.pl:	movaps	-0xb8(%rax),%xmm8
asm/rsaz-avx2.pl:	movaps	-0xa8(%rax),%xmm9
asm/rsaz-avx2.pl:	movaps	-0x98(%rax),%xmm10
asm/rsaz-avx2.pl:	movaps	-0x88(%rax),%xmm11
asm/rsaz-avx2.pl:	movaps	-0x78(%rax),%xmm12
asm/rsaz-avx2.pl:	movaps	-0x68(%rax),%xmm13
asm/rsaz-avx2.pl:	movaps	-0x58(%rax),%xmm14
asm/rsaz-avx2.pl:	movaps	-0x48(%rax),%xmm15
asm/rsaz-avx2.pl:	mov	-48(%rax),%r15
asm/rsaz-avx2.pl:	mov	-40(%rax),%r14
asm/rsaz-avx2.pl:	mov	-32(%rax),%r13
asm/rsaz-avx2.pl:	mov	-24(%rax),%r12
asm/rsaz-avx2.pl:	mov	-16(%rax),%rbp
asm/rsaz-avx2.pl:	mov	-8(%rax),%rbx
asm/rsaz-avx2.pl:.size	rsaz_1024_mul_avx2,.-rsaz_1024_mul_avx2
asm/rsaz-avx2.pl:.type	rsaz_1024_red2norm_avx2,\@abi-omnipotent
asm/rsaz-avx2.pl:	sub	\$-128,$inp	# size optimization
asm/rsaz-avx2.pl:	$code.="	mov	`8*$j-128`($inp), @T[0]\n";
asm/rsaz-avx2.pl:	$code.="	shl	\$`29*($j-$k)`,@T[-$k]\n";
asm/rsaz-avx2.pl:	$k--;
asm/rsaz-avx2.pl:	mov	@T[-1], @T[0]
asm/rsaz-avx2.pl:	shl	\$`29*($j-1)`, @T[-1]
asm/rsaz-avx2.pl:	shr	\$`-29*($j-1)`, @T[0]
asm/rsaz-avx2.pl:	$code.="	add	@T[-$l], %rax\n";
asm/rsaz-avx2.pl:	$l--;
asm/rsaz-avx2.pl:.size	rsaz_1024_red2norm_avx2,.-rsaz_1024_red2norm_avx2
asm/rsaz-avx2.pl:.type	rsaz_1024_norm2red_avx2,\@abi-omnipotent
asm/rsaz-avx2.pl:	sub	\$-128,$out	# size optimization
asm/rsaz-avx2.pl:	mov	@T[0],@T[-$k]
asm/rsaz-avx2.pl:	shr	\$`29*$j`,@T[-$k]
asm/rsaz-avx2.pl:	and	%rax,@T[-$k]				# &0x1fffffff
asm/rsaz-avx2.pl:	mov	@T[-$k],`8*$j-128`($out)
asm/rsaz-avx2.pl:	mov	@T[0],`8*$j-128`($out)
asm/rsaz-avx2.pl:	mov	@T[0],`8*$j-128`($out)			# zero
asm/rsaz-avx2.pl:	mov	@T[0],`8*($j+1)-128`($out)
asm/rsaz-avx2.pl:	mov	@T[0],`8*($j+2)-128`($out)
asm/rsaz-avx2.pl:	mov	@T[0],`8*($j+3)-128`($out)
asm/rsaz-avx2.pl:.size	rsaz_1024_norm2red_avx2,.-rsaz_1024_norm2red_avx2
asm/rsaz-avx2.pl:.type	rsaz_1024_scatter5_avx2,\@abi-omnipotent
asm/rsaz-avx2.pl:.size	rsaz_1024_scatter5_avx2,.-rsaz_1024_scatter5_avx2
asm/rsaz-avx2.pl:.type	rsaz_1024_gather5_avx2,\@abi-omnipotent
asm/rsaz-avx2.pl:	lea	-0x88(%rsp),%rax
asm/rsaz-avx2.pl:	# I can't trust assembler to use specific encoding:-(
asm/rsaz-avx2.pl:	.byte	0x48,0x8d,0x60,0xe0		# lea	-0x20(%rax),%rsp
asm/rsaz-avx2.pl:	.byte	0xc5,0xf8,0x29,0x70,0xe0	# vmovaps %xmm6,-0x20(%rax)
asm/rsaz-avx2.pl:	.byte	0xc5,0xf8,0x29,0x78,0xf0	# vmovaps %xmm7,-0x10(%rax)
asm/rsaz-avx2.pl:	lea	-0x100(%rsp),%rsp
asm/rsaz-avx2.pl:	and	\$-32, %rsp
asm/rsaz-avx2.pl:	lea	-128(%rsp),%rax			# control u-op density
asm/rsaz-avx2.pl:	vmovdqa	-32(%r10),%ymm7			# .Lgather_permd
asm/rsaz-avx2.pl:	vmovdqa		32*0-128($inp),	%ymm0
asm/rsaz-avx2.pl:	vmovdqa		32*1-128($inp),	%ymm1
asm/rsaz-avx2.pl:	vmovdqa		32*2-128($inp),	%ymm2
asm/rsaz-avx2.pl:	vmovdqa		32*3-128($inp),	%ymm3
asm/rsaz-avx2.pl:	vmovdqa		32*4-128($inp),	%ymm0
asm/rsaz-avx2.pl:	vmovdqa		32*5-128($inp),	%ymm1
asm/rsaz-avx2.pl:	vmovdqa		32*6-128($inp),	%ymm2
asm/rsaz-avx2.pl:	vmovdqa		32*7-128($inp),	%ymm3
asm/rsaz-avx2.pl:	vpand		32*8-128($inp),	%ymm8,	%ymm0
asm/rsaz-avx2.pl:	vpand		32*9-128($inp),	%ymm9,	%ymm1
asm/rsaz-avx2.pl:	vpand		32*10-128($inp),%ymm10,	%ymm2
asm/rsaz-avx2.pl:	vpand		32*11-128($inp),%ymm11,	%ymm3
asm/rsaz-avx2.pl:	vpand		32*12-128($inp),%ymm12,	%ymm0
asm/rsaz-avx2.pl:	vpand		32*13-128($inp),%ymm13,	%ymm1
asm/rsaz-avx2.pl:	vpand		32*14-128($inp),%ymm14,	%ymm2
asm/rsaz-avx2.pl:	vpand		32*15-128($inp),%ymm15,	%ymm3
asm/rsaz-avx2.pl:	movaps	-0xa8(%r11),%xmm6
asm/rsaz-avx2.pl:	movaps	-0x98(%r11),%xmm7
asm/rsaz-avx2.pl:	movaps	-0x88(%r11),%xmm8
asm/rsaz-avx2.pl:	movaps	-0x78(%r11),%xmm9
asm/rsaz-avx2.pl:	movaps	-0x68(%r11),%xmm10
asm/rsaz-avx2.pl:	movaps	-0x58(%r11),%xmm11
asm/rsaz-avx2.pl:	movaps	-0x48(%r11),%xmm12
asm/rsaz-avx2.pl:	movaps	-0x38(%r11),%xmm13
asm/rsaz-avx2.pl:	movaps	-0x28(%r11),%xmm14
asm/rsaz-avx2.pl:	movaps	-0x18(%r11),%xmm15
asm/rsaz-avx2.pl:.size	rsaz_1024_gather5_avx2,.-rsaz_1024_gather5_avx2
asm/rsaz-avx2.pl:.type	rsaz_avx2_eligible,\@abi-omnipotent
asm/rsaz-avx2.pl:.size	rsaz_avx2_eligible,.-rsaz_avx2_eligible
asm/rsaz-avx2.pl:.type	rsaz_se_handler,\@abi-omnipotent
asm/rsaz-avx2.pl:	mov	120($context),%rax	# pull context->Rax
asm/rsaz-avx2.pl:	mov	248($context),%rbx	# pull context->Rip
asm/rsaz-avx2.pl:	mov	8($disp),%rsi		# disp->ImageBase
asm/rsaz-avx2.pl:	mov	56($disp),%r11		# disp->HandlerData
asm/rsaz-avx2.pl:	cmp	%r10,%rbx		# context->Rip<prologue label
asm/rsaz-avx2.pl:	cmp	%r10,%rbx		# context->Rip>=epilogue label
asm/rsaz-avx2.pl:	mov	160($context),%rbp	# pull context->Rbp
asm/rsaz-avx2.pl:	cmp	%r10,%rbx		# context->Rip>="in tail" label
asm/rsaz-avx2.pl:	mov	-48(%rax),%r15
asm/rsaz-avx2.pl:	mov	-40(%rax),%r14
asm/rsaz-avx2.pl:	mov	-32(%rax),%r13
asm/rsaz-avx2.pl:	mov	-24(%rax),%r12
asm/rsaz-avx2.pl:	mov	-16(%rax),%rbp
asm/rsaz-avx2.pl:	mov	-8(%rax),%rbx
asm/rsaz-avx2.pl:	lea	-0xd8(%rax),%rsi	# %xmm save area
asm/rsaz-avx2.pl:	mov	%rax,152($context)	# restore context->Rsp
asm/rsaz-avx2.pl:	mov	%rsi,168($context)	# restore context->Rsi
asm/rsaz-avx2.pl:	mov	%rdi,176($context)	# restore context->Rdi
asm/rsaz-avx2.pl:	mov	40($disp),%rdi		# disp->ContextRecord
asm/rsaz-avx2.pl:	mov	8(%rsi),%rdx		# arg2, disp->ImageBase
asm/rsaz-avx2.pl:	mov	0(%rsi),%r8		# arg3, disp->ControlPc
asm/rsaz-avx2.pl:	mov	16(%rsi),%r9		# arg4, disp->FunctionEntry
asm/rsaz-avx2.pl:	mov	40(%rsi),%r10		# disp->ContextRecord
asm/rsaz-avx2.pl:	lea	56(%rsi),%r11		# &disp->HandlerData
asm/rsaz-avx2.pl:	lea	24(%rsi),%r12		# &disp->EstablisherFrame
asm/rsaz-avx2.pl:.size	rsaz_se_handler,.-rsaz_se_handler
asm/rsaz-avx2.pl:	s/\b(sh[rl]d?\s+\$)(-?[0-9]+)/$1.$2%64/ge		or
asm/rsaz-avx2.pl:	s/\b(vmov[dq])\b(.+)%ymm([0-9]+)/$1$2%xmm$3/go		or
asm/rsaz-avx2.pl:	s/\b(vmovdqu)\b(.+)%x%ymm([0-9]+)/$1$2%xmm$3/go		or
asm/rsaz-avx2.pl:	s/\b(vpinsr[qd])\b(.+)%ymm([0-9]+)/$1$2%xmm$3/go	or
asm/rsaz-avx2.pl:	s/\b(vpextr[qd])\b(.+)%ymm([0-9]+)/$1$2%xmm$3/go	or
asm/rsaz-avx2.pl:	s/\b(vpbroadcast[qd]\s+)%ymm([0-9]+)/$1%xmm$2/go;
asm/rsaz-avx2.pl:.type	rsaz_avx2_eligible,\@abi-omnipotent
asm/rsaz-avx2.pl:.size	rsaz_avx2_eligible,.-rsaz_avx2_eligible
asm/rsaz-avx2.pl:.type	rsaz_1024_sqr_avx2,\@abi-omnipotent
asm/rsaz-avx2.pl:.size	rsaz_1024_sqr_avx2,.-rsaz_1024_sqr_avx2
asm/rsaz-x86_64.pl:# Copyright 2013-2020 The OpenSSL Project Authors. All Rights Reserved.
asm/rsaz-x86_64.pl:# [2] S. Gueron, V. Krasnov. "Speeding up Big-Numbers Squaring".
asm/rsaz-x86_64.pl:#     Technology: New Generations (ITNG 2012), 821-823 (2012).
asm/rsaz-x86_64.pl:#     Journal of Cryptographic Engineering 2:31-43 (2012).
asm/rsaz-x86_64.pl:#     resistant 512-bit and 1024-bit modular exponentiation for optimizing
asm/rsaz-x86_64.pl:# While original submission covers 512- and 1024-bit exponentiation,
asm/rsaz-x86_64.pl:# this module is limited to 512-bit version only (and as such
asm/rsaz-x86_64.pl:# "monolithic" complete exponentiation jumbo-subroutine, but adheres
asm/rsaz-x86_64.pl:#			----------------+---------------------------
asm/rsaz-x86_64.pl:# Bulldozer		-0%		|-1%		+10%
asm/rsaz-x86_64.pl:# Haswell(**)		-0%		|+12%		+39%
asm/rsaz-x86_64.pl:( $xlate="${dir}x86_64-xlate.pl" and -f $xlate ) or
asm/rsaz-x86_64.pl:( $xlate="${dir}../../perlasm/x86_64-xlate.pl" and -f $xlate) or
asm/rsaz-x86_64.pl:die "can't locate x86_64-xlate.pl";
asm/rsaz-x86_64.pl:if (`$ENV{CC} -Wa,-v -c -o /dev/null -x assembler /dev/null 2>&1`
asm/rsaz-x86_64.pl:		=~ /GNU assembler version ([2-9]\.[0-9]+)/) {
asm/rsaz-x86_64.pl:	    `nasm -v 2>&1` =~ /NASM version ([2-9]\.[0-9]+)/) {
asm/rsaz-x86_64.pl:	    `ml64 2>&1` =~ /Version ([0-9]+)\./) {
asm/rsaz-x86_64.pl:if (!$addx && `$ENV{CC} -v 2>&1` =~ /((?:clang|LLVM) version|.*based on LLVM) ([0-9]+)\.([0-9]+)/) {
asm/rsaz-x86_64.pl:	my $ver = $2 + $3/100.0;	# 3.1->3.01, 3.10->3.10
asm/rsaz-x86_64.pl:rsaz_512_sqr:				# 25-29% faster than rsaz_512_mul
asm/rsaz-x86_64.pl:	movq	$mod, %xmm1		# common off-load
asm/rsaz-x86_64.pl:	movq	$out, %xmm0		# off-load
asm/rsaz-x86_64.pl:	movq	-48(%rax), %r15
asm/rsaz-x86_64.pl:	movq	-40(%rax), %r14
asm/rsaz-x86_64.pl:	movq	-32(%rax), %r13
asm/rsaz-x86_64.pl:	movq	-24(%rax), %r12
asm/rsaz-x86_64.pl:	movq	-16(%rax), %rbp
asm/rsaz-x86_64.pl:	movq	-8(%rax), %rbx
asm/rsaz-x86_64.pl:.size	rsaz_512_sqr,.-rsaz_512_sqr
asm/rsaz-x86_64.pl:	movq	$out, %xmm0		# off-load arguments
asm/rsaz-x86_64.pl:	movq	-48(%rax), %r15
asm/rsaz-x86_64.pl:	movq	-40(%rax), %r14
asm/rsaz-x86_64.pl:	movq	-32(%rax), %r13
asm/rsaz-x86_64.pl:	movq	-24(%rax), %r12
asm/rsaz-x86_64.pl:	movq	-16(%rax), %rbp
asm/rsaz-x86_64.pl:	movq	-8(%rax), %rbx
asm/rsaz-x86_64.pl:.size	rsaz_512_mul,.-rsaz_512_mul
asm/rsaz-x86_64.pl:	movq	$n0, 128(%rsp)		# off-load arguments
asm/rsaz-x86_64.pl:	mov	$n0, 128(%rsp)		# off-load arguments
asm/rsaz-x86_64.pl:	mov	\$-7, %rcx
asm/rsaz-x86_64.pl:	movaps	0xa0-0xc8(%rax),%xmm6
asm/rsaz-x86_64.pl:	movaps	0xb0-0xc8(%rax),%xmm7
asm/rsaz-x86_64.pl:	movaps	0xc0-0xc8(%rax),%xmm8
asm/rsaz-x86_64.pl:	movaps	0xd0-0xc8(%rax),%xmm9
asm/rsaz-x86_64.pl:	movaps	0xe0-0xc8(%rax),%xmm10
asm/rsaz-x86_64.pl:	movaps	0xf0-0xc8(%rax),%xmm11
asm/rsaz-x86_64.pl:	movaps	0x100-0xc8(%rax),%xmm12
asm/rsaz-x86_64.pl:	movaps	0x110-0xc8(%rax),%xmm13
asm/rsaz-x86_64.pl:	movaps	0x120-0xc8(%rax),%xmm14
asm/rsaz-x86_64.pl:	movaps	0x130-0xc8(%rax),%xmm15
asm/rsaz-x86_64.pl:	movq	-48(%rax), %r15
asm/rsaz-x86_64.pl:	movq	-40(%rax), %r14
asm/rsaz-x86_64.pl:	movq	-32(%rax), %r13
asm/rsaz-x86_64.pl:	movq	-24(%rax), %r12
asm/rsaz-x86_64.pl:	movq	-16(%rax), %rbp
asm/rsaz-x86_64.pl:	movq	-8(%rax), %rbx
asm/rsaz-x86_64.pl:.size	rsaz_512_mul_gather4,.-rsaz_512_mul_gather4
asm/rsaz-x86_64.pl:	movq	$out, %xmm0		# off-load arguments
asm/rsaz-x86_64.pl:	movq	-48(%rax), %r15
asm/rsaz-x86_64.pl:	movq	-40(%rax), %r14
asm/rsaz-x86_64.pl:	movq	-32(%rax), %r13
asm/rsaz-x86_64.pl:	movq	-24(%rax), %r12
asm/rsaz-x86_64.pl:	movq	-16(%rax), %rbp
asm/rsaz-x86_64.pl:	movq	-8(%rax), %rbx
asm/rsaz-x86_64.pl:.size	rsaz_512_mul_scatter4,.-rsaz_512_mul_scatter4
asm/rsaz-x86_64.pl:	movq	-48(%rax), %r15
asm/rsaz-x86_64.pl:	movq	-40(%rax), %r14
asm/rsaz-x86_64.pl:	movq	-32(%rax), %r13
asm/rsaz-x86_64.pl:	movq	-24(%rax), %r12
asm/rsaz-x86_64.pl:	movq	-16(%rax), %rbp
asm/rsaz-x86_64.pl:	movq	-8(%rax), %rbx
asm/rsaz-x86_64.pl:.size	rsaz_512_mul_by_one,.-rsaz_512_mul_by_one
asm/rsaz-x86_64.pl:	# input:	%r8-%r15, %rbp - mod, 128(%rsp) - n0
asm/rsaz-x86_64.pl:	# output:	%r8-%r15
asm/rsaz-x86_64.pl:.type	__rsaz_512_reduce,\@abi-omnipotent
asm/rsaz-x86_64.pl:.size	__rsaz_512_reduce,.-__rsaz_512_reduce
asm/rsaz-x86_64.pl:	# input:	%r8-%r15, %rbp - mod, 128(%rsp) - n0
asm/rsaz-x86_64.pl:	# output:	%r8-%r15
asm/rsaz-x86_64.pl:.type	__rsaz_512_reducex,\@abi-omnipotent
asm/rsaz-x86_64.pl:.size	__rsaz_512_reducex,.-__rsaz_512_reducex
asm/rsaz-x86_64.pl:	# input: %r8-%r15, %rdi - $out, %rbp - $mod, %rcx - mask
asm/rsaz-x86_64.pl:.type	__rsaz_512_subtract,\@abi-omnipotent
asm/rsaz-x86_64.pl:.size	__rsaz_512_subtract,.-__rsaz_512_subtract
asm/rsaz-x86_64.pl:	# input: %rsi - ap, %rbp - bp
asm/rsaz-x86_64.pl:.type	__rsaz_512_mul,\@abi-omnipotent
asm/rsaz-x86_64.pl:.size	__rsaz_512_mul,.-__rsaz_512_mul
asm/rsaz-x86_64.pl:	# input: %rsi - ap, %rbp - bp
asm/rsaz-x86_64.pl:.type	__rsaz_512_mulx,\@abi-omnipotent
asm/rsaz-x86_64.pl:	mov	\$-6, %rcx
asm/rsaz-x86_64.pl:	 movq	%rbx, 8+64-8(%rsp,%rcx,8)
asm/rsaz-x86_64.pl:	mov	%rbx, 8+64-8(%rsp)
asm/rsaz-x86_64.pl:.size	__rsaz_512_mulx,.-__rsaz_512_mulx
asm/rsaz-x86_64.pl:.type	rsaz_512_scatter4,\@abi-omnipotent
asm/rsaz-x86_64.pl:.size	rsaz_512_scatter4,.-rsaz_512_scatter4
asm/rsaz-x86_64.pl:.type	rsaz_512_gather4,\@abi-omnipotent
asm/rsaz-x86_64.pl:.size	rsaz_512_gather4,.-rsaz_512_gather4
asm/rsaz-x86_64.pl:.type	se_handler,\@abi-omnipotent
asm/rsaz-x86_64.pl:	mov	120($context),%rax	# pull context->Rax
asm/rsaz-x86_64.pl:	mov	248($context),%rbx	# pull context->Rip
asm/rsaz-x86_64.pl:	mov	8($disp),%rsi		# disp->ImageBase
asm/rsaz-x86_64.pl:	mov	56($disp),%r11		# disp->HandlerData
asm/rsaz-x86_64.pl:	cmp	%r10,%rbx		# context->Rip<end of prologue label
asm/rsaz-x86_64.pl:	mov	152($context),%rax	# pull context->Rsp
asm/rsaz-x86_64.pl:	cmp	%r10,%rbx		# context->Rip>=epilogue label
asm/rsaz-x86_64.pl:	lea	-48-0xa8(%rax),%rsi
asm/rsaz-x86_64.pl:	mov	-8(%rax),%rbx
asm/rsaz-x86_64.pl:	mov	-16(%rax),%rbp
asm/rsaz-x86_64.pl:	mov	-24(%rax),%r12
asm/rsaz-x86_64.pl:	mov	-32(%rax),%r13
asm/rsaz-x86_64.pl:	mov	-40(%rax),%r14
asm/rsaz-x86_64.pl:	mov	-48(%rax),%r15
asm/rsaz-x86_64.pl:	mov	%rbx,144($context)	# restore context->Rbx
asm/rsaz-x86_64.pl:	mov	%rbp,160($context)	# restore context->Rbp
asm/rsaz-x86_64.pl:	mov	%r12,216($context)	# restore context->R12
asm/rsaz-x86_64.pl:	mov	%r13,224($context)	# restore context->R13
asm/rsaz-x86_64.pl:	mov	%r14,232($context)	# restore context->R14
asm/rsaz-x86_64.pl:	mov	%r15,240($context)	# restore context->R15
asm/rsaz-x86_64.pl:	mov	%rax,152($context)	# restore context->Rsp
asm/rsaz-x86_64.pl:	mov	%rsi,168($context)	# restore context->Rsi
asm/rsaz-x86_64.pl:	mov	%rdi,176($context)	# restore context->Rdi
asm/rsaz-x86_64.pl:	mov	40($disp),%rdi		# disp->ContextRecord
asm/rsaz-x86_64.pl:	mov	8(%rsi),%rdx		# arg2, disp->ImageBase
asm/rsaz-x86_64.pl:	mov	0(%rsi),%r8		# arg3, disp->ControlPc
asm/rsaz-x86_64.pl:	mov	16(%rsi),%r9		# arg4, disp->FunctionEntry
asm/rsaz-x86_64.pl:	mov	40(%rsi),%r10		# disp->ContextRecord
asm/rsaz-x86_64.pl:	lea	56(%rsi),%r11		# &disp->HandlerData
asm/rsaz-x86_64.pl:	lea	24(%rsi),%r12		# &disp->EstablisherFrame
asm/rsaz-x86_64.pl:.size	se_handler,.-se_handler
asm/s390x-gf2m.pl:# Copyright 2011-2020 The OpenSSL Project Authors. All Rights Reserved.
asm/s390x-gf2m.pl:# in bn_gf2m.c. It's kind of low-hanging mechanical port from C for
asm/s390x-gf2m.pl:# the effort. And indeed, the module delivers 55%-90%(*) improvement
asm/s390x-gf2m.pl:# on heaviest ECDSA verify and ECDH benchmarks for 163- and 571-bit
asm/s390x-gf2m.pl:# key lengths on z990, 30%-55%(*) - on z10, and 70%-110%(*) - on z196.
asm/s390x-gf2m.pl:# This is for 64-bit build. In 32-bit "highgprs" case improvement is
asm/s390x-gf2m.pl:# even higher, for example on z990 it was measured 80%-150%. ECDSA
asm/s390x-gf2m.pl:# sign is modest 9%-12% faster. Keep in mind that these coefficients
asm/s390x-gf2m.pl:	 srlg	@i[1],$b,4-3
asm/s390x-gf2m.pl:	srlg	@i[0],$b,8-3
asm/s390x-gf2m.pl:	srlg	@i[1],$b,`($n+2)*4`-3
asm/s390x-gf2m.pl:	srlg	@T[1],@T[1],`64-$n*4`
asm/s390x-gf2m.pl:	srlg	@T[1],@T[1],`64-$n*4`
asm/s390x-gf2m.pl:	srlg	@T[0],@T[0],`64-($n+1)*4`
asm/s390x-gf2m.pl:.size	_mul_1x1,.-_mul_1x1
asm/s390x-gf2m.pl:	lghi	%r1,-$stdframe-128
asm/s390x-gf2m.pl:.size	bn_GF2m_mul_2x2,.-bn_GF2m_mul_2x2
asm/s390x-mont.pl:# Copyright 2007-2020 The OpenSSL Project Authors. All Rights Reserved.
asm/s390x-mont.pl:# 64x64=128-bit multiplication, which is not commonly available to C
asm/s390x-mont.pl:# programmers], at least hand-coded bn_asm.c replacement is known to
asm/s390x-mont.pl:# provide 30-40% better results for longest keys. Well, on a second
asm/s390x-mont.pl:# thought it's not very surprising, because z-CPUs are single-issue
asm/s390x-mont.pl:# and _strictly_ in-order execution, while bn_mul_mont is more or less
asm/s390x-mont.pl:# dependent on CPU ability to pipe-line instructions and have several
asm/s390x-mont.pl:# of them "in-flight" at the same time. I mean while other methods,
asm/s390x-mont.pl:# module performance by implementing dedicated squaring code-path and
asm/s390x-mont.pl:# make inner loops counter-based.
asm/s390x-mont.pl:# Adapt for -m31 build. If kernel supports what's called "highgprs"
asm/s390x-mont.pl:# feature on Linux [see /proc/cpuinfo], it's possible to use 64-bit
asm/s390x-mont.pl:# instructions and achieve "64-bit" performance even in 31-bit legacy
asm/s390x-mont.pl:# processor, as long as it's "z-CPU". Latter implies that the code
asm/s390x-mont.pl:# remains z/Architecture specific. Compatibility with 32-bit BN_ULONG
asm/s390x-mont.pl:# is achieved by swapping words after 64-bit loads, follow _dswap-s.
asm/s390x-mont.pl:# On z990 it was measured to perform 2.6-2.2 times better than
asm/s390x-mont.pl:# compiler-generated code, less for longer keys...
asm/s390x-mont.pl:	lgf	$num,`$stdframe+$SIZE_T-4`($sp)	# pull $num
asm/s390x-mont.pl:	lghi	$rp,-$stdframe-8	# leave room for carry bit
asm/s390x-mont.pl:	lcgr	$j,$num		# -$num
asm/s390x-mont.pl:	ahi	$num,-1		# adjust $num for inner loop
asm/s390x-mont.pl:	stg	$nlo,$stdframe-8($j,$sp)	# tp[j-1]=
asm/s390x-mont.pl:	stg	$NHI,$stdframe-8($j,$sp)
asm/s390x-mont.pl:	stg	$nlo,$stdframe-8($j,$sp)	# tp[j-1]=
asm/s390x-mont.pl:	stg	$NHI,$stdframe-8($j,$sp)
asm/s390x-mont.pl:	lghi	$NHI,-1
asm/s390x-mont.pl:.size	bn_mul_mont,.-bn_mul_mont
asm/s390x-mont.pl:	s/_dswap\s+(%r[0-9]+)/sprintf("rllg\t%s,%s,32",$1,$1) if($SIZE_T==4)/e;
asm/s390x.S:// Copyright 2007-2016 The OpenSSL Project Authors. All Rights Reserved.
asm/s390x.S:	slgr	%r1,%r3		// rp-=ap
asm/s390x.S:	brct	%r2,.Loop1_madd	// without touching condition code:-)
asm/s390x.S:.size	bn_mul_add_words,.-bn_mul_add_words
asm/s390x.S:	brct	%r10,.Loop1_mul		// without touching condition code:-)
asm/s390x.S:.size	bn_mul_words,.-bn_mul_words
asm/s390x.S:.size	bn_sqr_words,.-bn_sqr_words
asm/s390x.S:.size	bn_div_words,.-bn_div_words
asm/s390x.S:	brct	%r6,.Loop1_add	// without touching condition code:-)
asm/s390x.S:.size	bn_add_words,.-bn_add_words
asm/s390x.S:	brct	%r6,.Loop1_sub	// without touching condition code:-)
asm/s390x.S:.size	bn_sub_words,.-bn_sub_words
asm/s390x.S:.size	bn_mul_comba8,.-bn_mul_comba8
asm/s390x.S:.size	bn_mul_comba4,.-bn_mul_comba4
asm/s390x.S:.size	bn_sqr_comba8,.-bn_sqr_comba8
asm/s390x.S:.size	bn_sqr_comba4,.-bn_sqr_comba4
asm/sparct4-mont.pl:# Copyright 2012-2021 The OpenSSL Project Authors. All Rights Reserved.
asm/sparct4-mont.pl:# The module is licensed under 2-clause BSD license.
asm/sparct4-mont.pl:# Montgomery squaring-n-multiplication module for SPARC T4.
asm/sparct4-mont.pl:# 1) collection of "single-op" subroutines that perform single
asm/sparct4-mont.pl:#    operation, Montgomery squaring or multiplication, on 512-,
asm/sparct4-mont.pl:#    1024-, 1536- and 2048-bit operands;
asm/sparct4-mont.pl:# 2) collection of "multi-op" subroutines that perform 5 squaring and
asm/sparct4-mont.pl:# 3) fall-back and helper VIS3 subroutines.
asm/sparct4-mont.pl:# RSA sign is dominated by multi-op subroutine, while RSA verify and
asm/sparct4-mont.pl:# DSA - by single-op. Special note about 4096-bit RSA verify result.
asm/sparct4-mont.pl:# 64-bit process, VIS3:
asm/sparct4-mont.pl:# 64-bit process, this module:
asm/sparct4-mont.pl:# 32-bit process, VIS3:
asm/sparct4-mont.pl:# 32-bit process, this module:
asm/sparct4-mont.pl:# 32-bit code is prone to performance degradation as interrupt rate
asm/sparct4-mont.pl:# standard process of handling interrupt in 32-bit process context
asm/sparct4-mont.pl:# zeroed. This renders result invalid, and operation has to be re-run.
asm/sparct4-mont.pl:# even in 32-bit process context and preserves full register contents.
asm/sparct4-mont.pl:# http://www.oracle.com/technetwork/server-storage/sun-sparc-enterprise/documentation/.
asm/sparct4-mont.pl:	mov	-128,%g4
asm/sparct4-mont.pl:	mov	-2047,%g4
asm/sparct4-mont.pl:	mov	-1,$sentinel
asm/sparct4-mont.pl:	add	%g4,-128,%g4
asm/sparct4-mont.pl:	mov	-1,$sentinel
asm/sparct4-mont.pl:	mov	-128,%g4
asm/sparct4-mont.pl:	save	%sp,-128,%sp	! warm it up
asm/sparct4-mont.pl:	save	%sp,-128,%sp
asm/sparct4-mont.pl:	save	%sp,-128,%sp
asm/sparct4-mont.pl:	save	%sp,-128,%sp
asm/sparct4-mont.pl:	save	%sp,-128,%sp
asm/sparct4-mont.pl:	save	%sp,-128,%sp
asm/sparct4-mont.pl:	save	%sp,-128,%sp;		or	$sentinel,%fp,%fp
asm/sparct4-mont.pl:	save	%sp,-128,%sp;		or	$sentinel,%fp,%fp
asm/sparct4-mont.pl:	save	%sp,-128,%sp;		or	$sentinel,%fp,%fp
asm/sparct4-mont.pl:	save	%sp,-128,%sp;		or	$sentinel,%fp,%fp
asm/sparct4-mont.pl:my $lo=($i<$NUM-1)?@N[$i+1]:"%o7";
asm/sparct4-mont.pl:	save	%sp,-128,%sp;		or	$sentinel,%fp,%fp
asm/sparct4-mont.pl:	save	%sp,-128,%sp;		or	$sentinel,%fp,%fp
asm/sparct4-mont.pl:my $lo=($i<$NUM-1)?@B[$i+1]:"%o7";
asm/sparct4-mont.pl:	.word	0x81b02920+$NUM-1	! montmul	$NUM-1
asm/sparct4-mont.pl:@R[$i] =~ /%f([0-9]+)/;
asm/sparct4-mont.pl:	save	%sp,-128,%sp;		or	$sentinel,%fp,%fp
asm/sparct4-mont.pl:	save	%sp,-128,%sp;		or	$sentinel,%fp,%fp
asm/sparct4-mont.pl:	.word   0x81b02940+$NUM-1	! montsqr	$NUM-1
asm/sparct4-mont.pl:.size	bn_mul_mont_t4_$NUM, .-bn_mul_mont_t4_$NUM
asm/sparct4-mont.pl:	mov	-128,%g4
asm/sparct4-mont.pl:	mov	-2047,%g4
asm/sparct4-mont.pl:	mov	-1,$sentinel
asm/sparct4-mont.pl:	add	%g4,-128,%g4
asm/sparct4-mont.pl:	mov	-1,$sentinel
asm/sparct4-mont.pl:	mov	-128,%g4
asm/sparct4-mont.pl:	save	%sp,-128,%sp	! warm it up
asm/sparct4-mont.pl:	save	%sp,-128,%sp
asm/sparct4-mont.pl:	save	%sp,-128,%sp
asm/sparct4-mont.pl:	save	%sp,-128,%sp
asm/sparct4-mont.pl:	save	%sp,-128,%sp
asm/sparct4-mont.pl:	save	%sp,-128,%sp
asm/sparct4-mont.pl:	save	%sp,-128,%sp;		or	$sentinel,%fp,%fp
asm/sparct4-mont.pl:	save	%sp,-128,%sp;		or	$sentinel,%fp,%fp
asm/sparct4-mont.pl:	save	%sp,-128,%sp;		or	$sentinel,%fp,%fp
asm/sparct4-mont.pl:	save	%sp,-128,%sp;		or	$sentinel,%fp,%fp
asm/sparct4-mont.pl:	save	%sp,-128,%sp;		or	$sentinel,%fp,%fp
asm/sparct4-mont.pl:	sllx	%o4,	32,	$pwr		! re-pack $pwr
asm/sparct4-mont.pl:	save	%sp,-128,%sp;		or	$sentinel,%fp,%fp
asm/sparct4-mont.pl:	sllx	%o4,	32,	$pwr		! re-pack $pwr
asm/sparct4-mont.pl:	.word	0x81b02940+$NUM-1	! montsqr	$NUM-1
asm/sparct4-mont.pl:	.word	0x81b02920+$NUM-1	! montmul	$NUM-1
asm/sparct4-mont.pl:.size	bn_pwr5_mont_t4_$NUM, .-bn_pwr5_mont_t4_$NUM
asm/sparct4-mont.pl:# Fall-back subroutines
asm/sparct4-mont.pl:# copy of bn_mul_mont_vis3 adjusted for vectors of 64-bit values
asm/sparct4-mont.pl:#	+-------------------------------+<-----	%sp
asm/sparct4-mont.pl:#	+-------------------------------+<-----	aligned at 64 bytes
asm/sparct4-mont.pl:#	+-------------------------------+
asm/sparct4-mont.pl:#	+-------------------------------+<-----	aligned at 64 bytes
asm/sparct4-mont.pl:	sub	$num,	24,	$cnt	! cnt=num-3
asm/sparct4-mont.pl:	stxa	$lo1,	[$tp]0xe2	! tp[j-1]
asm/sparct4-mont.pl:	sub	$cnt,	8,	$cnt	! j--
asm/sparct4-mont.pl:	stxa	$lo1,	[$tp]0xe2	! tp[j-1]
asm/sparct4-mont.pl:	sub	$num,	16,	$i	! i=num-2
asm/sparct4-mont.pl:	sub	$num,	24,	$cnt	! cnt=num-3
asm/sparct4-mont.pl:	stx	$lo1,	[$tp]		! tp[j-1]
asm/sparct4-mont.pl:	stx	$lo1,	[$tp]		! tp[j-1]
asm/sparct4-mont.pl:	subcc	$num,	8,	$cnt	! cnt=num-1 and clear CCR.xcc
asm/sparct4-mont.pl:	subccc	$tj,	$nj,	$t2	! tp[j]-np[j]
asm/sparct4-mont.pl:	st	$t2,	[$rp-4]		! reverse order
asm/sparct4-mont.pl:	st	$t3,	[$rp-8]
asm/sparct4-mont.pl:.size	bn_mul_mont_t4, .-bn_mul_mont_t4
asm/sparct4-mont.pl:#	+-------------------------------+<-----	%sp
asm/sparct4-mont.pl:#	+-------------------------------+<-----	aligned at 64 bytes
asm/sparct4-mont.pl:#	+-------------------------------+
asm/sparct4-mont.pl:#	+-------------------------------+<-----	aligned at 64 bytes
asm/sparct4-mont.pl:	sub	$num,	24,	$cnt	! cnt=num-3
asm/sparct4-mont.pl:	stxa	$lo1,	[$tp]0xe2	! tp[j-1]
asm/sparct4-mont.pl:	sub	$cnt,	8,	$cnt	! j--
asm/sparct4-mont.pl:	stxa	$lo1,	[$tp]0xe2	! tp[j-1]
asm/sparct4-mont.pl:	sub	$num,	16,	$i	! i=num-2
asm/sparct4-mont.pl:	sub	$num,	24,	$cnt	! cnt=num-3
asm/sparct4-mont.pl:	stx	$lo1,	[$tp]		! tp[j-1]
asm/sparct4-mont.pl:	stx	$lo1,	[$tp]		! tp[j-1]
asm/sparct4-mont.pl:	subcc	$num,	8,	$cnt	! cnt=num-1 and clear CCR.xcc
asm/sparct4-mont.pl:	subccc	$tj,	$nj,	$t2	! tp[j]-np[j]
asm/sparct4-mont.pl:	st	$t2,	[$rp-4]		! reverse order
asm/sparct4-mont.pl:	st	$t3,	[$rp-8]
asm/sparct4-mont.pl:.size	bn_mul_mont_gather5_t4, .-bn_mul_mont_gather5_t4
asm/sparct4-mont.pl:.size	bn_flip_t4, .-bn_flip_t4
asm/sparct4-mont.pl:.size	bn_flip_n_scatter5_t4, .-bn_flip_n_scatter5_t4
asm/sparct4-mont.pl:.size	bn_gather5_t4, .-bn_gather5_t4
asm/sparcv8.S: * Copyright 1999-2016 The OpenSSL Project Authors. All Rights Reserved.
asm/sparcv8.S: * a drop-in SuperSPARC ISA replacement for crypto/bn/bn_asm.c
asm/sparcv8.S: * 1.1	- new loop unrolling model(*);
asm/sparcv8.S: * 1.2	- made gas friendly;
asm/sparcv8.S: * 1.3	- fixed problem with /usr/ccs/lib/cpp;
asm/sparcv8.S: * 1.4	- some retunes;
asm/sparcv8.S:	andcc	%o2,-4,%g0
asm/sparcv8.S:	st	%o4,[%o0-4]
asm/sparcv8.S:	andcc	%o2,-4,%g0
asm/sparcv8.S:.size	bn_mul_add_words,(.-bn_mul_add_words)
asm/sparcv8.S:	andcc	%o2,-4,%g0
asm/sparcv8.S:	st	%g3,[%o0-4]
asm/sparcv8.S:	andcc	%o2,-4,%g0
asm/sparcv8.S:.size	bn_mul_words,(.-bn_mul_words)
asm/sparcv8.S:	andcc	%o2,-4,%g0
asm/sparcv8.S:	st	%o4,[%o0-8]
asm/sparcv8.S:	st	%o5,[%o0-4]
asm/sparcv8.S:	andcc	%o2,-4,%g2
asm/sparcv8.S:.size	bn_sqr_words,(.-bn_sqr_words)
asm/sparcv8.S:.size	bn_div_words,(.-bn_div_words)
asm/sparcv8.S:	andcc	%o3,-4,%g0
asm/sparcv8.S:	ld	[%o1-4],%g3
asm/sparcv8.S:	st	%g3,[%o0-4]
asm/sparcv8.S:	andcc	%o3,-4,%g0
asm/sparcv8.S:	addcc	%g1,-1,%g0
asm/sparcv8.S:	addcc	%g1,-1,%g0
asm/sparcv8.S:	addcc	%g1,-1,%g0
asm/sparcv8.S:	addcc	%g1,-1,%g0
asm/sparcv8.S:.size	bn_add_words,(.-bn_add_words)
asm/sparcv8.S:	andcc	%o3,-4,%g0
asm/sparcv8.S:	ld	[%o1-4],%g3
asm/sparcv8.S:	st	%g4,[%o0-4]
asm/sparcv8.S:	andcc	%o3,-4,%g0
asm/sparcv8.S:	addcc	%g1,-1,%g0
asm/sparcv8.S:	addcc	%g1,-1,%g0
asm/sparcv8.S:	addcc	%g1,-1,%g0
asm/sparcv8.S:	addcc	%g1,-1,%g0
asm/sparcv8.S:.size	bn_sub_words,(.-bn_sub_words)
asm/sparcv8.S:#define FRAME_SIZE	-96
asm/sparcv8.S:.size	bn_mul_comba8,(.-bn_mul_comba8)
asm/sparcv8.S:.size	bn_mul_comba4,(.-bn_mul_comba4)
asm/sparcv8.S:.size	bn_sqr_comba8,(.-bn_sqr_comba8)
asm/sparcv8.S:.size	bn_sqr_comba4,(.-bn_sqr_comba4)
asm/sparcv8plus.S: * Copyright 1999-2016 The OpenSSL Project Authors. All Rights Reserved.
asm/sparcv8plus.S: * a drop-in UltraSPARC ISA replacement for crypto/bn/bn_asm.c
asm/sparcv8plus.S: * Questions-n-answers.
asm/sparcv8plus.S: *	cc -xarch=v8plus -c bn_asm.sparc.v8plus.S -o bn_asm.o
asm/sparcv8plus.S: *	gcc -mcpu=ultrasparc -c bn_asm.sparc.v8plus.S -o bn_asm.o
asm/sparcv8plus.S: *	gcc -E bn_asm.sparc.v8plus.S | as -xarch=v8plus /dev/fd/0 -o bn_asm.o
asm/sparcv8plus.S: *    Quick-n-dirty way to fuse the module into the library.
asm/sparcv8plus.S: *    (in 0.9.2 case with no-asm option):
asm/sparcv8plus.S: *	# cc -xarch=v8plus -c bn_asm.sparc.v8plus.S -o bn_asm.o
asm/sparcv8plus.S: *    Quick-n-dirty way to get rid of it:
asm/sparcv8plus.S: *    It's actually v9-compliant, i.e. *any* UltraSPARC, CPU under
asm/sparcv8plus.S: *    32 bits of otherwise 64-bit registers during a context switch.
asm/sparcv8plus.S: *    attempt to take advantage of UltraSPARC's 64-bitness under
asm/sparcv8plus.S: *    32-bit kernels even though it's perfectly possible (see next
asm/sparcv8plus.S: * Q. 64-bit registers under 32-bit kernels? Didn't you just say it
asm/sparcv8plus.S: * A. You can't address *all* registers as 64-bit wide:-( The catch is
asm/sparcv8plus.S: *    that you actually may rely upon %o0-%o5 and %g1-%g4 being fully
asm/sparcv8plus.S: *    10 registers is a handful. And as a matter of fact none-"comba"
asm/sparcv8plus.S: *    not allocate own stack frame for 'em:-)
asm/sparcv8plus.S: * Q. What about 64-bit kernels?
asm/sparcv8plus.S: * A. What about 'em? Just kidding:-) Pure 64-bit version is currently
asm/sparcv8plus.S: * A. What about 'em? Kidding again:-) Code does *not* contain any
asm/sparcv8plus.S: *	cc-5.0 -xarch=v8plus -xO5 -xdepend	+7-12%
asm/sparcv8plus.S: *	cc-4.2 -xarch=v8plus -xO5 -xdepend	+25-35%
asm/sparcv8plus.S: *	egcs-1.1.2 -mcpu=ultrasparc -O3		+35-45%
asm/sparcv8plus.S: *	cc-5.0 -xarch=v8 -xO5 -xdepend		+7-10%
asm/sparcv8plus.S: *	cc-4.2 -xarch=v8 -xO5 -xdepend		+10%
asm/sparcv8plus.S: *	egcs-1.1.2 -mv8 -O3			+35-45%
asm/sparcv8plus.S: *    assembler implementation:-)	
asm/sparcv8plus.S: * 1.0	- initial release;
asm/sparcv8plus.S: * 1.1	- new loop unrolling model(*);
asm/sparcv8plus.S: *	- some more fine tuning;
asm/sparcv8plus.S: * 1.2	- made gas friendly;
asm/sparcv8plus.S: *	- updates to documentation concerning v9;
asm/sparcv8plus.S: *	- new performance comparison matrix;
asm/sparcv8plus.S: * 1.3	- fixed problem with /usr/ccs/lib/cpp;
asm/sparcv8plus.S: * 1.4	- native V9 bn_*_comba[48] implementation (15% more efficient)
asm/sparcv8plus.S: *	- some retunes;
asm/sparcv8plus.S: *	- support for GNU as added;
asm/sparcv8plus.S: *		op(p+0); if (--n==0) break;
asm/sparcv8plus.S: *		op(p+1); if (--n==0) break;
asm/sparcv8plus.S: *		op(p+2); if (--n==0) break;
asm/sparcv8plus.S: *		op(p+3); if (--n==0) break;
asm/sparcv8plus.S: *		p+=4; n=-4;
asm/sparcv8plus.S: *		op(p+0); if (--n==0) return;
asm/sparcv8plus.S: *		op(p+2); if (--n==0) return;
asm/sparcv8plus.S:  /* They've said -xarch=v9 at command line */
asm/sparcv8plus.S:# define	FRAME_SIZE	-192
asm/sparcv8plus.S:  /* They've said -m64 at command line */
asm/sparcv8plus.S:# define	FRAME_SIZE	-192
asm/sparcv8plus.S:# define	FRAME_SIZE	-96
asm/sparcv8plus.S: * GNU assembler can't stand stuw:-(
asm/sparcv8plus.S:	andcc	%o2,-4,%g0
asm/sparcv8plus.S:	andcc	%o2,-4,%g0
asm/sparcv8plus.S:	stuw	%o4,[%o0-4]
asm/sparcv8plus.S:.size	bn_mul_add_words,(.-bn_mul_add_words)
asm/sparcv8plus.S:	andcc	%o2,-4,%g0
asm/sparcv8plus.S:	stuw	%o4,[%o0-4]
asm/sparcv8plus.S:	andcc	%o2,-4,%g0
asm/sparcv8plus.S:.size	bn_mul_words,(.-bn_mul_words)
asm/sparcv8plus.S:	andcc	%o2,-4,%g0
asm/sparcv8plus.S:	stuw	%o4,[%o0-8]
asm/sparcv8plus.S:	andcc	%o2,-4,%g2
asm/sparcv8plus.S:	stuw	%o5,[%o0-4]
asm/sparcv8plus.S:.size	bn_sqr_words,(.-bn_sqr_words)
asm/sparcv8plus.S:.size	bn_div_words,(.-bn_div_words)
asm/sparcv8plus.S:	andcc	%o3,-4,%g0
asm/sparcv8plus.S:	stuw	%o5,[%o0-4]
asm/sparcv8plus.S:	and	%o3,-4,%g1
asm/sparcv8plus.S:.size	bn_add_words,(.-bn_add_words)
asm/sparcv8plus.S:	andcc	%o3,-4,%g0
asm/sparcv8plus.S:	stuw	%o5,[%o0-4]
asm/sparcv8plus.S:	and	%o3,-4,%g1
asm/sparcv8plus.S:.size	bn_sub_words,(.-bn_sub_words)
asm/sparcv8plus.S: * Code below depends on the fact that upper parts of the %l0-%l7
asm/sparcv8plus.S: * and %i0-%i7 are zeroed by kernel after context switch. In
asm/sparcv8plus.S: * it's not feasible to implement the mumbo-jumbo in less V9
asm/sparcv8plus.S: * instructions:-(" which apparently isn't true thanks to
asm/sparcv8plus.S: * multicycle none-pairable 'rd %y,%rd' instructions.
asm/sparcv8plus.S:.size	bn_mul_comba8,(.-bn_mul_comba8)
asm/sparcv8plus.S:.size	bn_mul_comba4,(.-bn_mul_comba4)
asm/sparcv8plus.S:.size	bn_sqr_comba8,(.-bn_sqr_comba8)
asm/sparcv8plus.S:.size	bn_sqr_comba4,(.-bn_sqr_comba4)
asm/sparcv9-gf2m.pl:# Copyright 2012-2021 The OpenSSL Project Authors. All Rights Reserved.
asm/sparcv9-gf2m.pl:# in bn_gf2m.c. It's kind of low-hanging mechanical port from C for
asm/sparcv9-gf2m.pl:# for all SPARCv9 processors and one for VIS3-capable ones. Former
asm/sparcv9-gf2m.pl:# delivers ~25-45% more, more for longer keys, heaviest DH and DSA
asm/sparcv9-gf2m.pl:# ~100-230% faster than gcc-generated code and ~35-90% faster than
asm/sparcv9-gf2m.pl:	save	%sp,-STACK_FRAME-$locals,%sp
asm/sparcv9-gf2m.pl:	mov	-1,$a12
asm/sparcv9-gf2m.pl:	 srlx	$b,4-3,@i[1]
asm/sparcv9-gf2m.pl:	srlx	$b,8-3,@i[0]
asm/sparcv9-gf2m.pl:	srlx	@i[1],`64-$n*4`,@T[1]
asm/sparcv9-gf2m.pl:	srlx	$b,`($n+2)*4`-3,@i[1]
asm/sparcv9-gf2m.pl:	srlx	@i[1],`64-$n*4`,@T[1]
asm/sparcv9-gf2m.pl:	srlx	@i[0],`64-($n+1)*4`,@T[1]
asm/sparcv9-gf2m.pl:.size	bn_GF2m_mul_2x2,.-bn_GF2m_mul_2x2
asm/sparcv9-mont.pl:# Copyright 2005-2021 The OpenSSL Project Authors. All Rights Reserved.
asm/sparcv9-mont.pl:# Pure SPARCv9/8+ and IALU-only bn_mul_mont implementation. The reasons
asm/sparcv9-mont.pl:# the whole SPARCv9 universe and other VIS-free implementations deserve
asm/sparcv9-mont.pl:# a.k.a. Niagara, has shared FPU and concurrent FPU-intensive paths,
asm/sparcv9-mont.pl:# such as sparcv9a-mont, will simply sink it. Yes, T1 is equipped with
asm/sparcv9-mont.pl:# kernel driver [only(*)], but having decent user-land software
asm/sparcv9-mont.pl:#	improvement on single-threaded RSA sign. It should be noted
asm/sparcv9-mont.pl:#	that 6-10x improvement coefficient does not actually mean
asm/sparcv9-mont.pl:#	something extraordinary in terms of absolute [single-threaded]
asm/sparcv9-mont.pl:#	platforms. 6-10x factor simply places T1 in same performance
asm/sparcv9-mont.pl:#	domain as say AMD64 and IA-64. Improvement of RSA verify don't
asm/sparcv9-mont.pl:# You might notice that inner loops are modulo-scheduled:-) This has
asm/sparcv9-mont.pl:# the advantage... Currently this module surpasses sparcv9a-mont.pl
asm/sparcv9-mont.pl:# by ~20% on UltraSPARC-III and later cores, but recall that sparcv9a
asm/sparcv9-mont.pl:	save	%sp,-$frame,%sp
asm/sparcv9-mont.pl:	and	%o7,-1024,%o7
asm/sparcv9-mont.pl:	st	$car1,[$tp]		! tp[j-1]
asm/sparcv9-mont.pl:	st	$car1,[$tp]		! tp[j-1]
asm/sparcv9-mont.pl:	st	$car1,[$tp+4]		! tp[j-1]
asm/sparcv9-mont.pl:	sub	%g0,$num,%o7		! k=-num
asm/sparcv9-mont.pl:	subccc	%o0,%o1,%o1		! tp[j]-np[j]
asm/sparcv9-mont.pl:	st	$car1,[$tp]			! tp[j-1]
asm/sparcv9-mont.pl:	st	$car1,[$tp]			! tp[j-1]
asm/sparcv9-mont.pl:	st	$car1,[$tp]			! tp[j-1]
asm/sparcv9-mont.pl:	st	$car1,[$tp]			! tp[j-1]
asm/sparcv9-mont.pl:	cmp	$tmp0,$num			! i<num-1
asm/sparcv9-mont.pl:.size	$fname,(.-$fname)
asm/sparcv9a-mont.pl:# Copyright 2005-2021 The OpenSSL Project Authors. All Rights Reserved.
asm/sparcv9a-mont.pl:# binary compatibility. Well yes, it does exclude SPARC64 prior-V(!)
asm/sparcv9a-mont.pl:# integer-only pure SPARCv9 module to "fall down" to.
asm/sparcv9a-mont.pl:# USI&II cores currently exhibit uniform 2x improvement [over pre-
asm/sparcv9a-mont.pl:# out-of-order execution, which *might* mean that integer multiplier
asm/sparcv9a-mont.pl:# additional note, SPARC64 V implements FP Multiply-Add instruction,
asm/sparcv9a-mont.pl:# as Fujitsu SPARC64 V goes, talk to the author:-)
asm/sparcv9a-mont.pl:# The implementation implies following "non-natural" limitations on
asm/sparcv9a-mont.pl:# - num may not be less than 4;
asm/sparcv9a-mont.pl:# - num has to be even;
asm/sparcv9a-mont.pl:# - modulo-schedule inner loop for better performance (on in-order
asm/sparcv9a-mont.pl:# - dedicated squaring procedure[?];
asm/sparcv9a-mont.pl:# Modulo-scheduled inner loops allow to interleave floating point and
asm/sparcv9a-mont.pl:# integer instructions and minimize Read-After-Write penalties. This
asm/sparcv9a-mont.pl:# results in *further* 20-50% performance improvement [depending on
asm/sparcv9a-mont.pl:# key length, more for longer keys] on USI&II cores and 30-80% - on
asm/sparcv9a-mont.pl:# In order to provide for 32-/64-bit ABI duality, I keep integers wider
asm/sparcv9a-mont.pl:# than 32 bit in %g1-%g4 and %o0-%o5. %l0-%l7 and %i0-%i5 are used
asm/sparcv9a-mont.pl:$ap_l="%l1";	# a[num],n[num] are smashed to 32-bit words and saved
asm/sparcv9a-mont.pl:$ap_h="%l2";	# to these four vectors as double-precision FP values.
asm/sparcv9a-mont.pl:$np_h="%l4";	# loop and L1-cache aliasing is minimized...
asm/sparcv9a-mont.pl:$mask="%l7";	# 16-bit mask, 0xffff
asm/sparcv9a-mont.pl:$n0="%g4";	# reassigned(!) to "64-bit" register
asm/sparcv9a-mont.pl:#   --------
asm/sparcv9a-mont.pl:$ASI_FL16_P=0xD2;	# magic ASI value to engage 16-bit FP load
asm/sparcv9a-mont.pl:	save	%sp,-$frame-$locals,%sp
asm/sparcv9a-mont.pl:	and	%o0,-2048,%o0		! optimize TLB utilization
asm/sparcv9a-mont.pl:	wr	%g0,$ASI_FL16_P,%asi	! setup %asi for 16-bit FP loads
asm/sparcv9a-mont.pl:	sub	%g0,$num,$i		! i=-num
asm/sparcv9a-mont.pl:	sub	%g0,$num,$j		! j=-num
asm/sparcv9a-mont.pl:	ld	[%o3+0],$alo_	! load a[j] as pair of 32-bit words
asm/sparcv9a-mont.pl:	ld	[%o5+0],$nlo_	! load n[j] as pair of 32-bit words
asm/sparcv9a-mont.pl:	! transfer b[i] to FPU as 4x16-bit values
asm/sparcv9a-mont.pl:	! transfer ap[0]*b[0]*n0 to FPU as 4x16-bit values
asm/sparcv9a-mont.pl:	ld	[%o4+0],$alo_	! load a[j] as pair of 32-bit words
asm/sparcv9a-mont.pl:	ld	[%o5+0],$nlo_	! load n[j] as pair of 32-bit words
asm/sparcv9a-mont.pl:	!or	%o7,%o0,%o0		! 64-bit result
asm/sparcv9a-mont.pl:	srlx	%o3,16,%g1		! 34-bit carry
asm/sparcv9a-mont.pl:	ld	[%o4+0],$alo_	! load a[j] as pair of 32-bit words
asm/sparcv9a-mont.pl:	ld	[%o5+0],$nlo_	! load n[j] as pair of 32-bit words
asm/sparcv9a-mont.pl:	or	%o7,%o0,%o0		! 64-bit result
asm/sparcv9a-mont.pl:	srlx	%o3,16,%g1		! 34-bit carry
asm/sparcv9a-mont.pl:	stx	%o0,[$tp]		! tp[j-1]=
asm/sparcv9a-mont.pl:	or	%o7,%o0,%o0		! 64-bit result
asm/sparcv9a-mont.pl:	srlx	%o3,16,%g1		! 34-bit carry
asm/sparcv9a-mont.pl:	stx	%o0,[$tp]		! tp[j-1]=
asm/sparcv9a-mont.pl:	stx	%o4,[$tp]		! tp[num-1]=
asm/sparcv9a-mont.pl:	sub	%g0,$num,$j		! j=-num
asm/sparcv9a-mont.pl:	! transfer b[i] to FPU as 4x16-bit values
asm/sparcv9a-mont.pl:	! transfer (ap[0]*b[i]+t[0])*n0 to FPU as 4x16-bit values
asm/sparcv9a-mont.pl:	or	%o7,%o0,%o0		! 64-bit result
asm/sparcv9a-mont.pl:	! end-of-why?
asm/sparcv9a-mont.pl:	srlx	%o3,16,%g1		! 34-bit carry
asm/sparcv9a-mont.pl:	or	%o7,%o0,%o0		! 64-bit result
asm/sparcv9a-mont.pl:	srlx	%o3,16,%g1		! 34-bit carry
asm/sparcv9a-mont.pl:	stx	%o0,[$tp]		! tp[j-1]
asm/sparcv9a-mont.pl:	or	%o7,%o0,%o0		! 64-bit result
asm/sparcv9a-mont.pl:	srlx	%o3,16,%g1		! 34-bit carry
asm/sparcv9a-mont.pl:	stx	%o0,[$tp]		! tp[j-1]
asm/sparcv9a-mont.pl:	stx	%o4,[$tp]		! tp[num-1]
asm/sparcv9a-mont.pl:	sub	%g0,$num,%o7		! n=-num
asm/sparcv9a-mont.pl:	sub	%g0,$num,%o7		! n=-num
asm/sparcv9a-mont.pl:	sub	%g0,$num,%o7		! n=-num
asm/sparcv9a-mont.pl:.size	$fname,(.-$fname)
asm/sparcv9a-mont.pl:# VIS extensions on command line, e.g. -xarch=v9 vs. -xarch=v9a. I
asm/sparcv9a-mont.pl:# dare to do this, because VIS capability is detected at run-time now
asm/sparcv9a-mont.pl:$code =~ s/fzeros\s+%f([0-9]+)/
asm/via-mont.pl:# Copyright 2006-2020 The OpenSSL Project Authors. All Rights Reserved.
asm/via-mont.pl:# Wrapper around 'rep montmul', VIA-specific instruction accessing
asm/via-mont.pl:# PadLock Montgomery Multiplier. The wrapper is designed as drop-in
asm/via-mont.pl:# Lines marked with "software integer" denote performance of hand-
asm/via-mont.pl:# coded integer-only assembler found in OpenSSL 0.9.7. "Software SSE2"
asm/via-mont.pl:# refers to hand-coded SSE2 Montgomery multiplication procedure found
asm/via-mont.pl:# running hand-coded SSE2 bn_mul_mont found in 0.9.9, i.e. "software
asm/via-mont.pl:# - VIA SDK leaves a *lot* of room for improvement (which this
asm/via-mont.pl:#   implementation successfully fills:-);
asm/via-mont.pl:# - 'rep montmul' gives up to >3x performance improvement depending on
asm/via-mont.pl:# - in terms of absolute performance it delivers approximately as much
asm/via-mont.pl:#   as modern out-of-order 32-bit cores [again, for longer keys].
asm/via-mont.pl:	# expresses them in bits, while we work with amount of 32-bit words]
asm/via-mont.pl:	&and	("ebp",-64);		# align to cache-line
asm/via-mont.pl:	&lea	("ebp",&DWP(-$pad,"ecx"));
asm/via-mont.pl:	&lea	("ebp",&DWP(-$pad,"edi","ebp",4));	# so just "rewind"
asm/via-mont.pl:	&mov	(&DWP(0,"edi","edx",4),"eax");	# rp[i]=tp[i]-np[i]
asm/vis3-mont.pl:# Copyright 2012-2021 The OpenSSL Project Authors. All Rights Reserved.
asm/vis3-mont.pl:# are 1.54/1.87/2.11/2.26 times faster for 512/1024/2048/4096-bit key
asm/vis3-mont.pl:#	+-------------------------------+<-----	%sp
asm/vis3-mont.pl:#	+-------------------------------+<-----	aligned at 64 bytes
asm/vis3-mont.pl:#	+-------------------------------+
asm/vis3-mont.pl:#	+-------------------------------+<----- aligned at 64 bytes
asm/vis3-mont.pl:#	+-------------------------------+
asm/vis3-mont.pl:#	+-------------------------------+
asm/vis3-mont.pl:#	+-------------------------------+
asm/vis3-mont.pl:	sub	$num,	24,	$cnt	! cnt=num-3
asm/vis3-mont.pl:	stx	$lo1,	[$tp]		! tp[j-1]
asm/vis3-mont.pl:	sub	$cnt,	8,	$cnt	! j--
asm/vis3-mont.pl:	stx	$lo1,	[$tp]		! tp[j-1]
asm/vis3-mont.pl:	sub	$num,	16,	$i	! i=num-2
asm/vis3-mont.pl:	sub	$num,	24,	$cnt	! cnt=num-3
asm/vis3-mont.pl:	stx	$lo1,	[$tp]		! tp[j-1]
asm/vis3-mont.pl:	stx	$lo1,	[$tp]		! tp[j-1]
asm/vis3-mont.pl:	subcc	$num,	8,	$cnt	! cnt=num-1 and clear CCR.xcc
asm/vis3-mont.pl:	subccc	$tj,	$nj,	$t2	! tp[j]-np[j]
asm/vis3-mont.pl:	st	$t2,	[$rp-4]		! reverse order
asm/vis3-mont.pl:	st	$t3,	[$rp-8]
asm/vis3-mont.pl:.size	bn_mul_mont_vis3, .-bn_mul_mont_vis3
asm/vis3-mont.pl:# extensions on compiler command line, e.g. -xarch=v9 vs. -xarch=v9a.
asm/vis3-mont.pl:# programmer detect if current CPU is VIS capable at run-time.
asm/vis3-mont.pl:	    return $ref if (!/%([goli])([0-9])/);
asm/vis3-mont.pl:	s/\b(umulxhi|addxc[c]{0,2})\s+(%[goli][0-7]),\s*(%[goli][0-7]),\s*(%[goli][0-7])/
asm/x86-gf2m.pl:# Copyright 2011-2020 The OpenSSL Project Authors. All Rights Reserved.
asm/x86-gf2m.pl:# in bn_gf2m.c. It's kind of low-hanging mechanical port from C for
asm/x86-gf2m.pl:# from one benchmark and µ-arch to another. Below are interval values
asm/x86-gf2m.pl:# for 163- and 571-bit ECDH benchmarks relative to compiler-generated
asm/x86-gf2m.pl:# PIII		16%-30%
asm/x86-gf2m.pl:# P4		12%-12%
asm/x86-gf2m.pl:# Opteron	18%-40%
asm/x86-gf2m.pl:# Core2		19%-44%
asm/x86-gf2m.pl:# Atom		38%-64%
asm/x86-gf2m.pl:# Westmere	53%-121%(PCLMULQDQ)/20%-32%(MMX)
asm/x86-gf2m.pl:# Sandy Bridge	72%-127%(PCLMULQDQ)/27%-23%(MMX)
asm/x86-gf2m.pl:for (@ARGV) { $sse2=1 if (/-DOPENSSL_IA32_SSE2/); }
asm/x86-gf2m.pl:	&mov	(@i[1],0x7);		# 5-byte instruction!?
asm/x86-gf2m.pl:		&shr	(@T[0],32-3*$n);
asm/x86-gf2m.pl:	&shr	(@T[0],32-3*$n);	$n++;
asm/x86-gf2m.pl:	&shr	(@i[0],32-3*$n);
asm/x86-gf2m.pl:	&pxor	($R,"mm6");		# (a0+a1)·(b0+b1)-a1·b1-a0·b0
asm/x86-mont.pl:# Copyright 2005-2020 The OpenSSL Project Authors. All Rights Reserved.
asm/x86-mont.pl:# First of all non-SSE2 path should be implemented (yes, for now it
asm/x86-mont.pl:# performs Montgomery multiplication/convolution only on SSE2-capable
asm/x86-mont.pl:# can be unrolled and modulo-scheduled to improve ILP and possibly
asm/x86-mont.pl:# moved to 128-bit XMM register bank (though it would require input
asm/x86-mont.pl:# 110%(!), rsa1024 one - by 70% and rsa4096 - by 20%:-)
asm/x86-mont.pl:# Modulo-scheduling SSE2 loops results in further 15-20% improvement.
asm/x86-mont.pl:# Integer-only code [being equipped with dedicated squaring procedure]
asm/x86-mont.pl:for (@ARGV) { $sse2=1 if (/-DOPENSSL_IA32_SSE2/); }
asm/x86-mont.pl:	&lea	("ebp",&DWP(-$frame,"esp","edi",4));	# future alloca($frame+4*(num+2))
asm/x86-mont.pl:	&and	("ebp",-64);		# align to cache line
asm/x86-mont.pl:	# An OS-agnostic version of __chkstk.
asm/x86-mont.pl:	&and	("eax",-4096);
asm/x86-mont.pl:	&lea	("esp",&DWP(-4096,"esp"));
asm/x86-mont.pl:	&lea	($num,&DWP(-3,"edi"));	# num=num-1 to assist modulo-scheduling
asm/x86-mont.pl:	&mov	("eax",-1);
asm/x86-mont.pl:	&pand	($acc0,$mask);			# inter-register transfers
asm/x86-mont.pl:	&movd	(&DWP($frame-4,"esp",$j,4),$car1);	# tp[j-1]=
asm/x86-mont.pl:	&pmuludq($acc0,$mul0);			# ap[num-1]*bp[0]
asm/x86-mont.pl:	&pmuludq($acc1,$mul1);			# np[num-1]*m1
asm/x86-mont.pl:	&paddq	($car1,$acc0);			# +=ap[num-1]*bp[0];
asm/x86-mont.pl:	&movd	(&DWP($frame-4,"esp",$j,4),$car1);	# tp[num-2]=
asm/x86-mont.pl:	&movq	(&QWP($frame,"esp",$num,4),$car1);	# tp[num].tp[num-1]
asm/x86-mont.pl:	&movd	(&DWP($frame-4,"esp",$j,4),$car1);# tp[j-1]=
asm/x86-mont.pl:	&pmuludq($acc0,$mul0);			# ap[num-1]*bp[i]
asm/x86-mont.pl:	&pmuludq($acc1,$mul1);			# np[num-1]*m1
asm/x86-mont.pl:	&paddq	($car1,$acc0);			# +=ap[num-1]*bp[i]+tp[num-1]
asm/x86-mont.pl:	&movd	(&DWP($frame-4,"esp",$j,4),$car1);	# tp[num-2]=
asm/x86-mont.pl:	&movq	(&QWP($frame,"esp",$num,4),$car1);	# tp[num].tp[num-1]
asm/x86-mont.pl:	# than 10% slower for 4096-bit key elsewhere:-( "Competitive"
asm/x86-mont.pl:	# means compared to the original integer-only assembler.
asm/x86-mont.pl:	# 512-bit RSA sign is better by ~40%, but that's about all
asm/x86-mont.pl:	&mov	(&DWP($frame-4,"esp",$j,4),$carry);	# tp[j]=
asm/x86-mont.pl:	&mul	($word);				# ap[num-1]*bp[0]
asm/x86-mont.pl:	&mov	(&DWP($frame,"esp",$num,4),"eax");	# tp[num-1]=
asm/x86-mont.pl:	&mov	(&DWP($frame-4,"esp",$j,4),$carry);	# tp[j]=
asm/x86-mont.pl:	&mul	($word);				# ap[num-1]*bp[i]
asm/x86-mont.pl:	&add	("eax",&DWP($frame,"esp",$num,4));	# +=tp[num-1]
asm/x86-mont.pl:	&mov	(&DWP($frame,"esp",$num,4),$carry);	# tp[num-1]=
asm/x86-mont.pl:	&mov	(&DWP($frame-8,"esp",$j,4),$carry);	# tp[j-1]=
asm/x86-mont.pl:	&add	($carry,&DWP($frame,"esp",$num,4));	# +=tp[num-1]
asm/x86-mont.pl:	&mov	(&DWP($frame-4,"esp",$num,4),$carry);	# tp[num-2]=
asm/x86-mont.pl:	&mov	(&DWP($frame,"esp",$num,4),"edx");	# tp[num-1]=
asm/x86-mont.pl:	&mov	(&DWP($frame-4,"esp",$j,4),$carry);	# tp[j]=
asm/x86-mont.pl:	&mov	("eax",&DWP(0,$inp,$j,4));		# ap[num-1]
asm/x86-mont.pl:	&mul	($word);				# ap[num-1]*ap[0]
asm/x86-mont.pl:	&mov	(&DWP($frame,"esp",$j,4),$carry);	# tp[num-1]=
asm/x86-mont.pl:	&mov	(&DWP($frame-4,"esp",$j,4),$carry);	# tp[j-1]=
asm/x86-mont.pl:	&mov	(&DWP($frame-8,"esp",$j,4),$carry);	# tp[j]=
asm/x86-mont.pl:	&add	($carry,&DWP($frame,"esp",$num,4));	# +=tp[num-1]
asm/x86-mont.pl:	&mov	(&DWP($frame-4,"esp",$num,4),$carry);	# tp[num-2]=
asm/x86-mont.pl:	&mov	(&DWP($frame,"esp",$num,4),"edx");	# tp[num-1]=
asm/x86-mont.pl:	&mov	(&DWP($frame-4,"esp",$j,4),$carry);	# tp[j]=
asm/x86-mont.pl:	&lea	($num,&DWP(-1,$j));
asm/x86-mont.pl:	&mov	($j,$num);			# j=num-1
asm/x86-mont.pl:	&mov	(&DWP(0,$rp,$i,4),"eax");	# rp[i]=tp[i]-np[i]
asm/x86-mont.pl:	&mov	("edx",-1);
asm/x86_64-gcc.c: * Copyright 2002-2018 The OpenSSL Project Authors. All Rights Reserved.
asm/x86_64-gcc.c:/*-
asm/x86_64-gcc.c: * A. Well, that's because this code is basically a quick-n-dirty
asm/x86_64-gcc.c: *    proof-of-concept hack. As you can see it's implemented with
asm/x86_64-gcc.c: * A. 'apps/openssl speed rsa dsa' output with no-asm:
asm/x86_64-gcc.c: *    For the reference. IA-32 assembler implementation performs
asm/x86_64-gcc.c: *    very much like 64-bit code compiled with no-asm on the same
asm/x86_64-gcc.c:/*-
asm/x86_64-gcc.c: * "m"(a), "+m"(r)      is the way to favor DirectPath µ-code;
asm/x86_64-gcc.c:        num -= 4;
asm/x86_64-gcc.c:        if (--num == 0)
asm/x86_64-gcc.c:        if (--num == 0)
asm/x86_64-gcc.c:        num -= 4;
asm/x86_64-gcc.c:        if (--num == 0)
asm/x86_64-gcc.c:        if (--num == 0)
asm/x86_64-gcc.c:        n -= 4;
asm/x86_64-gcc.c:        if (--n == 0)
asm/x86_64-gcc.c:        if (--n == 0)
asm/x86_64-gcc.c:/* Simics 1.4<7 has buggy sbbq:-( */
asm/x86_64-gcc.c:        r[0] = (t1 - t2 - c) & BN_MASK2;
asm/x86_64-gcc.c:        if (--n <= 0)
asm/x86_64-gcc.c:        r[1] = (t1 - t2 - c) & BN_MASK2;
asm/x86_64-gcc.c:        if (--n <= 0)
asm/x86_64-gcc.c:        r[2] = (t1 - t2 - c) & BN_MASK2;
asm/x86_64-gcc.c:        if (--n <= 0)
asm/x86_64-gcc.c:        r[3] = (t1 - t2 - c) & BN_MASK2;
asm/x86_64-gcc.c:        if (--n <= 0)
asm/x86_64-gcc.c:/* mul_add_c(a,b,c0,c1,c2)  -- c+=a*b for three word number c=(c2,c1,c0) */
asm/x86_64-gcc.c:/* mul_add_c2(a,b,c0,c1,c2) -- c+=2*a*b for three word number c=(c2,c1,c0) */
asm/x86_64-gcc.c:/* sqr_add_c(a,i,c0,c1,c2)  -- c+=a[i]^2 for three word number c=(c2,c1,c0) */
asm/x86_64-gcc.c: * sqr_add_c2(a,i,c0,c1,c2) -- c+=2*a[i]*a[j] for three word number
asm/x86_64-gcc.c: * can not overflow, because it cannot be all-ones.
asm/x86_64-gf2m.pl:# Copyright 2011-2020 The OpenSSL Project Authors. All Rights Reserved.
asm/x86_64-gf2m.pl:# in bn_gf2m.c. It's kind of low-hanging mechanical port from C for
asm/x86_64-gf2m.pl:# later. Improvement varies from one benchmark and µ-arch to another.
asm/x86_64-gf2m.pl:# Vanilla code path is at most 20% faster than compiler-generated code
asm/x86_64-gf2m.pl:# [not very impressive], while PCLMULQDQ - whole 85%-160% better on
asm/x86_64-gf2m.pl:# 163- and 571-bit ECDH benchmarks on Intel CPUs. Keep in mind that
asm/x86_64-gf2m.pl:( $xlate="${dir}x86_64-xlate.pl" and -f $xlate ) or
asm/x86_64-gf2m.pl:( $xlate="${dir}../../perlasm/x86_64-xlate.pl" and -f $xlate) or
asm/x86_64-gf2m.pl:die "can't locate x86_64-xlate.pl";
asm/x86_64-gf2m.pl:.type	_mul_1x1,\@abi-omnipotent
asm/x86_64-gf2m.pl:	mov	\$-1,$a1
asm/x86_64-gf2m.pl:	shl	\$`8*$n-4`,$t1
asm/x86_64-gf2m.pl:	shr	\$`64-(8*$n-4)`,$t0
asm/x86_64-gf2m.pl:	shl	\$`8*$n-4`,$t1
asm/x86_64-gf2m.pl:	shr	\$`64-(8*$n-4)`,$t0
asm/x86_64-gf2m.pl:.cfi_adjust_cfa_offset	-128-8
asm/x86_64-gf2m.pl:.size	_mul_1x1,.-_mul_1x1
asm/x86_64-gf2m.pl:.type	bn_GF2m_mul_2x2,\@abi-omnipotent
asm/x86_64-gf2m.pl:	xorps		%xmm2,%xmm4	# (a0+a1)·(b0+b1)-a0·b0-a1·b1
asm/x86_64-gf2m.pl:	lea	-8*17(%rsp),%rsp
asm/x86_64-gf2m.pl:.cfi_adjust_cfa_offset	-8*17
asm/x86_64-gf2m.pl:.size	bn_GF2m_mul_2x2,.-bn_GF2m_mul_2x2
asm/x86_64-gf2m.pl:.type	se_handler,\@abi-omnipotent
asm/x86_64-gf2m.pl:	mov	120($context),%rax	# pull context->Rax
asm/x86_64-gf2m.pl:	mov	248($context),%rbx	# pull context->Rip
asm/x86_64-gf2m.pl:	cmp	%r10,%rbx		# context->Rip<"prologue" label
asm/x86_64-gf2m.pl:	mov	152($context),%rax	# pull context->Rsp
asm/x86_64-gf2m.pl:	cmp	%r10,%rbx		# context->Rip>="epilogue" label
asm/x86_64-gf2m.pl:	mov	%rbx,144($context)	# restore context->Rbx
asm/x86_64-gf2m.pl:	mov	%rbp,160($context)	# restore context->Rbp
asm/x86_64-gf2m.pl:	mov	%rsi,168($context)	# restore context->Rsi
asm/x86_64-gf2m.pl:	mov	%rdi,176($context)	# restore context->Rdi
asm/x86_64-gf2m.pl:	mov	%r12,216($context)	# restore context->R12
asm/x86_64-gf2m.pl:	mov	%r13,224($context)	# restore context->R13
asm/x86_64-gf2m.pl:	mov	%r14,232($context)	# restore context->R14
asm/x86_64-gf2m.pl:	mov	%rax,152($context)	# restore context->Rsp
asm/x86_64-gf2m.pl:	mov	40($disp),%rdi		# disp->ContextRecord
asm/x86_64-gf2m.pl:	mov	8(%rsi),%rdx		# arg2, disp->ImageBase
asm/x86_64-gf2m.pl:	mov	0(%rsi),%r8		# arg3, disp->ControlPc
asm/x86_64-gf2m.pl:	mov	16(%rsi),%r9		# arg4, disp->FunctionEntry
asm/x86_64-gf2m.pl:	mov	40(%rsi),%r10		# disp->ContextRecord
asm/x86_64-gf2m.pl:	lea	56(%rsi),%r11		# &disp->HandlerData
asm/x86_64-gf2m.pl:	lea	24(%rsi),%r12		# &disp->EstablisherFrame
asm/x86_64-gf2m.pl:.size	se_handler,.-se_handler
asm/x86_64-mont.pl:# Copyright 2005-2020 The OpenSSL Project Authors. All Rights Reserved.
asm/x86_64-mont.pl:# for 512-/1024-/2048-/4096-bit RSA *sign* benchmarks respectively.
asm/x86_64-mont.pl:# Unroll and modulo-schedule inner loops in such manner that they
asm/x86_64-mont.pl:# 1024-bit RSA *sign*. Average performance improvement in comparison
asm/x86_64-mont.pl:# for 512-/1024-/2048-/4096-bit RSA *sign* benchmarks respectively.
asm/x86_64-mont.pl:# Optimize reduction in squaring procedure and improve 1024+-bit RSA
asm/x86_64-mont.pl:# sign performance by 10-16% on Intel Sandy Bridge and later
asm/x86_64-mont.pl:# (virtually same on non-Intel processors).
asm/x86_64-mont.pl:( $xlate="${dir}x86_64-xlate.pl" and -f $xlate ) or
asm/x86_64-mont.pl:( $xlate="${dir}../../perlasm/x86_64-xlate.pl" and -f $xlate) or
asm/x86_64-mont.pl:die "can't locate x86_64-xlate.pl";
asm/x86_64-mont.pl:if (`$ENV{CC} -Wa,-v -c -o /dev/null -x assembler /dev/null 2>&1`
asm/x86_64-mont.pl:		=~ /GNU assembler version ([2-9]\.[0-9]+)/) {
asm/x86_64-mont.pl:	    `nasm -v 2>&1` =~ /NASM version ([2-9]\.[0-9]+)/) {
asm/x86_64-mont.pl:	    `ml64 2>&1` =~ /Version ([0-9]+)\./) {
asm/x86_64-mont.pl:if (!$addx && `$ENV{CC} -v 2>&1` =~ /((?:clang|LLVM) version|.*based on LLVM) ([0-9]+)\.([0-9]+)/) {
asm/x86_64-mont.pl:	my $ver = $2 + $3/100.0;	# 3.1->3.01, 3.10->3.10
asm/x86_64-mont.pl:	lea	-16(%rsp,$num,8),%r10	# future alloca(8*(num+2))
asm/x86_64-mont.pl:	and	\$-1024,%r10		# minimize TLB usage
asm/x86_64-mont.pl:	# An OS-agnostic version of __chkstk.
asm/x86_64-mont.pl:	and	\$-4096,%r11
asm/x86_64-mont.pl:	lea	-4096(%rsp),%rsp
asm/x86_64-mont.pl:	mov	$hi1,-16(%rsp,$j,8)	# tp[j-1]
asm/x86_64-mont.pl:	mov	$hi1,-16(%rsp,$j,8)	# tp[j-1]
asm/x86_64-mont.pl:	mov	$hi1,-8(%rsp,$num,8)
asm/x86_64-mont.pl:	mov	$hi1,-16(%rsp,$j,8)	# tp[j-1]
asm/x86_64-mont.pl:	mov	$hi1,-16(%rsp,$j,8)	# tp[j-1]
asm/x86_64-mont.pl:	mov	$hi1,-8(%rsp,$num,8)
asm/x86_64-mont.pl:	mov	%rax,($rp,$i,8)		# rp[i]=tp[i]-np[i]
asm/x86_64-mont.pl:	mov	\$-1,%rbx
asm/x86_64-mont.pl:	mov	-48(%rsi),%r15
asm/x86_64-mont.pl:	mov	-40(%rsi),%r14
asm/x86_64-mont.pl:	mov	-32(%rsi),%r13
asm/x86_64-mont.pl:	mov	-24(%rsi),%r12
asm/x86_64-mont.pl:	mov	-16(%rsi),%rbp
asm/x86_64-mont.pl:	mov	-8(%rsi),%rbx
asm/x86_64-mont.pl:.size	bn_mul_mont,.-bn_mul_mont
asm/x86_64-mont.pl:	lea	-32(%rsp,$num,8),%r10	# future alloca(8*(num+4))
asm/x86_64-mont.pl:	and	\$-1024,%r10		# minimize TLB usage
asm/x86_64-mont.pl:	and	\$-4096,%r11
asm/x86_64-mont.pl:	lea	-4096(%rsp),%rsp
asm/x86_64-mont.pl:	mov	-16($np,$j,8),%rax
asm/x86_64-mont.pl:	mov	-8($ap,$j,8),%rax
asm/x86_64-mont.pl:	mov	$N[0],-24(%rsp,$j,8)	# tp[j-1]
asm/x86_64-mont.pl:	mov	-8($np,$j,8),%rax
asm/x86_64-mont.pl:	mov	$N[1],-16(%rsp,$j,8)	# tp[j-1]
asm/x86_64-mont.pl:	mov	$N[0],-8(%rsp,$j,8)	# tp[j-1]
asm/x86_64-mont.pl:	mov	-16($ap,$j,8),%rax
asm/x86_64-mont.pl:	mov	$N[1],-32(%rsp,$j,8)	# tp[j-1]
asm/x86_64-mont.pl:	mov	-16($np,$j,8),%rax
asm/x86_64-mont.pl:	mov	-8($ap,$j,8),%rax
asm/x86_64-mont.pl:	mov	$N[0],-24(%rsp,$j,8)	# tp[j-1]
asm/x86_64-mont.pl:	mov	-8($np,$j,8),%rax
asm/x86_64-mont.pl:	mov	$N[1],-16(%rsp,$j,8)	# tp[j-1]
asm/x86_64-mont.pl:	mov	$N[0],-8(%rsp,$j,8)
asm/x86_64-mont.pl:	mov	$N[1],(%rsp)		# tp[j-1]
asm/x86_64-mont.pl:	mov	-16($np,$j,8),%rax
asm/x86_64-mont.pl:	add	-16(%rsp,$j,8),$A[0]	# ap[j]*bp[i]+tp[j]
asm/x86_64-mont.pl:	mov	-8($ap,$j,8),%rax
asm/x86_64-mont.pl:	mov	$N[0],-24(%rsp,$j,8)	# tp[j-1]
asm/x86_64-mont.pl:	mov	-8($np,$j,8),%rax
asm/x86_64-mont.pl:	add	-8(%rsp,$j,8),$A[1]
asm/x86_64-mont.pl:	mov	$N[1],-16(%rsp,$j,8)	# tp[j-1]
asm/x86_64-mont.pl:	mov	$N[0],-8(%rsp,$j,8)	# tp[j-1]
asm/x86_64-mont.pl:	mov	-16($ap,$j,8),%rax
asm/x86_64-mont.pl:	mov	$N[1],-32(%rsp,$j,8)	# tp[j-1]
asm/x86_64-mont.pl:	mov	-16($np,$j,8),%rax
asm/x86_64-mont.pl:	add	-16(%rsp,$j,8),$A[0]	# ap[j]*bp[i]+tp[j]
asm/x86_64-mont.pl:	mov	-8($ap,$j,8),%rax
asm/x86_64-mont.pl:	mov	$N[0],-24(%rsp,$j,8)	# tp[j-1]
asm/x86_64-mont.pl:	mov	-8($np,$j,8),%rax
asm/x86_64-mont.pl:	add	-8(%rsp,$j,8),$A[1]
asm/x86_64-mont.pl:	mov	$N[1],-16(%rsp,$j,8)	# tp[j-1]
asm/x86_64-mont.pl:	mov	$N[0],-8(%rsp,$j,8)
asm/x86_64-mont.pl:	lea	-4($num),$j
asm/x86_64-mont.pl:	shr	\$2,$j			# j=num/4-1
asm/x86_64-mont.pl:	mov	@ri[0],0($rp,$i,8)	# rp[i]=tp[i]-np[i]
asm/x86_64-mont.pl:	mov	@ri[1],8($rp,$i,8)	# rp[i]=tp[i]-np[i]
asm/x86_64-mont.pl:	mov	@ri[2],16($rp,$i,8)	# rp[i]=tp[i]-np[i]
asm/x86_64-mont.pl:	mov	@ri[3],24($rp,$i,8)	# rp[i]=tp[i]-np[i]
asm/x86_64-mont.pl:	mov	@ri[0],0($rp,$i,8)	# rp[i]=tp[i]-np[i]
asm/x86_64-mont.pl:	mov	@ri[1],8($rp,$i,8)	# rp[i]=tp[i]-np[i]
asm/x86_64-mont.pl:	mov	@ri[2],16($rp,$i,8)	# rp[i]=tp[i]-np[i]
asm/x86_64-mont.pl:	mov	@ri[3],24($rp,$i,8)	# rp[i]=tp[i]-np[i]
asm/x86_64-mont.pl:	mov	-48(%rsi),%r15
asm/x86_64-mont.pl:	mov	-40(%rsi),%r14
asm/x86_64-mont.pl:	mov	-32(%rsi),%r13
asm/x86_64-mont.pl:	mov	-24(%rsi),%r12
asm/x86_64-mont.pl:	mov	-16(%rsi),%rbp
asm/x86_64-mont.pl:	mov	-8(%rsi),%rbx
asm/x86_64-mont.pl:.size	bn_mul4x_mont,.-bn_mul4x_mont
asm/x86_64-mont.pl:.extern	bn_sqrx8x_internal		# see x86_64-mont5 module
asm/x86_64-mont.pl:.extern	bn_sqr8x_internal		# see x86_64-mont5 module
asm/x86_64-mont.pl:	lea	-64(%rsp,$num,2),%r11
asm/x86_64-mont.pl:	lea	-64(%rbp,$num,2),%rbp	# future alloca(frame+2*$num)
asm/x86_64-mont.pl:	lea	4096-64(,$num,2),%r10	# 4096-frame-2*$num
asm/x86_64-mont.pl:	lea	-64(%rbp,$num,2),%rbp	# future alloca(frame+2*$num)
asm/x86_64-mont.pl:	and	\$-64,%rbp
asm/x86_64-mont.pl:	and	\$-4096,%r11
asm/x86_64-mont.pl:	lea	-4096(%rsp),%rsp
asm/x86_64-mont.pl:	movq	%r10, %xmm3		# -$num
asm/x86_64-mont.pl:	call	bn_sqrx8x_internal	# see x86_64-mont5 module
asm/x86_64-mont.pl:					# %rax	top-most carry
asm/x86_64-mont.pl:					# %rcx	-8*num
asm/x86_64-mont.pl:	call	bn_sqr8x_internal	# see x86_64-mont5 module
asm/x86_64-mont.pl:					# %rax	top-most carry
asm/x86_64-mont.pl:					# %r8	-8*num
asm/x86_64-mont.pl:	sbb	\$0,%rax		# top-most carry
asm/x86_64-mont.pl:	movdqa	%xmm0,-16*2(%rbx)	# zero tp
asm/x86_64-mont.pl:	movdqa	%xmm0,-16*1(%rbx)
asm/x86_64-mont.pl:	movdqa	%xmm0,-16*2(%rbx,%rdx)
asm/x86_64-mont.pl:	movdqa	%xmm0,-16*1(%rbx,%rdx)
asm/x86_64-mont.pl:	movdqu	%xmm4,-16*2($rptr)
asm/x86_64-mont.pl:	movdqu	%xmm5,-16*1($rptr)
asm/x86_64-mont.pl:	mov	-48(%rsi),%r15
asm/x86_64-mont.pl:	mov	-40(%rsi),%r14
asm/x86_64-mont.pl:	mov	-32(%rsi),%r13
asm/x86_64-mont.pl:	mov	-24(%rsi),%r12
asm/x86_64-mont.pl:	mov	-16(%rsi),%rbp
asm/x86_64-mont.pl:	mov	-8(%rsi),%rbx
asm/x86_64-mont.pl:.size	bn_sqr8x_mont,.-bn_sqr8x_mont
asm/x86_64-mont.pl:	sub	$num,%r10		# -$num
asm/x86_64-mont.pl:	lea	-72(%rsp,%r10),%rbp	# future alloca(frame+$num+8)
asm/x86_64-mont.pl:	and	\$-128,%rbp
asm/x86_64-mont.pl:	and	\$-4096,%r11
asm/x86_64-mont.pl:	lea	-4096(%rsp),%rsp
asm/x86_64-mont.pl:	# +8	off-loaded &b[i]
asm/x86_64-mont.pl:	mov	$bptr,8(%rsp)		# off-load &b[i]
asm/x86_64-mont.pl:	mov	%r10,-4*8($tptr)
asm/x86_64-mont.pl:	mov	%r11,-3*8($tptr)
asm/x86_64-mont.pl:	mov	%r12,-2*8($tptr)
asm/x86_64-mont.pl:	adcx	$zero,%r15		# cf=0, modulo-scheduled
asm/x86_64-mont.pl:	mov	%r10,-5*8($tptr)
asm/x86_64-mont.pl:	mov	%r11,-4*8($tptr)
asm/x86_64-mont.pl:	mov	%r12,-3*8($tptr)
asm/x86_64-mont.pl:	mov	%r13,-2*8($tptr)
asm/x86_64-mont.pl:	mov	8(%rsp),$bptr		# re-load &b[i]
asm/x86_64-mont.pl:	adc	$zero,%r15		# modulo-scheduled
asm/x86_64-mont.pl:	sbb	%r15,%r15		# top-most carry
asm/x86_64-mont.pl:	mov	%r14,-1*8($tptr)
asm/x86_64-mont.pl:	mov	%r15,($tptr)		# save top-most carry
asm/x86_64-mont.pl:	adox	-4*8($tptr),$mi
asm/x86_64-mont.pl:	adox	-3*8($tptr),%r11
asm/x86_64-mont.pl:	adox	-2*8($tptr),%r12
asm/x86_64-mont.pl:	mov	$bptr,8(%rsp)		# off-load &b[i]
asm/x86_64-mont.pl:	adox	-1*8($tptr),%r13
asm/x86_64-mont.pl:	mov	%r10,-4*8($tptr)
asm/x86_64-mont.pl:	mov	%r11,-3*8($tptr)
asm/x86_64-mont.pl:	mov	%r12,-2*8($tptr)
asm/x86_64-mont.pl:	adcx	$zero,%r15		# cf=0, modulo-scheduled
asm/x86_64-mont.pl:	mov	%r10,-5*8($tptr)
asm/x86_64-mont.pl:	mov	%r11,-4*8($tptr)
asm/x86_64-mont.pl:	mov	%r12,-3*8($tptr)
asm/x86_64-mont.pl:	mov	%r13,-2*8($tptr)
asm/x86_64-mont.pl:	mov	8(%rsp),$bptr		# re-load &b[i]
asm/x86_64-mont.pl:	adc	$zero,%r15		# modulo-scheduled
asm/x86_64-mont.pl:	sub	0*8($tptr),$zero	# pull top-most carry
asm/x86_64-mont.pl:	sbb	%r15,%r15		# top-most carry
asm/x86_64-mont.pl:	mov	%r14,-1*8($tptr)
asm/x86_64-mont.pl:	sbb	\$0,%r15		# top-most carry
asm/x86_64-mont.pl:	movdqa	%xmm0,-16*2($tptr)	# zero tp
asm/x86_64-mont.pl:	movdqa	%xmm0,-16*1($tptr)
asm/x86_64-mont.pl:	movdqu	%xmm4,-16*2($rptr)
asm/x86_64-mont.pl:	movdqu	%xmm5,-16*1($rptr)
asm/x86_64-mont.pl:	mov	-48(%rsi),%r15
asm/x86_64-mont.pl:	mov	-40(%rsi),%r14
asm/x86_64-mont.pl:	mov	-32(%rsi),%r13
asm/x86_64-mont.pl:	mov	-24(%rsi),%r12
asm/x86_64-mont.pl:	mov	-16(%rsi),%rbp
asm/x86_64-mont.pl:	mov	-8(%rsi),%rbx
asm/x86_64-mont.pl:.size	bn_mulx4x_mont,.-bn_mulx4x_mont
asm/x86_64-mont.pl:.type	mul_handler,\@abi-omnipotent
asm/x86_64-mont.pl:	mov	120($context),%rax	# pull context->Rax
asm/x86_64-mont.pl:	mov	248($context),%rbx	# pull context->Rip
asm/x86_64-mont.pl:	mov	8($disp),%rsi		# disp->ImageBase
asm/x86_64-mont.pl:	mov	56($disp),%r11		# disp->HandlerData
asm/x86_64-mont.pl:	cmp	%r10,%rbx		# context->Rip<end of prologue label
asm/x86_64-mont.pl:	mov	152($context),%rax	# pull context->Rsp
asm/x86_64-mont.pl:	cmp	%r10,%rbx		# context->Rip>=epilogue label
asm/x86_64-mont.pl:.size	mul_handler,.-mul_handler
asm/x86_64-mont.pl:.type	sqr_handler,\@abi-omnipotent
asm/x86_64-mont.pl:	mov	120($context),%rax	# pull context->Rax
asm/x86_64-mont.pl:	mov	248($context),%rbx	# pull context->Rip
asm/x86_64-mont.pl:	mov	8($disp),%rsi		# disp->ImageBase
asm/x86_64-mont.pl:	mov	56($disp),%r11		# disp->HandlerData
asm/x86_64-mont.pl:	cmp	%r10,%rbx		# context->Rip<.Lsqr_prologue
asm/x86_64-mont.pl:	cmp	%r10,%rbx		# context->Rip<.Lsqr_body
asm/x86_64-mont.pl:	mov	152($context),%rax	# pull context->Rsp
asm/x86_64-mont.pl:	cmp	%r10,%rbx		# context->Rip>=.Lsqr_epilogue
asm/x86_64-mont.pl:	mov	-8(%rax),%rbx
asm/x86_64-mont.pl:	mov	-16(%rax),%rbp
asm/x86_64-mont.pl:	mov	-24(%rax),%r12
asm/x86_64-mont.pl:	mov	-32(%rax),%r13
asm/x86_64-mont.pl:	mov	-40(%rax),%r14
asm/x86_64-mont.pl:	mov	-48(%rax),%r15
asm/x86_64-mont.pl:	mov	%rbx,144($context)	# restore context->Rbx
asm/x86_64-mont.pl:	mov	%rbp,160($context)	# restore context->Rbp
asm/x86_64-mont.pl:	mov	%r12,216($context)	# restore context->R12
asm/x86_64-mont.pl:	mov	%r13,224($context)	# restore context->R13
asm/x86_64-mont.pl:	mov	%r14,232($context)	# restore context->R14
asm/x86_64-mont.pl:	mov	%r15,240($context)	# restore context->R15
asm/x86_64-mont.pl:	mov	%rax,152($context)	# restore context->Rsp
asm/x86_64-mont.pl:	mov	%rsi,168($context)	# restore context->Rsi
asm/x86_64-mont.pl:	mov	%rdi,176($context)	# restore context->Rdi
asm/x86_64-mont.pl:	mov	40($disp),%rdi		# disp->ContextRecord
asm/x86_64-mont.pl:	mov	8(%rsi),%rdx		# arg2, disp->ImageBase
asm/x86_64-mont.pl:	mov	0(%rsi),%r8		# arg3, disp->ControlPc
asm/x86_64-mont.pl:	mov	16(%rsi),%r9		# arg4, disp->FunctionEntry
asm/x86_64-mont.pl:	mov	40(%rsi),%r10		# disp->ContextRecord
asm/x86_64-mont.pl:	lea	56(%rsi),%r11		# &disp->HandlerData
asm/x86_64-mont.pl:	lea	24(%rsi),%r12		# &disp->EstablisherFrame
asm/x86_64-mont.pl:.size	sqr_handler,.-sqr_handler
asm/x86_64-mont5.pl:# Copyright 2011-2020 The OpenSSL Project Authors. All Rights Reserved.
asm/x86_64-mont5.pl:# Companion to x86_64-mont.pl that optimizes cache-timing attack
asm/x86_64-mont5.pl:# references in their x86_64-mont.pl counterparts with cache-neutral
asm/x86_64-mont5.pl:# is implemented, so that scatter-/gathering can be tuned without
asm/x86_64-mont5.pl:# with 0. This is to optimize post-condition...
asm/x86_64-mont5.pl:( $xlate="${dir}x86_64-xlate.pl" and -f $xlate ) or
asm/x86_64-mont5.pl:( $xlate="${dir}../../perlasm/x86_64-xlate.pl" and -f $xlate) or
asm/x86_64-mont5.pl:die "can't locate x86_64-xlate.pl";
asm/x86_64-mont5.pl:if (`$ENV{CC} -Wa,-v -c -o /dev/null -x assembler /dev/null 2>&1`
asm/x86_64-mont5.pl:		=~ /GNU assembler version ([2-9]\.[0-9]+)/) {
asm/x86_64-mont5.pl:	    `nasm -v 2>&1` =~ /NASM version ([2-9]\.[0-9]+)/) {
asm/x86_64-mont5.pl:	    `ml64 2>&1` =~ /Version ([0-9]+)\./) {
asm/x86_64-mont5.pl:if (!$addx && `$ENV{CC} -v 2>&1` =~ /((?:clang|LLVM) version|.*based on LLVM) ([0-9]+)\.([0-9]+)/) {
asm/x86_64-mont5.pl:	my $ver = $2 + $3/100.0;	# 3.1->3.01, 3.10->3.10
asm/x86_64-mont5.pl:		# int idx);	# 0 to 2^5-1, "index" in $bp holding
asm/x86_64-mont5.pl:				# pre-computed powers of a', interlaced
asm/x86_64-mont5.pl:	lea	-280(%rsp,$num,8),%r10	# future alloca(8*(num+2)+256+8)
asm/x86_64-mont5.pl:	and	\$-1024,%r10		# minimize TLB usage
asm/x86_64-mont5.pl:	# An OS-agnostic version of __chkstk.
asm/x86_64-mont5.pl:	and	\$-4096,%r11
asm/x86_64-mont5.pl:	lea	-4096(%rsp),%rsp
asm/x86_64-mont5.pl:	lea	24-112(%rsp,$num,8),%r10# place the mask after tp[num+3] (+ICache optimization)
asm/x86_64-mont5.pl:	and	\$-16,%r10
asm/x86_64-mont5.pl:for($k=0;$k<$STRIDE/16-4;$k+=4) {
asm/x86_64-mont5.pl:	pand	`16*($k+0)-128`($bp),%xmm0	# while it's still in register
asm/x86_64-mont5.pl:	pand	`16*($k+1)-128`($bp),%xmm1
asm/x86_64-mont5.pl:	pand	`16*($k+2)-128`($bp),%xmm2
asm/x86_64-mont5.pl:	pand	`16*($k+3)-128`($bp),%xmm3
asm/x86_64-mont5.pl:for($k=0;$k<$STRIDE/16-4;$k+=4) {
asm/x86_64-mont5.pl:	movdqa	`16*($k+0)-128`($bp),%xmm4
asm/x86_64-mont5.pl:	movdqa	`16*($k+1)-128`($bp),%xmm5
asm/x86_64-mont5.pl:	movdqa	`16*($k+2)-128`($bp),%xmm2
asm/x86_64-mont5.pl:	movdqa	`16*($k+3)-128`($bp),%xmm3
asm/x86_64-mont5.pl:	mov	$hi1,-16(%rsp,$j,8)	# tp[j-1]
asm/x86_64-mont5.pl:	mov	$hi1,-16(%rsp,$num,8)	# tp[num-1]
asm/x86_64-mont5.pl:	mov	$hi1,-8(%rsp,$num,8)
asm/x86_64-mont5.pl:	lea	24+128(%rsp,$num,8),%rdx	# where 256-byte mask is (+size optimization)
asm/x86_64-mont5.pl:	and	\$-16,%rdx
asm/x86_64-mont5.pl:	movdqa	`16*($k+0)-128`($bp),%xmm0
asm/x86_64-mont5.pl:	movdqa	`16*($k+1)-128`($bp),%xmm1
asm/x86_64-mont5.pl:	movdqa	`16*($k+2)-128`($bp),%xmm2
asm/x86_64-mont5.pl:	movdqa	`16*($k+3)-128`($bp),%xmm3
asm/x86_64-mont5.pl:	pand	`16*($k+0)-128`(%rdx),%xmm0
asm/x86_64-mont5.pl:	pand	`16*($k+1)-128`(%rdx),%xmm1
asm/x86_64-mont5.pl:	pand	`16*($k+2)-128`(%rdx),%xmm2
asm/x86_64-mont5.pl:	pand	`16*($k+3)-128`(%rdx),%xmm3
asm/x86_64-mont5.pl:	mov	$hi1,-16(%rsp,$j,8)	# tp[j-1]
asm/x86_64-mont5.pl:	mov	$hi1,-16(%rsp,$num,8)	# tp[num-1]
asm/x86_64-mont5.pl:	mov	$hi1,-8(%rsp,$num,8)
asm/x86_64-mont5.pl:	mov	%rax,($rp,$i,8)		# rp[i]=tp[i]-np[i]
asm/x86_64-mont5.pl:	mov	\$-1,%rbx
asm/x86_64-mont5.pl:	mov	-48(%rsi),%r15
asm/x86_64-mont5.pl:	mov	-40(%rsi),%r14
asm/x86_64-mont5.pl:	mov	-32(%rsi),%r13
asm/x86_64-mont5.pl:	mov	-24(%rsi),%r12
asm/x86_64-mont5.pl:	mov	-16(%rsi),%rbp
asm/x86_64-mont5.pl:	mov	-8(%rsi),%rbx
asm/x86_64-mont5.pl:.size	bn_mul_mont_gather5,.-bn_mul_mont_gather5
asm/x86_64-mont5.pl:	neg	$num			# -$num
asm/x86_64-mont5.pl:	lea	-320(%rsp,$num,2),%r11
asm/x86_64-mont5.pl:	lea	-320(%rbp,$num,2),%rbp	# future alloca(frame+2*num*8+256)
asm/x86_64-mont5.pl:	lea	4096-320(,$num,2),%r10
asm/x86_64-mont5.pl:	lea	-320(%rbp,$num,2),%rbp	# future alloca(frame+2*num*8+256)
asm/x86_64-mont5.pl:	and	\$-64,%rbp
asm/x86_64-mont5.pl:	and	\$-4096,%r11
asm/x86_64-mont5.pl:	lea	-4096(%rsp),%rsp
asm/x86_64-mont5.pl:	mov	-48(%rsi),%r15
asm/x86_64-mont5.pl:	mov	-40(%rsi),%r14
asm/x86_64-mont5.pl:	mov	-32(%rsi),%r13
asm/x86_64-mont5.pl:	mov	-24(%rsi),%r12
asm/x86_64-mont5.pl:	mov	-16(%rsi),%rbp
asm/x86_64-mont5.pl:	mov	-8(%rsi),%rbx
asm/x86_64-mont5.pl:.size	bn_mul4x_mont_gather5,.-bn_mul4x_mont_gather5
asm/x86_64-mont5.pl:.type	mul4x_internal,\@abi-omnipotent
asm/x86_64-mont5.pl:	lea	88-112(%rsp,$num),%r10	# place the mask after tp[num+1] (+ICache optimization)
asm/x86_64-mont5.pl:for($i=0;$i<$STRIDE/16-4;$i+=4) {
asm/x86_64-mont5.pl:	pand	`16*($i+0)-128`($bp),%xmm0	# while it's still in register
asm/x86_64-mont5.pl:	pand	`16*($i+1)-128`($bp),%xmm1
asm/x86_64-mont5.pl:	pand	`16*($i+2)-128`($bp),%xmm2
asm/x86_64-mont5.pl:	pand	`16*($i+3)-128`($bp),%xmm3
asm/x86_64-mont5.pl:for($i=0;$i<$STRIDE/16-4;$i+=4) {
asm/x86_64-mont5.pl:	movdqa	`16*($i+0)-128`($bp),%xmm4
asm/x86_64-mont5.pl:	movdqa	`16*($i+1)-128`($bp),%xmm5
asm/x86_64-mont5.pl:	movdqa	`16*($i+2)-128`($bp),%xmm2
asm/x86_64-mont5.pl:	movdqa	`16*($i+3)-128`($bp),%xmm3
asm/x86_64-mont5.pl:	mov	-8*2($np),%rax
asm/x86_64-mont5.pl:	mov	-8($ap,$j),%rax
asm/x86_64-mont5.pl:	mov	$N[0],-24($tp)		# tp[j-1]
asm/x86_64-mont5.pl:	mov	-8*1($np),%rax
asm/x86_64-mont5.pl:	mov	$N[1],-16($tp)		# tp[j-1]
asm/x86_64-mont5.pl:	mov	$N[0],-8($tp)		# tp[j-1]
asm/x86_64-mont5.pl:	mov	$N[1],($tp)		# tp[j-1]
asm/x86_64-mont5.pl:	mov	-8*2($np),%rax
asm/x86_64-mont5.pl:	mov	-8($ap),%rax
asm/x86_64-mont5.pl:	mov	$N[0],-24($tp)		# tp[j-1]
asm/x86_64-mont5.pl:	mov	-8*1($np),%rax
asm/x86_64-mont5.pl:	mov	$N[1],-16($tp)		# tp[j-1]
asm/x86_64-mont5.pl:	mov	$N[0],-8($tp)
asm/x86_64-mont5.pl:	lea	16+128($tp),%rdx	# where 256-byte mask is (+size optimization)
asm/x86_64-mont5.pl:	movdqa	`16*($i+0)-128`($bp),%xmm0
asm/x86_64-mont5.pl:	movdqa	`16*($i+1)-128`($bp),%xmm1
asm/x86_64-mont5.pl:	movdqa	`16*($i+2)-128`($bp),%xmm2
asm/x86_64-mont5.pl:	movdqa	`16*($i+3)-128`($bp),%xmm3
asm/x86_64-mont5.pl:	pand	`16*($i+0)-128`(%rdx),%xmm0
asm/x86_64-mont5.pl:	pand	`16*($i+1)-128`(%rdx),%xmm1
asm/x86_64-mont5.pl:	pand	`16*($i+2)-128`(%rdx),%xmm2
asm/x86_64-mont5.pl:	pand	`16*($i+3)-128`(%rdx),%xmm3
asm/x86_64-mont5.pl:	mov	-8*2($np),%rax
asm/x86_64-mont5.pl:	mov	-8($ap,$j),%rax
asm/x86_64-mont5.pl:	mov	$N[1],-32($tp)		# tp[j-1]
asm/x86_64-mont5.pl:	mov	-8*1($np),%rax
asm/x86_64-mont5.pl:	add	-8($tp),$A[1]
asm/x86_64-mont5.pl:	mov	$N[0],-24($tp)		# tp[j-1]
asm/x86_64-mont5.pl:	mov	$N[1],-16($tp)		# tp[j-1]
asm/x86_64-mont5.pl:	mov	$N[0],-8($tp)		# tp[j-1]
asm/x86_64-mont5.pl:	mov	-8*2($np),%rax
asm/x86_64-mont5.pl:	mov	-8($ap),%rax
asm/x86_64-mont5.pl:	mov	$N[1],-32($tp)		# tp[j-1]
asm/x86_64-mont5.pl:	mov	-8*1($np),$m1
asm/x86_64-mont5.pl:	add	-8($tp),$A[1]
asm/x86_64-mont5.pl:	mov	$N[0],-24($tp)		# tp[j-1]
asm/x86_64-mont5.pl:	mov	$N[1],-16($tp)		# tp[j-1]
asm/x86_64-mont5.pl:	mov	$N[0],-8($tp)
asm/x86_64-mont5.pl:	sub	$N[0],$m1		# compare top-most words
asm/x86_64-mont5.pl:	sub	$N[1],%rax		# %rax=-$N[1]
asm/x86_64-mont5.pl:	dec	%r12			# so that after 'not' we get -n[0]
asm/x86_64-mont5.pl:.size	mul4x_internal,.-mul4x_internal
asm/x86_64-mont5.pl:	lea	-320(%rsp,$num,2),%r11
asm/x86_64-mont5.pl:	lea	-320(%rbp,$num,2),%rbp	# future alloca(frame+2*num*8+256)
asm/x86_64-mont5.pl:	lea	4096-320(,$num,2),%r10
asm/x86_64-mont5.pl:	lea	-320(%rbp,$num,2),%rbp	# future alloca(frame+2*num*8+256)
asm/x86_64-mont5.pl:	and	\$-64,%rbp
asm/x86_64-mont5.pl:	and	\$-4096,%r11
asm/x86_64-mont5.pl:	lea	-4096(%rsp),%rsp
asm/x86_64-mont5.pl:	movq	%r10, %xmm3		# -$num, used in sqr8x
asm/x86_64-mont5.pl:	mov	-48(%rsi),%r15
asm/x86_64-mont5.pl:	mov	-40(%rsi),%r14
asm/x86_64-mont5.pl:	mov	-32(%rsi),%r13
asm/x86_64-mont5.pl:	mov	-24(%rsi),%r12
asm/x86_64-mont5.pl:	mov	-16(%rsi),%rbp
asm/x86_64-mont5.pl:	mov	-8(%rsi),%rbx
asm/x86_64-mont5.pl:.size	bn_power5,.-bn_power5
asm/x86_64-mont5.pl:.type	bn_sqr8x_internal,\@abi-omnipotent
asm/x86_64-mont5.pl:	# a) multiply-n-add everything but a[i]*a[i];
asm/x86_64-mont5.pl:	lea	32(%r10),$i		# $i=-($num-32)
asm/x86_64-mont5.pl:	mov	-32($aptr,$i),$a0	# a[0]
asm/x86_64-mont5.pl:	mov	-24($aptr,$i),%rax	# a[1]
asm/x86_64-mont5.pl:	lea	-32($tptr,$i),$tptr	# end of tp[] window, &tp[2*$num-"$i"]
asm/x86_64-mont5.pl:	mov	-16($aptr,$i),$ai	# a[2]
asm/x86_64-mont5.pl:	mov	$A0[0],-24($tptr,$i)	# t[1]
asm/x86_64-mont5.pl:	mov	$A0[1],-16($tptr,$i)	# t[2]
asm/x86_64-mont5.pl:	 mov	-8($aptr,$i),$ai	# a[3]
asm/x86_64-mont5.pl:	mov	$A0[0],-8($tptr,$j)	# t[3]
asm/x86_64-mont5.pl:	mov	$A0[0],-8($tptr,$j)	# t[7]
asm/x86_64-mont5.pl:	mov	-32($aptr,$i),$a0	# a[0]
asm/x86_64-mont5.pl:	mov	-24($aptr,$i),%rax	# a[1]
asm/x86_64-mont5.pl:	lea	-32($tptr,$i),$tptr	# end of tp[] window, &tp[2*$num-"$i"]
asm/x86_64-mont5.pl:	mov	-16($aptr,$i),$ai	# a[2]
asm/x86_64-mont5.pl:	mov	-24($tptr,$i),$A0[0]	# t[1]
asm/x86_64-mont5.pl:	mov	$A0[0],-24($tptr,$i)	# t[1]
asm/x86_64-mont5.pl:	add	-16($tptr,$i),$A0[1]	# a[2]*a[0]+t[2]
asm/x86_64-mont5.pl:	mov	$A0[1],-16($tptr,$i)	# t[2]
asm/x86_64-mont5.pl:	 mov	-8($aptr,$i),$ai	# a[3]
asm/x86_64-mont5.pl:	add	-8($tptr,$i),$A1[0]
asm/x86_64-mont5.pl:	mov	$A0[0],-8($tptr,$i)	# t[3]
asm/x86_64-mont5.pl:	mov	$A0[0],-8($tptr,$j)	# t[5], "preloaded t[1]" below
asm/x86_64-mont5.pl:	mov	-32($aptr),$a0		# a[0]
asm/x86_64-mont5.pl:	mov	-24($aptr),%rax		# a[1]
asm/x86_64-mont5.pl:	lea	-32($tptr,$i),$tptr	# end of tp[] window, &tp[2*$num-"$i"]
asm/x86_64-mont5.pl:	mov	-16($aptr),$ai		# a[2]
asm/x86_64-mont5.pl:	 mov	$A0[0],-24($tptr)	# t[1]
asm/x86_64-mont5.pl:	 mov	-8($aptr),$ai		# a[3]
asm/x86_64-mont5.pl:	 mov	$A0[1],-16($tptr)	# t[2]
asm/x86_64-mont5.pl:	mov	$A0[0],-8($tptr)	# t[3]
asm/x86_64-mont5.pl:	 mov	-16($aptr),%rax		# a[2]
asm/x86_64-mont5.pl:	 sub	$num,$i			# $i=16-$num
asm/x86_64-mont5.pl:	 mov	-16($aptr,$i),%rax	# a[0]
asm/x86_64-mont5.pl:	 mov	-8($aptr,$i),%rax	# a[i+1]	# prefetch
asm/x86_64-mont5.pl:	 mov	-16($tptr),$A0[0]	# t[2*i+2]	# prefetch
asm/x86_64-mont5.pl:	 mov	-8($tptr),$A0[1]	# t[2*i+2+1]	# prefetch
asm/x86_64-mont5.pl:	 mov	-8($aptr,$i),%rax	# a[i+1]	# prefetch
asm/x86_64-mont5.pl:	mov	$S[0],-32($tptr)
asm/x86_64-mont5.pl:	 mov	$S[1],-24($tptr)
asm/x86_64-mont5.pl:	mov	$S[2],-16($tptr)
asm/x86_64-mont5.pl:	 mov	$S[3],-8($tptr)
asm/x86_64-mont5.pl:	 mov	-16($tptr),$A0[0]	# t[2*i+2]	# prefetch
asm/x86_64-mont5.pl:	 mov	-8($tptr),$A0[1]	# t[2*i+2+1]	# prefetch
asm/x86_64-mont5.pl:	 mov	-8($aptr),%rax		# a[i+1]	# prefetch
asm/x86_64-mont5.pl:	mov	$S[0],-32($tptr)
asm/x86_64-mont5.pl:	 mov	$S[1],-24($tptr)
asm/x86_64-mont5.pl:	mov	$S[2],-16($tptr)
asm/x86_64-mont5.pl:	mov	$S[3],-8($tptr)
asm/x86_64-mont5.pl:# Montgomery reduction part, "word-by-word" algorithm.
asm/x86_64-mont5.pl:	mov	%rax,(%rdx)		# store top-most carry bit
asm/x86_64-mont5.pl:	 mov	$m0,48-8+8(%rsp,%rcx,8)	# put aside n0*a[i]
asm/x86_64-mont5.pl:	 imulq	%r8,$carry		# modulo-scheduled
asm/x86_64-mont5.pl:	 mov	48-16+8(%rsp,%rcx,8),$m0# pull n0*a[i]
asm/x86_64-mont5.pl:	adc	\$0,%rax		# top-most carry
asm/x86_64-mont5.pl:	 mov	-8($nptr),%rcx		# np[num-1]
asm/x86_64-mont5.pl:.size	bn_sqr8x_internal,.-bn_sqr8x_internal
asm/x86_64-mont5.pl:# Post-condition, 4x unrolled
asm/x86_64-mont5.pl:.type	__bn_post4x_internal,\@abi-omnipotent
asm/x86_64-mont5.pl:	movq	%xmm1,$aptr		# prepare for back-to-back call
asm/x86_64-mont5.pl:	dec	%r12			# so that after 'not' we get -n[0]
asm/x86_64-mont5.pl:	mov	$num,%r10		# prepare for back-to-back call
asm/x86_64-mont5.pl:.size	__bn_post4x_internal,.-__bn_post4x_internal
asm/x86_64-mont5.pl:	neg	$num			# -$num
asm/x86_64-mont5.pl:	lea	-320(%rsp,$num,2),%r11
asm/x86_64-mont5.pl:	lea	-320(%rbp,$num,2),%rbp	# future alloca(frame+2*$num*8+256)
asm/x86_64-mont5.pl:	lea	4096-320(,$num,2),%r10
asm/x86_64-mont5.pl:	lea	-320(%rbp,$num,2),%rbp	# future alloca(frame+2*$num*8+256)
asm/x86_64-mont5.pl:	and	\$-64,%rbp		# ensure alignment
asm/x86_64-mont5.pl:	and	\$-4096,%r11
asm/x86_64-mont5.pl:	lea	-4096(%rsp),%rsp
asm/x86_64-mont5.pl:	# +0	-num
asm/x86_64-mont5.pl:	# +8	off-loaded &b[i]
asm/x86_64-mont5.pl:	mov	-48(%rsi),%r15
asm/x86_64-mont5.pl:	mov	-40(%rsi),%r14
asm/x86_64-mont5.pl:	mov	-32(%rsi),%r13
asm/x86_64-mont5.pl:	mov	-24(%rsi),%r12
asm/x86_64-mont5.pl:	mov	-16(%rsi),%rbp
asm/x86_64-mont5.pl:	mov	-8(%rsi),%rbx
asm/x86_64-mont5.pl:.size	bn_mulx4x_mont_gather5,.-bn_mulx4x_mont_gather5
asm/x86_64-mont5.pl:.type	mulx4x_internal,\@abi-omnipotent
asm/x86_64-mont5.pl:	mov	$num,8(%rsp)		# save -$num (it was in bytes)
asm/x86_64-mont5.pl:	lea	88-112(%rsp,%r10),%r10	# place the mask after tp[num+1] (+ICache optimization)
asm/x86_64-mont5.pl:for($i=0;$i<$STRIDE/16-4;$i+=4) {
asm/x86_64-mont5.pl:	pand	`16*($i+0)-128`($bptr),%xmm0	# while it's still in register
asm/x86_64-mont5.pl:	pand	`16*($i+1)-128`($bptr),%xmm1
asm/x86_64-mont5.pl:	pand	`16*($i+2)-128`($bptr),%xmm2
asm/x86_64-mont5.pl:	pand	`16*($i+3)-128`($bptr),%xmm3
asm/x86_64-mont5.pl:for($i=0;$i<$STRIDE/16-4;$i+=4) {
asm/x86_64-mont5.pl:	movdqa	`16*($i+0)-128`($bptr),%xmm4
asm/x86_64-mont5.pl:	movdqa	`16*($i+1)-128`($bptr),%xmm5
asm/x86_64-mont5.pl:	movdqa	`16*($i+2)-128`($bptr),%xmm2
asm/x86_64-mont5.pl:	movdqa	`16*($i+3)-128`($bptr),%xmm3
asm/x86_64-mont5.pl:	mov	$bptr,8+8(%rsp)		# off-load &b[i]
asm/x86_64-mont5.pl:	mov	%r10,-8*4($tptr)
asm/x86_64-mont5.pl:	mov	%r11,-8*3($tptr)
asm/x86_64-mont5.pl:	mov	%r12,-8*2($tptr)
asm/x86_64-mont5.pl:	adcx	$zero,%r15		# cf=0, modulo-scheduled
asm/x86_64-mont5.pl:	mov	%r10,-5*8($tptr)
asm/x86_64-mont5.pl:	mov	%r11,-4*8($tptr)
asm/x86_64-mont5.pl:	mov	%r12,-3*8($tptr)
asm/x86_64-mont5.pl:	mov	%r13,-2*8($tptr)
asm/x86_64-mont5.pl:	mov	8(%rsp),$num		# load -num
asm/x86_64-mont5.pl:	adc	$zero,%r15		# modulo-scheduled
asm/x86_64-mont5.pl:	mov	8+8(%rsp),$bptr		# re-load &b[i]
asm/x86_64-mont5.pl:	adc	$zero,$zero		# top-most carry
asm/x86_64-mont5.pl:	mov	%r14,-1*8($tptr)
asm/x86_64-mont5.pl:	lea	16-256($tptr),%r10	# where 256-byte mask is (+density control)
asm/x86_64-mont5.pl:	movdqa	`16*($i+0)-128`($bptr),%xmm0
asm/x86_64-mont5.pl:	movdqa	`16*($i+1)-128`($bptr),%xmm1
asm/x86_64-mont5.pl:	movdqa	`16*($i+2)-128`($bptr),%xmm2
asm/x86_64-mont5.pl:	movdqa	`16*($i+3)-128`($bptr),%xmm3
asm/x86_64-mont5.pl:	mov	$zero,($tptr)		# save top-most carry
asm/x86_64-mont5.pl:	adox	-4*8($tptr),$mi		# +t[0]
asm/x86_64-mont5.pl:	adox	-3*8($tptr),%r11
asm/x86_64-mont5.pl:	adox	-2*8($tptr),%r12
asm/x86_64-mont5.pl:	adox	-1*8($tptr),%r13
asm/x86_64-mont5.pl:	mov	$bptr,8+8(%rsp)		# off-load &b[i]
asm/x86_64-mont5.pl:	mov	%r10,-8*4($tptr)
asm/x86_64-mont5.pl:	mov	%r11,-8*3($tptr)
asm/x86_64-mont5.pl:	mov	%r12,-8*2($tptr)
asm/x86_64-mont5.pl:	adcx	$zero,%r15		# cf=0, modulo-scheduled
asm/x86_64-mont5.pl:	mov	%r10,-5*8($tptr)
asm/x86_64-mont5.pl:	mov	%r11,-4*8($tptr)
asm/x86_64-mont5.pl:	mov	%r12,-3*8($tptr)
asm/x86_64-mont5.pl:	mov	%r13,-2*8($tptr)
asm/x86_64-mont5.pl:	mov	0+8(%rsp),$num		# load -num
asm/x86_64-mont5.pl:	adc	$zero,%r15		# modulo-scheduled
asm/x86_64-mont5.pl:	sub	0*8($tptr),$bptr	# pull top-most carry to %cf
asm/x86_64-mont5.pl:	mov	8+8(%rsp),$bptr		# re-load &b[i]
asm/x86_64-mont5.pl:	adc	$zero,$zero		# top-most carry
asm/x86_64-mont5.pl:	mov	%r14,-1*8($tptr)
asm/x86_64-mont5.pl:	mov	-8($nptr),%r10
asm/x86_64-mont5.pl:	sub	%r14,%r10		# compare top-most words
asm/x86_64-mont5.pl:	sub	%r8,%rax		# %rax=-%r8
asm/x86_64-mont5.pl:	dec	%r12			# so that after 'not' we get -n[0]
asm/x86_64-mont5.pl:	jmp	.Lsqrx4x_sub_entry	# common post-condition
asm/x86_64-mont5.pl:.size	mulx4x_internal,.-mulx4x_internal
asm/x86_64-mont5.pl:	lea	-320(%rsp,$num,2),%r11
asm/x86_64-mont5.pl:	lea	-320(%rbp,$num,2),%rbp	# future alloca(frame+2*$num*8+256)
asm/x86_64-mont5.pl:	lea	4096-320(,$num,2),%r10
asm/x86_64-mont5.pl:	lea	-320(%rbp,$num,2),%rbp	# alloca(frame+2*$num*8+256)
asm/x86_64-mont5.pl:	and	\$-64,%rbp
asm/x86_64-mont5.pl:	and	\$-4096,%r11
asm/x86_64-mont5.pl:	lea	-4096(%rsp),%rsp
asm/x86_64-mont5.pl:	# +24	top-most carry bit, used in reduction section
asm/x86_64-mont5.pl:	movq	%r10, %xmm3		# -$num
asm/x86_64-mont5.pl:	mov	%r10,$num		# -num
asm/x86_64-mont5.pl:	mov	-48(%rsi),%r15
asm/x86_64-mont5.pl:	mov	-40(%rsi),%r14
asm/x86_64-mont5.pl:	mov	-32(%rsi),%r13
asm/x86_64-mont5.pl:	mov	-24(%rsi),%r12
asm/x86_64-mont5.pl:	mov	-16(%rsi),%rbp
asm/x86_64-mont5.pl:	mov	-8(%rsi),%rbx
asm/x86_64-mont5.pl:.size	bn_powerx5,.-bn_powerx5
asm/x86_64-mont5.pl:.type	bn_sqrx8x_internal,\@abi-omnipotent
asm/x86_64-mont5.pl:	# a) multiply-n-add everything but a[i]*a[i];
asm/x86_64-mont5.pl:	mov	0*8($aptr),%rdx		# a[0], modulo-scheduled
asm/x86_64-mont5.pl:	#xor	%r9,%r9			# t[1], ex-$num, zero already
asm/x86_64-mont5.pl:	mov	\$-8,%rcx
asm/x86_64-mont5.pl:	mov	-64($aptr),%rdx		# a[0]
asm/x86_64-mont5.pl:	mov	\$-8,%rcx
asm/x86_64-mont5.pl:	mov	-64($aptr),%rdx
asm/x86_64-mont5.pl:	mov	0*8($aptr),%rdx		# a[8], modulo-scheduled
asm/x86_64-mont5.pl:	 movq	%xmm3,%rcx		# -$num
asm/x86_64-mont5.pl:# Montgomery reduction part, "word-by-word" algorithm.
asm/x86_64-mont5.pl:	xor	%eax,%eax		# initial top-most carry bit
asm/x86_64-mont5.pl:	lea	-8*8($nptr,$num),%rcx	# end of n[]
asm/x86_64-mont5.pl:	mov	%rax,24+8(%rsp)		# store top-most carry bit
asm/x86_64-mont5.pl:	mov	\$-8,%rcx
asm/x86_64-mont5.pl:	mov	\$-8,%rcx
asm/x86_64-mont5.pl:	sub	\$8,%rcx		# mov	\$-8,%rcx
asm/x86_64-mont5.pl:	adc	\$0,%rax		# top-most carry
asm/x86_64-mont5.pl:	mov	8*8($tptr,%rcx),%rdx	# modulo-scheduled "%r8"
asm/x86_64-mont5.pl:.size	bn_sqrx8x_internal,.-bn_sqrx8x_internal
asm/x86_64-mont5.pl:# Post-condition, 4x unrolled
asm/x86_64-mont5.pl:	mov	%rcx,%r10		# -$num
asm/x86_64-mont5.pl:	mov	%rcx,%r9		# -$num
asm/x86_64-mont5.pl:	movq	%xmm1,$aptr		# prepare for back-to-back call
asm/x86_64-mont5.pl:	dec	%r12			# so that after 'not' we get -n[0]
asm/x86_64-mont5.pl:.size	__bn_postx4x_internal,.-__bn_postx4x_internal
asm/x86_64-mont5.pl:.type	bn_get_bits5,\@abi-omnipotent
asm/x86_64-mont5.pl:	lea	-8(%ecx),%eax
asm/x86_64-mont5.pl:.size	bn_get_bits5,.-bn_get_bits5
asm/x86_64-mont5.pl:.type	bn_scatter5,\@abi-omnipotent
asm/x86_64-mont5.pl:.size	bn_scatter5,.-bn_scatter5
asm/x86_64-mont5.pl:.type	bn_gather5,\@abi-omnipotent
asm/x86_64-mont5.pl:	# I can't trust assembler to use specific encoding:-(
asm/x86_64-mont5.pl:	and	\$-16,%rsp		# shouldn't be formally required
asm/x86_64-mont5.pl:	movdqa	%xmm3,`16*($i-1)-128`(%rax)
asm/x86_64-mont5.pl:	movdqa	%xmm0,`16*($i+0)-128`(%rax)
asm/x86_64-mont5.pl:	movdqa	%xmm1,`16*($i+1)-128`(%rax)
asm/x86_64-mont5.pl:	movdqa	%xmm2,`16*($i+2)-128`(%rax)
asm/x86_64-mont5.pl:	movdqa	%xmm3,`16*($i-1)-128`(%rax)
asm/x86_64-mont5.pl:	movdqa	`16*($i+0)-128`(%r11),%xmm0
asm/x86_64-mont5.pl:	movdqa	`16*($i+1)-128`(%r11),%xmm1
asm/x86_64-mont5.pl:	movdqa	`16*($i+2)-128`(%r11),%xmm2
asm/x86_64-mont5.pl:	pand	`16*($i+0)-128`(%rax),%xmm0
asm/x86_64-mont5.pl:	movdqa	`16*($i+3)-128`(%r11),%xmm3
asm/x86_64-mont5.pl:	pand	`16*($i+1)-128`(%rax),%xmm1
asm/x86_64-mont5.pl:	pand	`16*($i+2)-128`(%rax),%xmm2
asm/x86_64-mont5.pl:	pand	`16*($i+3)-128`(%rax),%xmm3
asm/x86_64-mont5.pl:.size	bn_gather5,.-bn_gather5
asm/x86_64-mont5.pl:.type	mul_handler,\@abi-omnipotent
asm/x86_64-mont5.pl:	mov	120($context),%rax	# pull context->Rax
asm/x86_64-mont5.pl:	mov	248($context),%rbx	# pull context->Rip
asm/x86_64-mont5.pl:	mov	8($disp),%rsi		# disp->ImageBase
asm/x86_64-mont5.pl:	mov	56($disp),%r11		# disp->HandlerData
asm/x86_64-mont5.pl:	cmp	%r10,%rbx		# context->Rip<end of prologue label
asm/x86_64-mont5.pl:	cmp	%r10,%rbx		# context->Rip<body label
asm/x86_64-mont5.pl:	mov	152($context),%rax	# pull context->Rsp
asm/x86_64-mont5.pl:	cmp	%r10,%rbx		# context->Rip>=epilogue label
asm/x86_64-mont5.pl:	mov	-8(%rax),%rbx
asm/x86_64-mont5.pl:	mov	-16(%rax),%rbp
asm/x86_64-mont5.pl:	mov	-24(%rax),%r12
asm/x86_64-mont5.pl:	mov	-32(%rax),%r13
asm/x86_64-mont5.pl:	mov	-40(%rax),%r14
asm/x86_64-mont5.pl:	mov	-48(%rax),%r15
asm/x86_64-mont5.pl:	mov	%rbx,144($context)	# restore context->Rbx
asm/x86_64-mont5.pl:	mov	%rbp,160($context)	# restore context->Rbp
asm/x86_64-mont5.pl:	mov	%r12,216($context)	# restore context->R12
asm/x86_64-mont5.pl:	mov	%r13,224($context)	# restore context->R13
asm/x86_64-mont5.pl:	mov	%r14,232($context)	# restore context->R14
asm/x86_64-mont5.pl:	mov	%r15,240($context)	# restore context->R15
asm/x86_64-mont5.pl:	mov	%rax,152($context)	# restore context->Rsp
asm/x86_64-mont5.pl:	mov	%rsi,168($context)	# restore context->Rsi
asm/x86_64-mont5.pl:	mov	%rdi,176($context)	# restore context->Rdi
asm/x86_64-mont5.pl:	mov	40($disp),%rdi		# disp->ContextRecord
asm/x86_64-mont5.pl:	mov	8(%rsi),%rdx		# arg2, disp->ImageBase
asm/x86_64-mont5.pl:	mov	0(%rsi),%r8		# arg3, disp->ControlPc
asm/x86_64-mont5.pl:	mov	16(%rsi),%r9		# arg4, disp->FunctionEntry
asm/x86_64-mont5.pl:	mov	40(%rsi),%r10		# disp->ContextRecord
asm/x86_64-mont5.pl:	lea	56(%rsi),%r11		# &disp->HandlerData
asm/x86_64-mont5.pl:	lea	24(%rsi),%r12		# &disp->EstablisherFrame
asm/x86_64-mont5.pl:.size	mul_handler,.-mul_handler
bn_add.c: * Copyright 1995-2020 The OpenSSL Project Authors. All Rights Reserved.
bn_add.c:    if (a->neg == b->neg) {
bn_add.c:        r_neg = a->neg;
bn_add.c:            r_neg = a->neg;
bn_add.c:            r_neg = b->neg;
bn_add.c:    r->neg = r_neg;
bn_add.c:    if (a->neg != b->neg) {
bn_add.c:        r_neg = a->neg;
bn_add.c:            r_neg = a->neg;
bn_add.c:            r_neg = !b->neg;
bn_add.c:    r->neg = r_neg;
bn_add.c:    if (a->top < b->top) {
bn_add.c:    max = a->top;
bn_add.c:    min = b->top;
bn_add.c:    dif = max - min;
bn_add.c:    r->top = max;
bn_add.c:    ap = a->d;
bn_add.c:    bp = b->d;
bn_add.c:    rp = r->d;
bn_add.c:        dif--;
bn_add.c:    r->top += carry;
bn_add.c:    r->neg = 0;
bn_add.c:    max = a->top;
bn_add.c:    min = b->top;
bn_add.c:    dif = max - min;
bn_add.c:    ap = a->d;
bn_add.c:    bp = b->d;
bn_add.c:    rp = r->d;
bn_add.c:        dif--;
bn_add.c:        t2 = (t1 - borrow) & BN_MASK2;
bn_add.c:    while (max && *--rp == 0)
bn_add.c:        max--;
bn_add.c:    r->top = max;
bn_add.c:    r->neg = 0;
bn_asm.c: * Copyright 1995-2016 The OpenSSL Project Authors. All Rights Reserved.
bn_asm.c:        num -= 4;
bn_asm.c:        num--;
bn_asm.c:        num -= 4;
bn_asm.c:        num--;
bn_asm.c:        n -= 4;
bn_asm.c:        n--;
bn_asm.c:        num -= 4;
bn_asm.c:        num--;
bn_asm.c:        num -= 4;
bn_asm.c:        num--;
bn_asm.c:        n -= 4;
bn_asm.c:        n--;
bn_asm.c:/* I need to test this some more :-( */
bn_asm.c:    i = BN_BITS2 - i;
bn_asm.c:        h -= d;
bn_asm.c:        h = (h << i) | (l >> (BN_BITS2 - i));
bn_asm.c:            t = h - th;
bn_asm.c:            q--;
bn_asm.c:            th -= dh;
bn_asm.c:            tl -= dl;
bn_asm.c:        l -= tl;
bn_asm.c:            q--;
bn_asm.c:        h -= th;
bn_asm.c:        if (--count == 0)
bn_asm.c:        n -= 4;
bn_asm.c:        n--;
bn_asm.c:        n -= 4;
bn_asm.c:        n--;
bn_asm.c:        r[0] = (t1 - t2 - c) & BN_MASK2;
bn_asm.c:        r[1] = (t1 - t2 - c) & BN_MASK2;
bn_asm.c:        r[2] = (t1 - t2 - c) & BN_MASK2;
bn_asm.c:        r[3] = (t1 - t2 - c) & BN_MASK2;
bn_asm.c:        n -= 4;
bn_asm.c:        r[0] = (t1 - t2 - c) & BN_MASK2;
bn_asm.c:        n--;
bn_asm.c:/* mul_add_c(a,b,c0,c1,c2)  -- c+=a*b for three word number c=(c2,c1,c0) */
bn_asm.c:/* mul_add_c2(a,b,c0,c1,c2) -- c+=2*a*b for three word number c=(c2,c1,c0) */
bn_asm.c:/* sqr_add_c(a,i,c0,c1,c2)  -- c+=a[i]^2 for three word number c=(c2,c1,c0) */
bn_asm.c: * sqr_add_c2(a,i,c0,c1,c2) -- c+=2*a[i]*a[j] for three word number
bn_asm.c: * overflow, because its high half cannot be all-ones.
bn_asm.c: * the high word of a multiplication result cannot be all-ones.
bn_asm.c: * the high word of a multiplication result cannot be all-ones.
bn_asm.c: * the high word of a multiplication result cannot be all-ones.
bn_asm.c: * result in performance improvement. E.g. on IA-32 this routine was
bn_asm.c: * platform-specific assembler. Mentioned numbers apply to compiler
bn_asm.c: * generated code compiled with and without -DOPENSSL_BN_ASM_MONT and
bn_asm.c:#   if 0                        /* template for platform-specific
bn_asm.c:            tp[j - 1] = c1 & BN_MASK2;
bn_asm.c:        tp[num - 1] = c1;
bn_asm.c:    if (tp[num] != 0 || tp[num - 1] >= np[num - 1]) {
bn_asm.c: * code-path.
bn_asm.c:    if (tp[num] != 0 || tp[num - 1] >= np[num - 1]) {
bn_blind.c: * Copyright 1998-2021 The OpenSSL Project Authors. All Rights Reserved.
bn_blind.c:    ret->lock = CRYPTO_THREAD_lock_new();
bn_blind.c:    if (ret->lock == NULL) {
bn_blind.c:        if ((ret->A = BN_dup(A)) == NULL)
bn_blind.c:        if ((ret->Ai = BN_dup(Ai)) == NULL)
bn_blind.c:    if ((ret->mod = BN_dup(mod)) == NULL)
bn_blind.c:        BN_set_flags(ret->mod, BN_FLG_CONSTTIME);
bn_blind.c:     * Set the counter to the special value -1 to indicate that this is
bn_blind.c:     * never-used fresh blinding that does not need updating before first
bn_blind.c:    ret->counter = -1;
bn_blind.c:    BN_free(r->A);
bn_blind.c:    BN_free(r->Ai);
bn_blind.c:    BN_free(r->e);
bn_blind.c:    BN_free(r->mod);
bn_blind.c:    CRYPTO_THREAD_lock_free(r->lock);
bn_blind.c:    if ((b->A == NULL) || (b->Ai == NULL)) {
bn_blind.c:    if (b->counter == -1)
bn_blind.c:        b->counter = 0;
bn_blind.c:    if (++b->counter == BN_BLINDING_COUNTER && b->e != NULL &&
bn_blind.c:        !(b->flags & BN_BLINDING_NO_RECREATE)) {
bn_blind.c:        /* re-create blinding parameters */
bn_blind.c:    } else if (!(b->flags & BN_BLINDING_NO_UPDATE)) {
bn_blind.c:        if (b->m_ctx != NULL) {
bn_blind.c:            if (!bn_mul_mont_fixed_top(b->Ai, b->Ai, b->Ai, b->m_ctx, ctx)
bn_blind.c:                || !bn_mul_mont_fixed_top(b->A, b->A, b->A, b->m_ctx, ctx))
bn_blind.c:            if (!BN_mod_mul(b->Ai, b->Ai, b->Ai, b->mod, ctx)
bn_blind.c:                || !BN_mod_mul(b->A, b->A, b->A, b->mod, ctx))
bn_blind.c:    if (b->counter == BN_BLINDING_COUNTER)
bn_blind.c:        b->counter = 0;
bn_blind.c:    if ((b->A == NULL) || (b->Ai == NULL)) {
bn_blind.c:    if (b->counter == -1)
bn_blind.c:        b->counter = 0;
bn_blind.c:    if (r != NULL && (BN_copy(r, b->Ai) == NULL))
bn_blind.c:    if (b->m_ctx != NULL)
bn_blind.c:        ret = BN_mod_mul_montgomery(n, n, b->A, b->m_ctx, ctx);
bn_blind.c:        ret = BN_mod_mul(n, n, b->A, b->mod, ctx);
bn_blind.c:    if (r == NULL && (r = b->Ai) == NULL) {
bn_blind.c:    if (b->m_ctx != NULL) {
bn_blind.c:        /* ensure that BN_mod_mul_montgomery takes pre-defined path */
bn_blind.c:        if (n->dmax >= r->top) {
bn_blind.c:            size_t i, rtop = r->top, ntop = n->top;
bn_blind.c:                mask = (BN_ULONG)0 - ((i - ntop) >> (8 * sizeof(i) - 1));
bn_blind.c:                n->d[i] &= mask;
bn_blind.c:            mask = (BN_ULONG)0 - ((rtop - ntop) >> (8 * sizeof(ntop) - 1));
bn_blind.c:            /* always true, if (rtop >= ntop) n->top = r->top; */
bn_blind.c:            n->top = (int)(rtop & ~mask) | (ntop & mask);
bn_blind.c:            n->flags |= (BN_FLG_FIXED_TOP & ~mask);
bn_blind.c:        ret = BN_mod_mul_montgomery(n, n, r, b->m_ctx, ctx);
bn_blind.c:        ret = BN_mod_mul(n, n, r, b->mod, ctx);
bn_blind.c:    return CRYPTO_THREAD_compare_id(CRYPTO_THREAD_get_current_id(), b->tid);
bn_blind.c:    b->tid = CRYPTO_THREAD_get_current_id();
bn_blind.c:    return CRYPTO_THREAD_write_lock(b->lock);
bn_blind.c:    return CRYPTO_THREAD_unlock(b->lock);
bn_blind.c:    return b->flags;
bn_blind.c:    b->flags = flags;
bn_blind.c:    if (ret->A == NULL && (ret->A = BN_new()) == NULL)
bn_blind.c:    if (ret->Ai == NULL && (ret->Ai = BN_new()) == NULL)
bn_blind.c:        BN_free(ret->e);
bn_blind.c:        ret->e = BN_dup(e);
bn_blind.c:    if (ret->e == NULL)
bn_blind.c:        ret->bn_mod_exp = bn_mod_exp;
bn_blind.c:        ret->m_ctx = m_ctx;
bn_blind.c:        if (!BN_priv_rand_range_ex(ret->A, ret->mod, 0, ctx))
bn_blind.c:        if (int_bn_mod_inverse(ret->Ai, ret->A, ret->mod, ctx, &rv))
bn_blind.c:        if (retry_counter-- == 0) {
bn_blind.c:    if (ret->bn_mod_exp != NULL && ret->m_ctx != NULL) {
bn_blind.c:        if (!ret->bn_mod_exp(ret->A, ret->A, ret->e, ret->mod, ctx, ret->m_ctx))
bn_blind.c:        if (!BN_mod_exp(ret->A, ret->A, ret->e, ret->mod, ctx))
bn_blind.c:    if (ret->m_ctx != NULL) {
bn_blind.c:        if (!bn_to_mont_fixed_top(ret->Ai, ret->Ai, ret->m_ctx, ctx)
bn_blind.c:            || !bn_to_mont_fixed_top(ret->A, ret->A, ret->m_ctx, ctx))
bn_const.c: * Copyright 2005-2021 The OpenSSL Project Authors. All Rights Reserved.
bn_const.c:/*-
bn_const.c: * The prime is: 2^768 - 2 ^704 - 1 + 2^64 * { [2^638 pi] + 149686 }
bn_const.c:/*-
bn_const.c: * The prime is: 2^1024 - 2^960 - 1 + 2^64 * { [2^894 pi] + 129093 }.
bn_const.c:/*-
bn_const.c: * "1536-bit MODP Group" from RFC3526, Section 2.
bn_const.c: * The prime is: 2^1536 - 2^1472 - 1 + 2^64 * { [2^1406 pi] + 741804 }
bn_const.c:/*-
bn_const.c: * "2048-bit MODP Group" from RFC3526, Section 3.
bn_const.c: * The prime is: 2^2048 - 2^1984 - 1 + 2^64 * { [2^1918 pi] + 124476 }
bn_const.c:/*-
bn_const.c: * "3072-bit MODP Group" from RFC3526, Section 4.
bn_const.c: * The prime is: 2^3072 - 2^3008 - 1 + 2^64 * { [2^2942 pi] + 1690314 }
bn_const.c:/*-
bn_const.c: * "4096-bit MODP Group" from RFC3526, Section 5.
bn_const.c: * The prime is: 2^4096 - 2^4032 - 1 + 2^64 * { [2^3966 pi] + 240904 }
bn_const.c:/*-
bn_const.c: * "6144-bit MODP Group" from RFC3526, Section 6.
bn_const.c: * The prime is: 2^6144 - 2^6080 - 1 + 2^64 * { [2^6014 pi] + 929484 }
bn_const.c:/*-
bn_const.c: * "8192-bit MODP Group" from RFC3526, Section 7.
bn_const.c: * The prime is: 2^8192 - 2^8128 - 1 + 2^64 * { [2^8062 pi] + 4743158 }
bn_conv.c: * Copyright 1995-2020 The OpenSSL Project Authors. All Rights Reserved.
bn_conv.c:    buf = OPENSSL_malloc(a->top * BN_BYTES * 2 + 2);
bn_conv.c:    if (a->neg)
bn_conv.c:        *p++ = '-';
bn_conv.c:    for (i = a->top - 1; i >= 0; i--) {
bn_conv.c:        for (j = BN_BITS2 - 8; j >= 0; j -= 8) {
bn_conv.c:            v = (int)((a->d[i] >> j) & 0xff);
bn_conv.c:    /*-
bn_conv.c:            *p++ = '-';
bn_conv.c:            if (lp - bn_data >= bn_data_num)
bn_conv.c:            if (*lp == (BN_ULONG)-1)
bn_conv.c:        lp--;
bn_conv.c:        n = BIO_snprintf(p, tbytes - (size_t)(p - buf), BN_DEC_FMT1, *lp);
bn_conv.c:            lp--;
bn_conv.c:            n = BIO_snprintf(p, tbytes - (size_t)(p - buf), BN_DEC_FMT2, *lp);
bn_conv.c:    if (*a == '-') {
bn_conv.c:            c = a[j - m];
bn_conv.c:            if (--m <= 0) {
bn_conv.c:                ret->d[h++] = l;
bn_conv.c:        j -= BN_BYTES * 2;
bn_conv.c:    ret->top = h;
bn_conv.c:    if (ret->top != 0)
bn_conv.c:        ret->neg = neg;
bn_conv.c:    if (*a == '-') {
bn_conv.c:    j = BN_DEC_NUM - i % BN_DEC_NUM;
bn_conv.c:    while (--i >= 0) {
bn_conv.c:        l += *a - '0';
bn_conv.c:    if (ret->top != 0)
bn_conv.c:        ret->neg = neg;
bn_conv.c:    if (*p == '-')
bn_conv.c:    if (*a == '-' && (*bn)->top != 0)
bn_conv.c:        (*bn)->neg = 1;
bn_ctx.c: * Copyright 2000-2021 The OpenSSL Project Authors. All Rights Reserved.
bn_ctx.c:/* The stack frame info is resizing, set a first-time expansion size; */
bn_ctx.c:    /* Linked-list admin */
bn_ctx.c:/* A linked-list of bignums grouped in bundles */
bn_ctx.c:    /* Linked-list admin */
bn_ctx.c:    BN_POOL_ITEM *item = ctx->pool.head;
bn_ctx.c:    BN_STACK *stack = &ctx->stack;
bn_ctx.c:    while (bnidx < ctx->used) {
bn_ctx.c:                   item->vals[bnidx++ % BN_CTX_POOL_SIZE].dmax);
bn_ctx.c:            item = item->next;
bn_ctx.c:    while (fpidx < stack->depth) {
bn_ctx.c:        while (bnidx++ < stack->indexes[fpidx])
bn_ctx.c:    BN_POOL_init(&ret->pool);
bn_ctx.c:    BN_STACK_init(&ret->stack);
bn_ctx.c:    ret->libctx = ctx;
bn_ctx.c:        ret->flags = BN_FLG_SECURE;
bn_ctx.c:        BN_POOL_ITEM *pool = ctx->pool.head;
bn_ctx.c:                   "BN_CTX_free(): stack-size=%d, pool-bignums=%d\n",
bn_ctx.c:                   ctx->stack.size, ctx->pool.size);
bn_ctx.c:                BIO_printf(trc_out, "%02x ", pool->vals[loop++].dmax);
bn_ctx.c:            pool = pool->next;
bn_ctx.c:    BN_STACK_finish(&ctx->stack);
bn_ctx.c:    BN_POOL_finish(&ctx->pool);
bn_ctx.c:    if (ctx->err_stack || ctx->too_many)
bn_ctx.c:        ctx->err_stack++;
bn_ctx.c:    else if (!BN_STACK_push(&ctx->stack, ctx->used)) {
bn_ctx.c:        ctx->err_stack++;
bn_ctx.c:    if (ctx->err_stack)
bn_ctx.c:        ctx->err_stack--;
bn_ctx.c:        unsigned int fp = BN_STACK_pop(&ctx->stack);
bn_ctx.c:        if (fp < ctx->used)
bn_ctx.c:            BN_POOL_release(&ctx->pool, ctx->used - fp);
bn_ctx.c:        ctx->used = fp;
bn_ctx.c:        ctx->too_many = 0;
bn_ctx.c:    if (ctx->err_stack || ctx->too_many)
bn_ctx.c:    if ((ret = BN_POOL_get(&ctx->pool, ctx->flags)) == NULL) {
bn_ctx.c:        ctx->too_many = 1;
bn_ctx.c:    ret->flags &= (~BN_FLG_CONSTTIME);
bn_ctx.c:    ctx->used++;
bn_ctx.c:    return ctx->libctx;
bn_ctx.c:    st->indexes = NULL;
bn_ctx.c:    st->depth = st->size = 0;
bn_ctx.c:    OPENSSL_free(st->indexes);
bn_ctx.c:    st->indexes = NULL;
bn_ctx.c:    if (st->depth == st->size) {
bn_ctx.c:            st->size ? (st->size * 3 / 2) : BN_CTX_START_FRAMES;
bn_ctx.c:        if (st->depth)
bn_ctx.c:            memcpy(newitems, st->indexes, sizeof(*newitems) * st->depth);
bn_ctx.c:        OPENSSL_free(st->indexes);
bn_ctx.c:        st->indexes = newitems;
bn_ctx.c:        st->size = newsize;
bn_ctx.c:    st->indexes[(st->depth)++] = idx;
bn_ctx.c:    return st->indexes[--(st->depth)];
bn_ctx.c:    p->head = p->current = p->tail = NULL;
bn_ctx.c:    p->used = p->size = 0;
bn_ctx.c:    while (p->head) {
bn_ctx.c:        for (loop = 0, bn = p->head->vals; loop++ < BN_CTX_POOL_SIZE; bn++)
bn_ctx.c:            if (bn->d)
bn_ctx.c:        p->current = p->head->next;
bn_ctx.c:        OPENSSL_free(p->head);
bn_ctx.c:        p->head = p->current;
bn_ctx.c:    if (p->used == p->size) {
bn_ctx.c:        for (loop = 0, bn = item->vals; loop++ < BN_CTX_POOL_SIZE; bn++) {
bn_ctx.c:        item->prev = p->tail;
bn_ctx.c:        item->next = NULL;
bn_ctx.c:        if (p->head == NULL)
bn_ctx.c:            p->head = p->current = p->tail = item;
bn_ctx.c:            p->tail->next = item;
bn_ctx.c:            p->tail = item;
bn_ctx.c:            p->current = item;
bn_ctx.c:        p->size += BN_CTX_POOL_SIZE;
bn_ctx.c:        p->used++;
bn_ctx.c:        return item->vals;
bn_ctx.c:    if (!p->used)
bn_ctx.c:        p->current = p->head;
bn_ctx.c:    else if ((p->used % BN_CTX_POOL_SIZE) == 0)
bn_ctx.c:        p->current = p->current->next;
bn_ctx.c:    return p->current->vals + ((p->used++) % BN_CTX_POOL_SIZE);
bn_ctx.c:    unsigned int offset = (p->used - 1) % BN_CTX_POOL_SIZE;
bn_ctx.c:    p->used -= num;
bn_ctx.c:    while (num--) {
bn_ctx.c:        bn_check_top(p->current->vals + offset);
bn_ctx.c:            offset = BN_CTX_POOL_SIZE - 1;
bn_ctx.c:            p->current = p->current->prev;
bn_ctx.c:            offset--;
bn_depr.c: * Copyright 2002-2021 The OpenSSL Project Authors. All Rights Reserved.
bn_depr.c: * Support for deprecated functions goes here - static linkage will only
bn_depr.c:    /* we have a prime :-) */
bn_dh.c: * Copyright 2014-2021 The OpenSSL Project Authors. All Rights Reserved.
bn_dh.c: * "1536-bit MODP Group" from RFC3526, Section 2.
bn_dh.c: * The prime is: 2^1536 - 2^1472 - 1 + 2^64 * { [2^1406 pi] + 741804 }
bn_dh.c:/* q = (p - 1) / 2 */
bn_dh.c:/*-
bn_dh.c: * "2048-bit MODP Group" from RFC3526, Section 3.
bn_dh.c: * The prime is: 2^2048 - 2^1984 - 1 + 2^64 * { [2^1918 pi] + 124476 }
bn_dh.c:/* q = (p - 1) / 2 */
bn_dh.c:/*-
bn_dh.c: * "3072-bit MODP Group" from RFC3526, Section 4.
bn_dh.c: * The prime is: 2^3072 - 2^3008 - 1 + 2^64 * { [2^2942 pi] + 1690314 }
bn_dh.c:/* q = (p - 1) / 2 */
bn_dh.c:/*-
bn_dh.c: * "4096-bit MODP Group" from RFC3526, Section 5.
bn_dh.c: * The prime is: 2^4096 - 2^4032 - 1 + 2^64 * { [2^3966 pi] + 240904 }
bn_dh.c:/* q = (p - 1) / 2 */
bn_dh.c:/*-
bn_dh.c: * "6144-bit MODP Group" from RFC3526, Section 6.
bn_dh.c: * The prime is: 2^6144 - 2^6080 - 1 + 2^64 * { [2^6014 pi] + 929484 }
bn_dh.c:/* q = (p - 1) / 2 */
bn_dh.c: * "8192-bit MODP Group" from RFC3526, Section 7.
bn_dh.c: * The prime is: 2^8192 - 2^8128 - 1 + 2^64 * { [2^8062 pi] + 4743158 }
bn_dh.c:/* q = (p - 1) / 2 */
bn_dh.c:/* q = (p - 1) / 2 */
bn_dh.c:/* q = (p - 1) / 2 */
bn_dh.c:/* q = (p - 1) / 2 */
bn_dh.c:/* q = (p - 1) / 2 */
bn_dh.c:/* q = (p - 1) / 2 */
bn_div.c: * Copyright 1995-2022 The OpenSSL Project Authors. All Rights Reserved.
bn_div.c:     * The next 2 are needed so we can do a dv->d[0]|=1 later since
bn_div.c:     * BN_lshift1 will only work once there is a value :-)
bn_div.c:    dv->top = 1;
bn_div.c:    if (!BN_lshift(D, D, nm - nd))
bn_div.c:    for (i = nm - nd; i >= 0; i--) {
bn_div.c:            dv->d[0] |= 1;
bn_div.c:    rem->neg = BN_is_zero(rem) ? 0 : m->neg;
bn_div.c:    dv->neg = m->neg ^ d->neg;
bn_div.c: * This is #if-ed away, because it's a reference for assembly implementations,
bn_div.c: * where it can and should be made constant-time. But if you want to test it,
bn_div.c: * and less significant limb is referred at |m[-1]|. This means that caller
bn_div.c: * is responsible for ensuring that |m[-1]| is valid. Second condition that
bn_div.c: * other words divisor has to be "bit-aligned to the left." bn_div_fixed_top
bn_div.c:    BN_ULLONG R = ((BN_ULLONG)m[0] << BN_BITS2) | m[-1];
bn_div.c:            R -= D;
bn_div.c:    mask = 0 - (Q >> (BN_BITS2 - 1));   /* does it overflow? */
bn_div.c:    BN_ULONG *d = num->d, n, m, rmask;
bn_div.c:    int top = num->top;
bn_div.c:    int rshift = BN_num_bits_word(d[top - 1]), lshift, i;
bn_div.c:    lshift = BN_BITS2 - rshift;
bn_div.c:    rmask = (BN_ULONG)0 - rshift;  /* rmask = 0 - (rshift != 0) */
bn_div.c:   /*-
bn_div.c:    * - GNU C generates a call to a function (__udivdi3 to be exact)
bn_div.c:    * - divl doesn't only calculate quotient, but also leaves
bn_div.c:    *   remainder in %edx which we can definitely use here:-)
bn_div.c:    * Same story here, but it's 128-bit by 64-bit division. Wow!
bn_div.c:/*-
bn_div.c: *     dv->neg == num->neg ^ divisor->neg  (unless the result is zero)
bn_div.c: *     rm->neg == num->neg                 (unless the remainder is zero)
bn_div.c:     * Invalid zero-padding would have particularly bad consequences so don't
bn_div.c:    if (divisor->d[divisor->top - 1] == 0) {
bn_div.c: * public, but not *value*. Former is likely to be pre-defined by
bn_div.c: * may not be zero-padded, yet claim this subroutine "constant-time"(*).
bn_div.c: * This is because zero-padded dividend, |num|, is tolerated, so that
bn_div.c: * contain correspongly less significant limbs as well, and will be zero-
bn_div.c: * as divisor, also zero-padded if needed. These actually leave sign bits
bn_div.c: * zero-padded zeros would retain sign.
bn_div.c: * (*) "Constant-time-ness" has two pre-conditions:
bn_div.c: *     - availability of constant-time bn_div_3_words;
bn_div.c: *     - dividend is at least as "wide" as divisor, limb-wise, zero-padded
bn_div.c:    assert(divisor->top > 0 && divisor->d[divisor->top - 1] != 0);
bn_div.c:    sdiv->neg = 0;
bn_div.c:    div_n = sdiv->top;
bn_div.c:    num_n = snum->top;
bn_div.c:        /* caller didn't pad dividend -> no constant-time guarantee... */
bn_div.c:        memset(&(snum->d[num_n]), 0, (div_n - num_n + 1) * sizeof(BN_ULONG));
bn_div.c:        snum->top = num_n = div_n + 1;
bn_div.c:    loop = num_n - div_n;
bn_div.c:    wnum = &(snum->d[loop]);
bn_div.c:    wnumtop = &(snum->d[num_n - 1]);
bn_div.c:    d0 = sdiv->d[div_n - 1];
bn_div.c:    d1 = (div_n == 1) ? 0 : sdiv->d[div_n - 2];
bn_div.c:    num_neg = num->neg;
bn_div.c:    res->neg = (num_neg ^ divisor->neg);
bn_div.c:    res->top = loop;
bn_div.c:    res->flags |= BN_FLG_FIXED_TOP;
bn_div.c:    resp = &(res->d[loop]);
bn_div.c:    for (i = 0; i < loop; i++, wnumtop--) {
bn_div.c:         * to calculate a BN_ULONG q such that | wnum - sdiv * q | < sdiv
bn_div.c:        n1 = wnumtop[-1];
bn_div.c:            BN_ULONG n2 = (wnumtop == wnum) ? 0 : wnumtop[-2];
bn_div.c:            rem = (n1 - q * d0) & BN_MASK2;
bn_div.c:                q--;
bn_div.c:                t2 -= d1;
bn_div.c:            rem = (n1 - q * d0) & BN_MASK2;
bn_div.c:                q--;
bn_div.c:                    t2h--;
bn_div.c:                t2l -= d1;
bn_div.c:        l0 = bn_mul_words(tmp->d, sdiv->d, div_n, q);
bn_div.c:        tmp->d[div_n] = l0;
bn_div.c:        wnum--;
bn_div.c:        l0 = bn_sub_words(wnum, wnum, tmp->d, div_n + 1);
bn_div.c:        q -= l0;
bn_div.c:         * then (q-1) * sdiv is less or equal than wnum)
bn_div.c:        for (l0 = 0 - l0, j = 0; j < div_n; j++)
bn_div.c:            tmp->d[j] = sdiv->d[j] & l0;
bn_div.c:        l0 = bn_add_words(wnum, wnum, tmp->d, div_n);
bn_div.c:        *--resp = q;
bn_div.c:    snum->neg = num_neg;
bn_div.c:    snum->top = div_n;
bn_div.c:    snum->flags |= BN_FLG_FIXED_TOP;
bn_err.c: * Copyright 1995-2022 The OpenSSL Project Authors. All Rights Reserved.
bn_exp.c: * Copyright 1995-2022 The OpenSSL Project Authors. All Rights Reserved.
bn_exp.c:/* this one works - simple but works */
bn_exp.c:    /*-
bn_exp.c:     * exponentiation using the reciprocal-based quick remaindering
bn_exp.c:     *   BN_mod_exp_mont   33 .. 40 %  [AMD K6-2, Linux, debug configuration]
bn_exp.c:     *                                  debug-solaris-sparcv8-gcc conf.]
bn_exp.c:     *   BN_mod_exp_recp   50 .. 70 %  [AMD K6-2, Linux, debug configuration]
bn_exp.c:     *                     62 .. 118 % [UltraSparc, debug-solaris-sparcv8-gcc]
bn_exp.c:     * "Real" timings [linux-elf, solaris-sparcv9-gcc configurations]
bn_exp.c:        if (a->top == 1 && !a->neg
bn_exp.c:            BN_ULONG A = a->d[0];
bn_exp.c:        /* x**0 mod 1, or x**0 mod -1 is still zero. */
bn_exp.c:    if (m->neg) {
bn_exp.c:        aa->neg = 0;
bn_exp.c:        j = 1 << (window - 1);
bn_exp.c:                !BN_mod_mul_reciprocal(val[i], val[i - 1], aa, &recp, ctx))
bn_exp.c:    wstart = bits - 1;          /* The top bit of the window */
bn_exp.c:            wstart--;
bn_exp.c:            if (wstart - i < 0)
bn_exp.c:            if (BN_is_bit_set(p, wstart - i)) {
bn_exp.c:                wvalue <<= (i - wend);
bn_exp.c:        wstart -= wend + 1;
bn_exp.c:        /* x**0 mod 1, or x**0 mod -1 is still zero. */
bn_exp.c:    if (a->neg || BN_ucmp(a, m) >= 0) {
bn_exp.c:        j = 1 << (window - 1);
bn_exp.c:                !bn_mul_mont_fixed_top(val[i], val[i - 1], d, mont, ctx))
bn_exp.c:    wstart = bits - 1;          /* The top bit of the window */
bn_exp.c:    j = m->top;                 /* borrow j */
bn_exp.c:    if (m->d[j - 1] & (((BN_ULONG)1) << (BN_BITS2 - 1))) {
bn_exp.c:        /* 2^(top*BN_BITS2) - m */
bn_exp.c:        r->d[0] = (0 - m->d[0]) & BN_MASK2;
bn_exp.c:            r->d[i] = (~m->d[i]) & BN_MASK2;
bn_exp.c:        r->top = j;
bn_exp.c:        r->flags |= BN_FLG_FIXED_TOP;
bn_exp.c:            wstart--;
bn_exp.c:            if (wstart - i < 0)
bn_exp.c:            if (BN_is_bit_set(p, wstart - i)) {
bn_exp.c:                wvalue <<= (i - wend);
bn_exp.c:        wstart -= wend + 1;
bn_exp.c:     * Done with zero-padded intermediate BIGNUMs. Final BN_from_montgomery
bn_exp.c:        j = mont->N.top;        /* borrow j */
bn_exp.c:        val[0]->d[0] = 1;       /* borrow val[0] */
bn_exp.c:            val[0]->d[i] = 0;
bn_exp.c:        val[0]->top = j;
bn_exp.c:    if (wordpos >= 0 && wordpos < a->top) {
bn_exp.c:        ret = a->d[wordpos] & BN_MASK2;
bn_exp.c:            if (++wordpos < a->top)
bn_exp.c:                ret |= a->d[wordpos] << (BN_BITS2 - bitpos);
bn_exp.c:    if (top > b->top)
bn_exp.c:        top = b->top;           /* this works because 'buf' is explicitly
bn_exp.c:        table[j] = b->d[i];
bn_exp.c:                       ((BN_ULONG)0 - (constant_time_eq_int(j,idx)&1));
bn_exp.c:            b->d[i] = acc;
bn_exp.c:        int xstride = 1 << (window - 2);
bn_exp.c:        i = idx >> (window - 2);        /* equivalent of idx / xstride */
bn_exp.c:        idx &= xstride - 1;             /* equivalent of idx % xstride */
bn_exp.c:        y0 = (BN_ULONG)0 - (constant_time_eq_int(i,0)&1);
bn_exp.c:        y1 = (BN_ULONG)0 - (constant_time_eq_int(i,1)&1);
bn_exp.c:        y2 = (BN_ULONG)0 - (constant_time_eq_int(i,2)&1);
bn_exp.c:        y3 = (BN_ULONG)0 - (constant_time_eq_int(i,3)&1);
bn_exp.c:                       & ((BN_ULONG)0 - (constant_time_eq_int(j,idx)&1));
bn_exp.c:            b->d[i] = acc;
bn_exp.c:    b->top = top;
bn_exp.c:    b->flags |= BN_FLG_FIXED_TOP;
bn_exp.c:        ((unsigned char*)(x_) + (MOD_EXP_CTIME_MIN_CACHE_LINE_WIDTH - (((size_t)(x_)) & (MOD_EXP_CTIME_MIN_CACHE_LINE_MASK))))
bn_exp.c: * precomputation memory layout to limit data-dependency to a minimum to
bn_exp.c: * protect secret exponents (cf. the hyper-threading timing attacks pointed
bn_exp.c: * http://www.daemonology.net/hyperthreading-considered-harmful/)
bn_exp.c:    top = m->top;
bn_exp.c:    bits = p->top * BN_BITS2;
bn_exp.c:        /* x**0 mod 1, or x**0 mod -1 is still zero. */
bn_exp.c:    if (a->neg || BN_ucmp(a, m) >= 0) {
bn_exp.c:    if ((16 == a->top) && (16 == p->top) && (BN_num_bits(m) == 1024)
bn_exp.c:        RSAZ_1024_mod_exp_avx2(rr->d, a->d, p->d, m->d, mont->RR.d,
bn_exp.c:                               mont->n0[0]);
bn_exp.c:        rr->top = 16;
bn_exp.c:        rr->neg = 0;
bn_exp.c:    } else if ((8 == a->top) && (8 == p->top) && (BN_num_bits(m) == 512)) {
bn_exp.c:        RSAZ_512_mod_exp(rr->d, a->d, p->d, m->d, mont->n0[0], mont->RR.d);
bn_exp.c:        rr->top = 8;
bn_exp.c:        rr->neg = 0;
bn_exp.c:        /* reserve space for mont->N.d[] copy */
bn_exp.c:        powerbufLen += top * sizeof(mont->N.d[0]);
bn_exp.c:     * Allocate a buffer large enough to hold all of the pre-computed powers
bn_exp.c:    powerbufLen += sizeof(m->d[0]) * (top * numPowers +
bn_exp.c:    tmp.d = (BN_ULONG *)(powerbuf + sizeof(m->d[0]) * top * numPowers);
bn_exp.c:    if (m->d[top - 1] & (((BN_ULONG)1) << (BN_BITS2 - 1))) {
bn_exp.c:        /* 2^(top*BN_BITS2) - m */
bn_exp.c:        tmp.d[0] = (0 - m->d[0]) & BN_MASK2;
bn_exp.c:            tmp.d[i] = (~m->d[i]) & BN_MASK2;
bn_exp.c:        bn_pwr5_mont_f pwr5_worker = pwr5_funcs[top / 16 - 1];
bn_exp.c:        bn_mul_mont_f mul_worker = mul_funcs[top / 16 - 1];
bn_exp.c:        BN_ULONG *np = mont->N.d, *n0 = mont->n0;
bn_exp.c:        int stride = 5 * (6 - (top / 16 - 1)); /* multiple of 5, but less
bn_exp.c:            /* Calculate a^i = a^(i-1) * a */
bn_exp.c:        /* switch to 64-bit domain */
bn_exp.c:        bn_flip_t4(np, mont->N.d, top);
bn_exp.c:         * The exponent may not have a whole number of fixed-size windows.
bn_exp.c:         * full-window-size bits such that what remains is always a whole
bn_exp.c:        window0 = (bits - 1) % 5 + 1;
bn_exp.c:        wmask = (1 << window0) - 1;
bn_exp.c:        bits -= window0;
bn_exp.c:            bits -= stride;
bn_exp.c:            bits += stride - 5;
bn_exp.c:            wvalue >>= stride - 5;
bn_exp.c:        /* back to 32-bit domain */
bn_exp.c:         * specifically optimization of cache-timing attack countermeasures,
bn_exp.c:         * pre-computation optimization, and Almost Montgomery Multiplication.
bn_exp.c:         * The paper discusses a 4-bit window to optimize 512-bit modular
bn_exp.c:         * exponentiation, used in RSA-1024 with CRT, but RSA-1024 is no longer
bn_exp.c:        BN_ULONG *n0 = mont->n0, *np;
bn_exp.c:         * copy mont->N.d[] to improve cache locality
bn_exp.c:            np[i] = mont->N.d[i];
bn_exp.c:            /* Calculate a^i = a^(i-1) * a */
bn_exp.c:            bn_mul_mont_gather5(tmp.d, am.d, powerbuf, np, n0, top, i - 1);
bn_exp.c:            bn_mul_mont_gather5(tmp.d, am.d, powerbuf, np, n0, top, i - 1);
bn_exp.c:            bn_mul_mont_gather5(tmp.d, am.d, powerbuf, np, n0, top, i - 1);
bn_exp.c:            bn_mul_mont_gather5(tmp.d, am.d, powerbuf, np, n0, top, i - 1);
bn_exp.c:         * The exponent may not have a whole number of fixed-size windows.
bn_exp.c:         * full-window-size bits such that what remains is always a whole
bn_exp.c:        window0 = (bits - 1) % 5 + 1;
bn_exp.c:        wmask = (1 << window0) - 1;
bn_exp.c:        bits -= window0;
bn_exp.c:                                    bn_get_bits5(p->d, bits -= 5));
bn_exp.c:                          bn_get_bits5(p->d, bits -= 5));
bn_exp.c:         * val[i=2..2^winsize-1]. Powers are computed as a*a^(i-1) (even
bn_exp.c:                /* Calculate a^i = a^(i-1) * a */
bn_exp.c:         * The exponent may not have a whole number of fixed-size windows.
bn_exp.c:         * full-window-size bits such that what remains is always a whole
bn_exp.c:        window0 = (bits - 1) % window + 1;
bn_exp.c:        wmask = (1 << window0) - 1;
bn_exp.c:        bits -= window0;
bn_exp.c:        wmask = (1 << window) - 1;
bn_exp.c:            /* Square the result window-size times */
bn_exp.c:             * EM (and likely other) side-channel attacks like One&Done
bn_exp.c:             * (for details see "One&Done: A Single-Decryption EM-Based
bn_exp.c:             *  Attack on OpenSSL's Constant-Time Blinded RSA" by M. Alam,
bn_exp.c:            bits -= window;
bn_exp.c:             * Fetch the appropriate pre-computed value from the pre-buf
bn_exp.c:     * Done with zero-padded intermediate BIGNUMs. Final BN_from_montgomery
bn_exp.c:    if (m->top == 1)
bn_exp.c:        a %= m->d[0];           /* make sure that 'a' is reduced */
bn_exp.c:        /* x**0 mod 1, or x**0 mod -1 is still zero. */
bn_exp.c:    /* bits-1 >= 0 */
bn_exp.c:    w = a;                      /* bit 'bits-1' of 'p' is always set */
bn_exp.c:    for (b = bits - 2; b >= 0; b--) {
bn_exp.c:/* The old fallback, simple version :-) */
bn_exp.c:        /* x**0 mod 1, or x**0 mod -1 is still zero. */
bn_exp.c:        j = 1 << (window - 1);
bn_exp.c:                !BN_mod_mul(val[i], val[i - 1], d, m, ctx))
bn_exp.c:    wstart = bits - 1;          /* The top bit of the window */
bn_exp.c:            wstart--;
bn_exp.c:            if (wstart - i < 0)
bn_exp.c:            if (BN_is_bit_set(p, wstart - i)) {
bn_exp.c:                wvalue <<= (i - wend);
bn_exp.c:        wstart -= wend + 1;
bn_exp.c: * parallel 2-primes exponentiation using 256-bit (AVX512VL) AVX512_IFMA ISA
bn_exp.c: * in 52-bit binary redundant representation.
bn_exp.c:        (((a1->top == 16) && (p1->top == 16) && (BN_num_bits(m1) == 1024) &&
bn_exp.c:          (a2->top == 16) && (p2->top == 16) && (BN_num_bits(m2) == 1024)) ||
bn_exp.c:         ((a1->top == 24) && (p1->top == 24) && (BN_num_bits(m1) == 1536) &&
bn_exp.c:          (a2->top == 24) && (p2->top == 24) && (BN_num_bits(m2) == 1536)) ||
bn_exp.c:         ((a1->top == 32) && (p1->top == 32) && (BN_num_bits(m1) == 2048) &&
bn_exp.c:          (a2->top == 32) && (p2->top == 32) && (BN_num_bits(m2) == 2048)))) {
bn_exp.c:        int topn = a1->top;
bn_exp.c:        ret = ossl_rsaz_mod_exp_avx512_x2(rr1->d, a1->d, p1->d, m1->d,
bn_exp.c:                                          mont1->RR.d, mont1->n0[0],
bn_exp.c:                                          rr2->d, a2->d, p2->d, m2->d,
bn_exp.c:                                          mont2->RR.d, mont2->n0[0],
bn_exp.c:        rr1->top = topn;
bn_exp.c:        rr1->neg = 0;
bn_exp.c:        rr2->top = topn;
bn_exp.c:        rr2->neg = 0;
bn_exp2.c: * Copyright 1995-2022 The OpenSSL Project Authors. All Rights Reserved.
bn_exp2.c:     * Build table for a1:   val1[i] := a1^(2*i + 1) mod m  for i = 0 .. 2^(window1-1)
bn_exp2.c:    if (a1->neg || BN_ucmp(a1, m) >= 0) {
bn_exp2.c:        j = 1 << (window1 - 1);
bn_exp2.c:                !BN_mod_mul_montgomery(val1[i], val1[i - 1], d, mont, ctx))
bn_exp2.c:     * Build table for a2:   val2[i] := a2^(2*i + 1) mod m  for i = 0 .. 2^(window2-1)
bn_exp2.c:    if (a2->neg || BN_ucmp(a2, m) >= 0) {
bn_exp2.c:        j = 1 << (window2 - 1);
bn_exp2.c:                !BN_mod_mul_montgomery(val2[i], val2[i - 1], d, mont, ctx))
bn_exp2.c:    for (b = bits - 1; b >= 0; b--) {
bn_exp2.c:                 * consider bits b-window1+1 .. b for this window
bn_exp2.c:                i = b - window1 + 1;
bn_exp2.c:                for (i = b - 1; i >= wpos1; i--) {
bn_exp2.c:                 * consider bits b-window2+1 .. b for this window
bn_exp2.c:                i = b - window2 + 1;
bn_exp2.c:                for (i = b - 1; i >= wpos2; i--) {
bn_gcd.c: * Copyright 1995-2020 The OpenSSL Project Authors. All Rights Reserved.
bn_gcd.c: * arguments: all passed pointers here are non-NULL.
bn_gcd.c:    A->neg = 0;
bn_gcd.c:    if (B->neg || (BN_ucmp(B, A) >= 0)) {
bn_gcd.c:    sign = -1;
bn_gcd.c:    /*-
bn_gcd.c:     *     -sign*X*a  ==  B   (mod |n|),
bn_gcd.c:        /*-
bn_gcd.c:         * (*) -sign*X*a  ==  B   (mod |n|),
bn_gcd.c:        /*-
bn_gcd.c:        /*-
bn_gcd.c:         *       sign*Y*a - D*A  ==  B    (mod |n|).
bn_gcd.c:         *      -sign*X*a  ==  A          (mod |n|).
bn_gcd.c:         * So if we set  (X, Y, sign) := (Y + D*X, X, -sign), we arrive back at
bn_gcd.c:         *      -sign*X*a  ==  B   (mod |n|),
bn_gcd.c:         * Note that  X  and  Y  stay non-negative all the time.
bn_gcd.c:        sign = -sign;
bn_gcd.c:    /*-
bn_gcd.c:     * where  Y  is non-negative.
bn_gcd.c:        if (!Y->neg && BN_ucmp(Y, n) < 0) {
bn_gcd.c: * all pointers passed here are assumed non-NULL.
bn_gcd.c:    A->neg = 0;
bn_gcd.c:    if (B->neg || (BN_ucmp(B, A) >= 0)) {
bn_gcd.c:    sign = -1;
bn_gcd.c:    /*-
bn_gcd.c:     *     -sign*X*a  ==  B   (mod |n|),
bn_gcd.c:         * (about 400 .. 500 bits on 32-bit systems, but much more on 64-bit
bn_gcd.c:            /*-
bn_gcd.c:             * (1) -sign*X*a  ==  B   (mod |n|),
bn_gcd.c:            /*-
bn_gcd.c:             * (1) -sign*X*a  ==  B   (mod |n|),
bn_gcd.c:                /* -sign*(X + Y)*a == B - A  (mod |n|) */
bn_gcd.c:                /*  sign*(X + Y)*a == A - B  (mod |n|) */
bn_gcd.c:            /*-
bn_gcd.c:             * (*) -sign*X*a  ==  B   (mod |n|),
bn_gcd.c:                         * M (= A - 2*B) already has the correct value
bn_gcd.c:                         * currently M = A - 2*B, but we need M = A - 3*B
bn_gcd.c:            /*-
bn_gcd.c:            /*-
bn_gcd.c:             *       sign*Y*a - D*A  ==  B    (mod |n|).
bn_gcd.c:             *      -sign*X*a  ==  A          (mod |n|).
bn_gcd.c:             * So if we set  (X, Y, sign) := (Y + D*X, X, -sign), we arrive back at
bn_gcd.c:             *      -sign*X*a  ==  B   (mod |n|),
bn_gcd.c:             * Note that  X  and  Y  stay non-negative all the time.
bn_gcd.c:                } else if (D->top == 1) {
bn_gcd.c:                    if (!BN_mul_word(tmp, D->d[0]))
bn_gcd.c:            sign = -sign;
bn_gcd.c:    /*-
bn_gcd.c:     * where  Y  is non-negative.
bn_gcd.c:        if (!Y->neg && BN_ucmp(Y, n) < 0) {
bn_gcd.c:/*-
bn_gcd.c: * This function is based on the constant-time GCD work by Bernstein and Yang:
bn_gcd.c:    /* Note 2: zero input corner cases are not constant-time since they are
bn_gcd.c:     * assumption without the need of side-channel information. */
bn_gcd.c:        r->neg = 0;
bn_gcd.c:        r->neg = 0;
bn_gcd.c:    for (i = 0; i < r->dmax && i < g->dmax; i++) {
bn_gcd.c:        mask = ~(r->d[i] | g->d[i]);
bn_gcd.c:    top = 1 + ((r->top >= g->top) ? r->top : g->top);
bn_gcd.c:    BN_consttime_swap((~r->d[0]) & 1, r, g, top);
bn_gcd.c:        cond = (-delta >> (8 * sizeof(delta) - 1)) & g->d[0] & 1
bn_gcd.c:            /* make sure g->top > 0 (i.e. if top == 0 then g == 0 always) */
bn_gcd.c:            & (~((g->top - 1) >> (sizeof(g->top) * 8 - 1)));
bn_gcd.c:        delta = (-cond & -delta) | ((cond - 1) & delta);
bn_gcd.c:        r->neg ^= cond;
bn_gcd.c:        BN_consttime_swap(g->d[0] & 1 /* g is odd */
bn_gcd.c:                /* make sure g->top > 0 (i.e. if top == 0 then g == 0 always) */
bn_gcd.c:                & (~((g->top - 1) >> (sizeof(g->top) * 8 - 1))),
bn_gcd.c:    r->neg = 0;
bn_gf2m.c: * Copyright 2002-2021 The OpenSSL Project Authors. All Rights Reserved.
bn_gf2m.c:/* Platform-specific macros to accelerate squaring. */
bn_gf2m.c: * Product of two polynomials a, b each with degree < BN_BITS2 - 1, result is
bn_gf2m.c: * a polynomial r with degree < 2 * BN_BITS - 1 The caller MUST ensure that
bn_gf2m.c: * Product of two polynomials a, b each with degree < 2 * BN_BITS2 - 1,
bn_gf2m.c: * result is a polynomial r with degree < 4 * BN_BITS2 - 1 The caller MUST
bn_gf2m.c:    if (a->top < b->top) {
bn_gf2m.c:    if (bn_wexpand(r, at->top) == NULL)
bn_gf2m.c:    for (i = 0; i < bt->top; i++) {
bn_gf2m.c:        r->d[i] = at->d[i] ^ bt->d[i];
bn_gf2m.c:    for (; i < at->top; i++) {
bn_gf2m.c:        r->d[i] = at->d[i];
bn_gf2m.c:    r->top = at->top;
bn_gf2m.c:/*-
bn_gf2m.c:        if (!bn_wexpand(r, a->top))
bn_gf2m.c:        for (j = 0; j < a->top; j++) {
bn_gf2m.c:            r->d[j] = a->d[j];
bn_gf2m.c:        r->top = a->top;
bn_gf2m.c:    z = r->d;
bn_gf2m.c:    for (j = r->top - 1; j > dN;) {
bn_gf2m.c:            j--;
bn_gf2m.c:            n = p[0] - p[k];
bn_gf2m.c:            d1 = BN_BITS2 - d0;
bn_gf2m.c:            z[j - n] ^= (zz >> d0);
bn_gf2m.c:                z[j - n - 1] ^= (zz << d1);
bn_gf2m.c:        d1 = BN_BITS2 - d0;
bn_gf2m.c:        z[j - n] ^= (zz >> d0);
bn_gf2m.c:            z[j - n - 1] ^= (zz << d1);
bn_gf2m.c:        d1 = BN_BITS2 - d0;
bn_gf2m.c:            d1 = BN_BITS2 - d0;
bn_gf2m.c:    zlen = a->top + b->top + 4;
bn_gf2m.c:    s->top = zlen;
bn_gf2m.c:        s->d[i] = 0;
bn_gf2m.c:    for (j = 0; j < b->top; j += 2) {
bn_gf2m.c:        y0 = b->d[j];
bn_gf2m.c:        y1 = ((j + 1) == b->top) ? 0 : b->d[j + 1];
bn_gf2m.c:        for (i = 0; i < a->top; i += 2) {
bn_gf2m.c:            x0 = a->d[i];
bn_gf2m.c:            x1 = ((i + 1) == a->top) ? 0 : a->d[i + 1];
bn_gf2m.c:                s->d[i + j + k] ^= zz[k];
bn_gf2m.c:    if (!bn_wexpand(s, 2 * a->top))
bn_gf2m.c:    for (i = a->top - 1; i >= 0; i--) {
bn_gf2m.c:        s->d[2 * i + 1] = SQR1(a->d[i]);
bn_gf2m.c:        s->d[2 * i] = SQR0(a->d[i]);
bn_gf2m.c:    s->top = 2 * a->top;
bn_gf2m.c:        int top = p->top;
bn_gf2m.c:        udp = u->d;
bn_gf2m.c:        for (i = u->top; i < top; i++)
bn_gf2m.c:        u->top = top;
bn_gf2m.c:        bdp = b->d;
bn_gf2m.c:        b->top = top;
bn_gf2m.c:        cdp = c->d;
bn_gf2m.c:        c->top = top;
bn_gf2m.c:        vdp = v->d;             /* It pays off to "cache" *->d pointers,
bn_gf2m.c:                                 * p->d, because *p is declared 'const'... */
bn_gf2m.c:                mask = (BN_ULONG)0 - (b0 & 1);
bn_gf2m.c:                b0 ^= p->d[0] & mask;
bn_gf2m.c:                for (i = 0; i < top - 1; i++) {
bn_gf2m.c:                    udp[i] = ((u0 >> 1) | (u1 << (BN_BITS2 - 1))) & BN_MASK2;
bn_gf2m.c:                    b1 = bdp[i + 1] ^ (p->d[i + 1] & mask);
bn_gf2m.c:                    bdp[i] = ((b0 >> 1) | (b1 << (BN_BITS2 - 1))) & BN_MASK2;
bn_gf2m.c:                ubits--;
bn_gf2m.c:                vdp = v->d;
bn_gf2m.c:                cdp = c->d;
bn_gf2m.c:                int utop = (ubits - 1) / BN_BITS2;
bn_gf2m.c:                    utop--;
bn_gf2m.c:/*-
bn_gf2m.c:        if (!BN_priv_rand_ex(b, BN_num_bits(p) - 1,
bn_gf2m.c: * could be a. Uses simple square-and-multiply algorithm A.5.1 from IEEE
bn_gf2m.c:    n = BN_num_bits(b) - 1;
bn_gf2m.c:    for (i = n - 1; i >= 0; i--) {
bn_gf2m.c:    if (!BN_set_bit(u, p[0] - 1))
bn_gf2m.c:        /* compute half-trace of a */
bn_gf2m.c:        for (j = 1; j <= (p[0] - 1) / 2; j++) {
bn_gf2m.c:            for (j = 1; j <= p[0] - 1; j++) {
bn_gf2m.c: * Convert the bit-string representation of a polynomial ( \sum_{i=0}^n a_i *
bn_gf2m.c: * x^i) into an array of integers corresponding to the bits with non-zero
bn_gf2m.c: * coefficient.  Array is terminated with -1. Up to max elements of the array
bn_gf2m.c:    for (i = a->top - 1; i >= 0; i--) {
bn_gf2m.c:        if (!a->d[i])
bn_gf2m.c:            /* skip word if a->d[i] == 0 */
bn_gf2m.c:        for (j = BN_BITS2 - 1; j >= 0; j--) {
bn_gf2m.c:            if (a->d[i] & mask) {
bn_gf2m.c:        p[k] = -1;
bn_gf2m.c: * bit-string.  The array must be terminated by -1.
bn_gf2m.c:    for (i = 0; p[i] != -1; i++) {
bn_intern.c: * Copyright 2014-2020 The OpenSSL Project Authors. All Rights Reserved.
bn_intern.c: * Determine the modified width-(w+1) Non-Adjacent Form (wNAF) of 'scalar'.
bn_intern.c: * where at most one of any  w+1  consecutive digits is non-zero
bn_intern.c: * w-1 zeros away from that next non-zero digit.
bn_intern.c:    mask = next_bit - 1;        /* at most 255 */
bn_intern.c:        sign = -1;
bn_intern.c:    if (scalar->d == NULL || scalar->top == 0) {
bn_intern.c:    window_val = scalar->d[0] & mask;
bn_intern.c:                digit = window_val - next_bit; /* -2^w < digit < 0 */
bn_intern.c:            if (digit <= -bit || digit >= bit || !(digit & 1)) {
bn_intern.c:            window_val -= digit;
bn_intern.c:    return a->top;
bn_intern.c:    return a->dmax;
bn_intern.c:    for (i = a->top; i < a->dmax; i++)
bn_intern.c:        a->d[i] = 0;
bn_intern.c:    if (in->top > size)
bn_intern.c:    if (in->d != NULL)
bn_intern.c:        memcpy(out, in->d, sizeof(*out) * in->top);
bn_intern.c:    return a->d;
bn_intern.c:     * flag, which effectively means "read-only data".
bn_intern.c:    a->d = (BN_ULONG *)words;
bn_intern.c:    a->dmax = a->top = size;
bn_intern.c:    a->neg = 0;
bn_intern.c:    a->flags |= BN_FLG_STATIC_DATA;
bn_intern.c:    memcpy(a->d, words, sizeof(BN_ULONG) * num_words);
bn_intern.c:    a->top = num_words;
bn_kron.c: * Copyright 2000-2016 The OpenSSL Project Authors. All Rights Reserved.
bn_kron.c:#define BN_lsw(n) (((n)->top == 0) ? (BN_ULONG) 0 : (n)->d[0])
bn_kron.c:/* Returns -2 for errors because both -1 and 0 are valid results. */
bn_kron.c:    int ret = -2;               /* avoid 'uninitialized' warning */
bn_kron.c:    /*-
bn_kron.c:     * In 'tab', only odd-indexed entries are relevant:
bn_kron.c:     * is $(-1)^{(n^2-1)/8}$ (using TeX notation).
bn_kron.c:    static const int tab[8] = { 0, 1, 0, -1, 0, -1, 0, 1 };
bn_kron.c:    /* now  B  is non-zero */
bn_kron.c:        /* set 'ret' to $(-1)^{(A^2-1)/8}$ */
bn_kron.c:    if (B->neg) {
bn_kron.c:        B->neg = 0;
bn_kron.c:        if (A->neg)
bn_kron.c:            ret = -ret;
bn_kron.c:        /* now  A  is non-zero */
bn_kron.c:            /* multiply 'ret' by  $(-1)^{(B^2-1)/8}$ */
bn_kron.c:        /* multiply 'ret' by  $(-1)^{(A-1)(B-1)/4}$ */
bn_kron.c:        if ((A->neg ? ~BN_lsw(A) : BN_lsw(A)) & BN_lsw(B) & 2)
bn_kron.c:            ret = -ret;
bn_kron.c:        tmp->neg = 0;
bn_kron.c:        return -2;
bn_lib.c: * Copyright 1995-2022 The OpenSSL Project Authors. All Rights Reserved.
bn_lib.c:/*-
bn_lib.c: * 2 -   4 ==  128
bn_lib.c: * 3 -   8 ==  256
bn_lib.c: * 4 -  16 ==  512
bn_lib.c: * 5 -  32 == 1024
bn_lib.c: * 6 -  64 == 2048
bn_lib.c: * 7 - 128 == 4096
bn_lib.c: * 8 - 256 == 8192
bn_lib.c:        if (mult > (int)(sizeof(int) * 8) - 1)
bn_lib.c:            mult = sizeof(int) * 8 - 1;
bn_lib.c:        if (high > (int)(sizeof(int) * 8) - 1)
bn_lib.c:            high = sizeof(int) * 8 - 1;
bn_lib.c:        if (low > (int)(sizeof(int) * 8) - 1)
bn_lib.c:            low = sizeof(int) * 8 - 1;
bn_lib.c:        if (mont > (int)(sizeof(int) * 8) - 1)
bn_lib.c:            mont = sizeof(int) * 8 - 1;
bn_lib.c: * https://mta.openssl.org/pipermail/openssl-users/2018-August/008465.html
bn_lib.c:    mask = (0 - x) & BN_MASK2;
bn_lib.c:    mask = (0 - (mask >> (BN_BITS2 - 1)));
bn_lib.c:    mask = (0 - x) & BN_MASK2;
bn_lib.c:    mask = (0 - (mask >> (BN_BITS2 - 1)));
bn_lib.c:    mask = (0 - x) & BN_MASK2;
bn_lib.c:    mask = (0 - (mask >> (BN_BITS2 - 1)));
bn_lib.c:    mask = (0 - x) & BN_MASK2;
bn_lib.c:    mask = (0 - (mask >> (BN_BITS2 - 1)));
bn_lib.c:    mask = (0 - x) & BN_MASK2;
bn_lib.c:    mask = (0 - (mask >> (BN_BITS2 - 1)));
bn_lib.c:    mask = (0 - x) & BN_MASK2;
bn_lib.c:    mask = (0 - (mask >> (BN_BITS2 - 1)));
bn_lib.c: * This function still leaks `a->dmax`: it's caller's responsibility to
bn_lib.c:    int i = a->top - 1;
bn_lib.c:    for (j = 0, past_i = 0, ret = 0; j < a->dmax; j++) {
bn_lib.c:        ret += BN_num_bits_word(a->d[j]) & mask;
bn_lib.c:     * if BN_is_zero(a) => i is -1 and ret contains garbage, so we mask the
bn_lib.c:    mask = ~(constant_time_eq_int(i, ((int)-1)));
bn_lib.c:    int i = a->top - 1;
bn_lib.c:    if (a->flags & BN_FLG_CONSTTIME) {
bn_lib.c:         * so that a->dmax is not leaking secret information.
bn_lib.c:    return ((i * BN_BITS2) + BN_num_bits_word(a->d[i]));
bn_lib.c:        OPENSSL_secure_clear_free(a->d, a->dmax * sizeof(a->d[0]));
bn_lib.c:        OPENSSL_clear_free(a->d, a->dmax * sizeof(a->d[0]));
bn_lib.c:        OPENSSL_free(a->d);
bn_lib.c:    if (a->d != NULL && !BN_get_flags(a, BN_FLG_STATIC_DATA))
bn_lib.c:    if (a->flags & BN_FLG_MALLOCED)
bn_lib.c:    ret->flags = BN_FLG_MALLOCED;
bn_lib.c:         ret->flags |= BN_FLG_SECURE;
bn_lib.c:/* The caller MUST check that words > b->dmax before calling this */
bn_lib.c:    assert(b->top <= words);
bn_lib.c:    if (b->top > 0)
bn_lib.c:        memcpy(a, b->d, sizeof(*a) * b->top);
bn_lib.c: * any unused part of b->d with leading zeros. It is mostly used by the
bn_lib.c:    if (words > b->dmax) {
bn_lib.c:        if (b->d != NULL)
bn_lib.c:        b->d = a;
bn_lib.c:        b->dmax = words;
bn_lib.c:    bn_words = BN_get_flags(b, BN_FLG_CONSTTIME) ? b->dmax : b->top;
bn_lib.c:    if (b->top > 0)
bn_lib.c:        memcpy(a->d, b->d, sizeof(b->d[0]) * bn_words);
bn_lib.c:    a->neg = b->neg;
bn_lib.c:    a->top = b->top;
bn_lib.c:    a->flags |= b->flags & BN_FLG_FIXED_TOP;
bn_lib.c:    flags_old_a = a->flags;
bn_lib.c:    flags_old_b = b->flags;
bn_lib.c:    tmp_d = a->d;
bn_lib.c:    tmp_top = a->top;
bn_lib.c:    tmp_dmax = a->dmax;
bn_lib.c:    tmp_neg = a->neg;
bn_lib.c:    a->d = b->d;
bn_lib.c:    a->top = b->top;
bn_lib.c:    a->dmax = b->dmax;
bn_lib.c:    a->neg = b->neg;
bn_lib.c:    b->d = tmp_d;
bn_lib.c:    b->top = tmp_top;
bn_lib.c:    b->dmax = tmp_dmax;
bn_lib.c:    b->neg = tmp_neg;
bn_lib.c:    a->flags = FLAGS_STRUCT(flags_old_a) | FLAGS_DATA(flags_old_b);
bn_lib.c:    b->flags = FLAGS_STRUCT(flags_old_b) | FLAGS_DATA(flags_old_a);
bn_lib.c:    if (a->d != NULL)
bn_lib.c:        OPENSSL_cleanse(a->d, sizeof(*a->d) * a->dmax);
bn_lib.c:    a->neg = 0;
bn_lib.c:    a->top = 0;
bn_lib.c:    a->flags &= ~BN_FLG_FIXED_TOP;
bn_lib.c:    if (a->top > 1)
bn_lib.c:    else if (a->top == 1)
bn_lib.c:        return a->d[0];
bn_lib.c:    /* a->top == 0 */
bn_lib.c:    a->neg = 0;
bn_lib.c:    a->d[0] = w;
bn_lib.c:    a->top = (w ? 1 : 0);
bn_lib.c:    a->flags &= ~BN_FLG_FIXED_TOP;
bn_lib.c:        s2 = s + len - 1;
bn_lib.c:        inc2 = -1;
bn_lib.c:        inc = -1;
bn_lib.c:        s += len - 1;
bn_lib.c:    for ( ; len > 0 && *s2 == xor; s2 += inc2, len--)
bn_lib.c:        ret->top = 0;
bn_lib.c:    n = ((len - 1) / BN_BYTES) + 1; /* Number of resulting bignum chunks */
bn_lib.c:    ret->top = n;
bn_lib.c:    ret->neg = neg;
bn_lib.c:    for (i = 0; n-- > 0; i++) {
bn_lib.c:        for (; len > 0 && m < BN_BYTES * 8; len--, s += inc, m += 8) {
bn_lib.c:        ret->d[i] = l;
bn_lib.c:     * bit set (-ve number)
bn_lib.c:     * In case |a| is fixed-top, BN_num_bits can return bogus length,
bn_lib.c:     * but it's assumed that fixed-top inputs ought to be "nominated"
bn_lib.c:        xor = a->neg ? 0xff : 0x00;
bn_lib.c:        carry = a->neg;
bn_lib.c:            ? !a->neg            /* MSbit set on nonnegative bignum */
bn_lib.c:            : a->neg;            /* MSbit unset on negative bignum */
bn_lib.c:    if (tolen == -1) {
bn_lib.c:            return -1;
bn_lib.c:    atop = a->dmax * BN_BYTES;
bn_lib.c:        inc = -1;
bn_lib.c:        to += tolen - 1;         /* Move to the last byte, not beyond */
bn_lib.c:    lasti = atop - 1;
bn_lib.c:    atop = a->top * BN_BYTES;
bn_lib.c:        l = a->d[i / BN_BYTES];
bn_lib.c:        mask = 0 - ((j - atop) >> (8 * sizeof(i) - 1));
bn_lib.c:        i += (i - lasti) >> (8 * sizeof(i) - 1); /* stay on last limb */
bn_lib.c:        return -1;
bn_lib.c:        return -1;
bn_lib.c:    return bn2binpad(a, to, -1, BIG, UNSIGNED);
bn_lib.c:        return -1;
bn_lib.c:        return -1;
bn_lib.c:    i = a->top - b->top;
bn_lib.c:    ap = a->d;
bn_lib.c:    bp = b->d;
bn_lib.c:    for (i = a->top - 1; i >= 0; i--) {
bn_lib.c:            return ((t1 > t2) ? 1 : -1);
bn_lib.c:            return -1;
bn_lib.c:    if (a->neg != b->neg) {
bn_lib.c:        if (a->neg)
bn_lib.c:            return -1;
bn_lib.c:    if (a->neg == 0) {
bn_lib.c:        lt = -1;
bn_lib.c:        gt = -1;
bn_lib.c:    if (a->top > b->top)
bn_lib.c:    if (a->top < b->top)
bn_lib.c:    for (i = a->top - 1; i >= 0; i--) {
bn_lib.c:        t1 = a->d[i];
bn_lib.c:        t2 = b->d[i];
bn_lib.c:    if (a->top <= i) {
bn_lib.c:        for (k = a->top; k < i + 1; k++)
bn_lib.c:            a->d[k] = 0;
bn_lib.c:        a->top = i + 1;
bn_lib.c:        a->flags &= ~BN_FLG_FIXED_TOP;
bn_lib.c:    a->d[i] |= (((BN_ULONG)1) << j);
bn_lib.c:    if (a->top <= i)
bn_lib.c:    a->d[i] &= (~(((BN_ULONG)1) << j));
bn_lib.c:    if (a->top <= i)
bn_lib.c:    return (int)(((a->d[i]) >> j) & ((BN_ULONG)1));
bn_lib.c:    if (w >= a->top)
bn_lib.c:        a->top = w;
bn_lib.c:        a->top = w + 1;
bn_lib.c:        a->d[w] &= ~(BN_MASK2 << b);
bn_lib.c:        a->neg = 1;
bn_lib.c:        a->neg = 0;
bn_lib.c:    aa = a[n - 1];
bn_lib.c:    bb = b[n - 1];
bn_lib.c:        return ((aa > bb) ? 1 : -1);
bn_lib.c:    for (i = n - 2; i >= 0; i--) {
bn_lib.c:            return ((aa > bb) ? 1 : -1);
bn_lib.c: * two lengths, calculated as len(a)-len(b). All lengths are the number of
bn_lib.c:    n = cl - 1;
bn_lib.c:            if (b[n - i] != 0)
bn_lib.c:                return -1;      /* a < b */
bn_lib.c:        for (i = dl; i > 0; i--) {
bn_lib.c:/*-
bn_lib.c: * Constant-time conditional swap of a and b.
bn_lib.c:    condition = ((~condition & ((condition - 1))) >> (BN_BITS2 - 1)) - 1;
bn_lib.c:    t = (a->top ^ b->top) & condition;
bn_lib.c:    a->top ^= t;
bn_lib.c:    b->top ^= t;
bn_lib.c:    t = (a->neg ^ b->neg) & condition;
bn_lib.c:    a->neg ^= t;
bn_lib.c:    b->neg ^= t;
bn_lib.c:    /*-
bn_lib.c:     * is actually to treat it as it's read-only data, and some (if not most)
bn_lib.c:     * of it does reside in read-only segment. In other words observation of
bn_lib.c:     * BN_FLG_SECURE: must be preserved, because it determines how x->d was
bn_lib.c:    t = ((a->flags ^ b->flags) & BN_CONSTTIME_SWAP_FLAGS) & condition;
bn_lib.c:    a->flags ^= t;
bn_lib.c:    b->flags ^= t;
bn_lib.c:        t = (a->d[i] ^ b->d[i]) & condition;
bn_lib.c:        a->d[i] ^= t;
bn_lib.c:        b->d[i] ^= t;
bn_lib.c:/* Bits of security, see SP800-57 */
bn_lib.c:    if (N == -1)
bn_lib.c:    a->neg = 0;
bn_lib.c:    a->top = 0;
bn_lib.c:    a->flags &= ~BN_FLG_FIXED_TOP;
bn_lib.c:    return ((a->top == 1) && (a->d[0] == w)) || ((w == 0) && (a->top == 0));
bn_lib.c:    return a->top == 0;
bn_lib.c:    return BN_abs_is_word(a, 1) && !a->neg;
bn_lib.c:    return BN_abs_is_word(a, w) && (!w || !a->neg);
bn_lib.c:    return (a->top > 0) && (a->d[0] & 1);
bn_lib.c:    return (a->neg != 0);
bn_lib.c:    return BN_mod_mul_montgomery(r, a, &(mont->RR), mont, ctx);
bn_lib.c:    dest->d = b->d;
bn_lib.c:    dest->top = b->top;
bn_lib.c:    dest->dmax = b->dmax;
bn_lib.c:    dest->neg = b->neg;
bn_lib.c:    dest->flags = ((dest->flags & BN_FLG_MALLOCED)
bn_lib.c:                   | (b->flags & ~BN_FLG_MALLOCED)
bn_lib.c:    b->flags |= n;
bn_lib.c:    return b->flags & n;
bn_lib.c:/* Populate a BN_GENCB structure with an "old"-style callback */
bn_lib.c:    tmp_gencb->ver = 1;
bn_lib.c:    tmp_gencb->arg = cb_arg;
bn_lib.c:    tmp_gencb->cb.cb_1 = callback;
bn_lib.c:/* Populate a BN_GENCB structure with a "new"-style callback */
bn_lib.c:    tmp_gencb->ver = 2;
bn_lib.c:    tmp_gencb->arg = cb_arg;
bn_lib.c:    tmp_gencb->cb.cb_2 = callback;
bn_lib.c:    return cb->arg;
bn_lib.c:    return (words <= a->dmax) ? a : bn_expand2(a, words);
bn_lib.c:    int tmp_top = a->top;
bn_lib.c:        for (ftl = &(a->d[tmp_top]); tmp_top > 0; tmp_top--) {
bn_lib.c:            ftl--;
bn_lib.c:        a->top = tmp_top;
bn_lib.c:    if (a->top == 0)
bn_lib.c:        a->neg = 0;
bn_lib.c:    a->flags &= ~BN_FLG_FIXED_TOP;
bn_local.h: * Copyright 1995-2022 The OpenSSL Project Authors. All Rights Reserved.
bn_local.h: * SIXTY_FOUR_BIT in its own environment since it doesn't re-run our
bn_local.h: * Configure script and needs to support both 32-bit and 64-bit.
bn_local.h: * itself. BN_DEBUG - turn on various debugging alterations to the bignum
bn_local.h: * code BN_RAND_DEBUG - uses random poisoning of unused words to trip up
bn_local.h: * 64-bit processor with LP64 ABI
bn_local.h: * 64-bit processor other than LP64 ABI
bn_local.h:/*-
bn_local.h: * bn_fix_top() has become an overabused duct-tape because bignum data is
bn_local.h: * - bn_fix_top()s implementation has been moved to bn_correct_top()
bn_local.h: * - if BN_DEBUG isn't defined, bn_fix_top() maps to bn_correct_top(), and
bn_local.h: * - if BN_DEBUG *is* defined;
bn_local.h: *   - bn_check_top() tries to pollute unused words even if the bignum 'top' is
bn_local.h: *   - bn_fix_top() maps to bn_check_top() rather than "fixing" anything.
bn_local.h: * It's BN_DEBUG-only flag, because user application is not supposed to
bn_local.h: * all operations manipulating the bit in question in non-BN_DEBUG build.
bn_local.h:            if (_bnum1->top < _bnum1->dmax) { \
bn_local.h:                memcpy(&_not_const, &_bnum1->d, sizeof(_not_const)); \
bn_local.h:                (void)RAND_bytes(&_tmp_char, 1); /* Debug only - safe to ignore error return */\
bn_local.h:                memset(_not_const + _bnum1->top, _tmp_char, \
bn_local.h:                       sizeof(*_not_const) * (_bnum1->dmax - _bnum1->top)); \
bn_local.h:                        int _top = _bnum2->top; \
bn_local.h:                        (void)ossl_assert((_top == 0 && !_bnum2->neg) || \
bn_local.h:                                  (_top && ((_bnum2->flags & BN_FLG_FIXED_TOP) \
bn_local.h:                                            || _bnum2->d[_top - 1] != 0))); \
bn_local.h:#  define bn_check_size(bn, bits) bn_wcheck_size(bn, ((bits+BN_BITS2-1))/BN_BITS2)
bn_local.h:                assert((words) <= (_bnum2)->dmax && \
bn_local.h:                       (words) >= (_bnum2)->top); \
bn_local.h:                                   possibly zero-padded */
bn_local.h:    BIGNUM Ni;                  /* R*(1/R mod N) - N*Ni = 1 (Ni is only
bn_local.h:    void *arg;                  /* callback-specific data */
bn_local.h:        /* if (ver==1) - handles old style callbacks */
bn_local.h:        /* if (ver==2) - new callback style */
bn_local.h:/*-
bn_local.h: * BN_window_bits_for_exponent_size -- macro for sliding window mod_exp functions
bn_local.h: *    2^(w-1) + (b-w)/(w+1);
bn_local.h: * here  2^(w-1)  is for precomputing the table (we actually need
bn_local.h: * (b-w)/(w+1)  is an approximation for the expected number of
bn_local.h: * w-bit windows, not counting the first one.
bn_local.h:# define MOD_EXP_CTIME_MIN_CACHE_LINE_MASK       (MOD_EXP_CTIME_MIN_CACHE_LINE_WIDTH - 1)
bn_local.h: * 2011-02-22 SMS. In various places, a size_t variable or a type cast to
bn_local.h: * size_t was used to perform integer-only operations on pointers.  This
bn_local.h: * failed on VMS with 64-bit pointers (CC /POINTER_SIZE = 64) because size_t
bn_local.h: * only fix here is VMS-specific.
bn_local.h: * PowerPC, Alpha, and IA-64) provide *separate* instruction calculating
bn_local.h: * what BN_UMULT_HIGH macro is about:-) Note that more recent compilers do
bn_local.h:        int      ind = (a)->dmax - (a)->top; \
bn_local.h:        BN_ULONG *ftl = &(a)->d[(a)->top-1]; \
bn_local.h:        for (; ind != 0; ind--) \
bn_local.h:        h+=(m&BN_MASK2h1)>>(BN_BITS4-1); \
bn_local.h:        /* non-multiply part */ \
bn_local.h:        /* non-multiply part */ \
bn_local.h:    if (bits > (INT_MAX - BN_BITS2 + 1))
bn_local.h:    if (((bits+BN_BITS2-1)/BN_BITS2) <= (a)->dmax)
bn_local.h:    return bn_expand2((a),(bits+BN_BITS2-1)/BN_BITS2);
bn_mod.c: * Copyright 1998-2021 The OpenSSL Project Authors. All Rights Reserved.
bn_mod.c:     * like BN_mod, but returns non-negative remainder (i.e., 0 <= r < |d|
bn_mod.c:    if (!r->neg)
bn_mod.c:    /* now   -|d| < r < 0,  so we have to set  r := r + |d| */
bn_mod.c:    return (d->neg ? BN_sub : BN_add) (r, r, d);
bn_mod.c: * BN_mod_add variant that may be used if both a and b are non-negative and
bn_mod.c:    size_t i, ai, bi, mtop = m->top;
bn_mod.c:    ap = a->d != NULL ? a->d : tp;
bn_mod.c:    bp = b->d != NULL ? b->d : tp;
bn_mod.c:        mask = (BN_ULONG)0 - ((i - a->top) >> (8 * sizeof(i) - 1));
bn_mod.c:        mask = (BN_ULONG)0 - ((i - b->top) >> (8 * sizeof(i) - 1));
bn_mod.c:        ai += (i - a->dmax) >> (8 * sizeof(i) - 1);
bn_mod.c:        bi += (i - b->dmax) >> (8 * sizeof(i) - 1);
bn_mod.c:    rp = r->d;
bn_mod.c:    carry -= bn_sub_words(rp, tp, m->d, mtop);
bn_mod.c:    r->top = mtop;
bn_mod.c:    r->flags |= BN_FLG_FIXED_TOP;
bn_mod.c:    r->neg = 0;
bn_mod.c: * BN_mod_sub variant that may be used if both a and b are non-negative,
bn_mod.c: * -2*m < r = a - b < m
bn_mod.c:    size_t i, ai, bi, mtop = m->top;
bn_mod.c:    rp = r->d;
bn_mod.c:    ap = a->d != NULL ? a->d : rp;
bn_mod.c:    bp = b->d != NULL ? b->d : rp;
bn_mod.c:        mask = (BN_ULONG)0 - ((i - a->top) >> (8 * sizeof(i) - 1));
bn_mod.c:        mask = (BN_ULONG)0 - ((i - b->top) >> (8 * sizeof(i) - 1));
bn_mod.c:        rp[i] = ta - tb - borrow;
bn_mod.c:        ai += (i - a->dmax) >> (8 * sizeof(i) - 1);
bn_mod.c:        bi += (i - b->dmax) >> (8 * sizeof(i) - 1);
bn_mod.c:    ap = m->d;
bn_mod.c:    for (i = 0, mask = 0 - borrow, carry = 0; i < mtop; i++) {
bn_mod.c:    borrow -= carry;
bn_mod.c:    for (i = 0, mask = 0 - borrow, carry = 0; i < mtop; i++) {
bn_mod.c:    r->top = mtop;
bn_mod.c:    r->flags |= BN_FLG_FIXED_TOP;
bn_mod.c:    r->neg = 0;
bn_mod.c: * BN_mod_sub variant that may be used if both a and b are non-negative and
bn_mod.c:    if (r->neg)
bn_mod.c:    /* r->neg == 0,  thus we don't need BN_nnmod */
bn_mod.c: * BN_mod_lshift1 variant that may be used if a is non-negative and less than
bn_mod.c:    if (m->neg) {
bn_mod.c:        abs_m->neg = 0;
bn_mod.c: * BN_mod_lshift variant that may be used if a is non-negative and less than
bn_mod.c:        max_shift = BN_num_bits(m) - BN_num_bits(r);
bn_mod.c:            n -= max_shift;
bn_mod.c:            --n;
bn_mont.c: * Copyright 1995-2021 The OpenSSL Project Authors. All Rights Reserved.
bn_mont.c:#define MONT_WORD               /* use the faster word-based algorithm */
bn_mont.c:    int num = mont->N.top;
bn_mont.c:    if (num > 1 && a->top == num && b->top == num) {
bn_mont.c:        if (bn_mul_mont(r->d, a->d, b->d, mont->N.d, mont->n0, num)) {
bn_mont.c:            r->neg = a->neg ^ b->neg;
bn_mont.c:            r->top = num;
bn_mont.c:            r->flags |= BN_FLG_FIXED_TOP;
bn_mont.c:    if ((a->top + b->top) > 2 * num)
bn_mont.c:    n = &(mont->N);
bn_mont.c:    nl = n->top;
bn_mont.c:        ret->top = 0;
bn_mont.c:    r->neg ^= n->neg;
bn_mont.c:    np = n->d;
bn_mont.c:    rp = r->d;
bn_mont.c:    for (rtop = r->top, i = 0; i < max; i++) {
bn_mont.c:        v = (BN_ULONG)0 - ((i - rtop) >> (8 * sizeof(rtop) - 1));
bn_mont.c:    r->top = max;
bn_mont.c:    r->flags |= BN_FLG_FIXED_TOP;
bn_mont.c:    n0 = mont->n0[0];
bn_mont.c:    ret->top = nl;
bn_mont.c:    ret->flags |= BN_FLG_FIXED_TOP;
bn_mont.c:    ret->neg = r->neg;
bn_mont.c:    rp = ret->d;
bn_mont.c:    ap = &(r->d[nl]);
bn_mont.c:    carry -= bn_sub_words(rp, ap, np, nl);
bn_mont.c:     * |carry| is -1 if |ap| - |np| underflowed or zero if it did not. Note
bn_mont.c:    BN_mask_bits(t1, mont->ri);
bn_mont.c:    if (!BN_mul(t2, t1, &mont->Ni, ctx))
bn_mont.c:    BN_mask_bits(t2, mont->ri);
bn_mont.c:    if (!BN_mul(t1, t2, &mont->N, ctx))
bn_mont.c:    if (!BN_rshift(ret, t2, mont->ri))
bn_mont.c:    if (BN_ucmp(ret, &(mont->N)) >= 0) {
bn_mont.c:        if (!BN_usub(ret, ret, &(mont->N)))
bn_mont.c:    return bn_mul_mont_fixed_top(r, a, &(mont->RR), mont, ctx);
bn_mont.c:    ret->flags = BN_FLG_MALLOCED;
bn_mont.c:    ctx->ri = 0;
bn_mont.c:    bn_init(&ctx->RR);
bn_mont.c:    bn_init(&ctx->N);
bn_mont.c:    bn_init(&ctx->Ni);
bn_mont.c:    ctx->n0[0] = ctx->n0[1] = 0;
bn_mont.c:    ctx->flags = 0;
bn_mont.c:    BN_clear_free(&mont->RR);
bn_mont.c:    BN_clear_free(&mont->N);
bn_mont.c:    BN_clear_free(&mont->Ni);
bn_mont.c:    if (mont->flags & BN_FLG_MALLOCED)
bn_mont.c:    R = &(mont->RR);            /* grab RR as a temp */
bn_mont.c:    if (!BN_copy(&(mont->N), mod))
bn_mont.c:        BN_set_flags(&(mont->N), BN_FLG_CONSTTIME);
bn_mont.c:    mont->N.neg = 0;
bn_mont.c:        mont->ri = (BN_num_bits(mod) + (BN_BITS2 - 1)) / BN_BITS2 * BN_BITS2;
bn_mont.c:        if ((buf[0] = mod->d[0]))
bn_mont.c:        if ((buf[1] = mod->top > 1 ? mod->d[1] : 0))
bn_mont.c:            /* Ri-- (mod double word size) */
bn_mont.c:            Ri->neg = 0;
bn_mont.c:            Ri->d[0] = BN_MASK2;
bn_mont.c:            Ri->d[1] = BN_MASK2;
bn_mont.c:            Ri->top = 2;
bn_mont.c:         * Ni = (R*Ri-1)/N, keep only couple of least significant words:
bn_mont.c:        mont->n0[0] = (Ri->top > 0) ? Ri->d[0] : 0;
bn_mont.c:        mont->n0[1] = (Ri->top > 1) ? Ri->d[1] : 0;
bn_mont.c:        buf[0] = mod->d[0];     /* tmod = N mod word size */
bn_mont.c:        /* Ri = R^-1 mod N */
bn_mont.c:                goto err;       /* Ri-- (mod word size) */
bn_mont.c:         * Ni = (R*Ri-1)/N, keep only least significant word:
bn_mont.c:        mont->n0[0] = (Ri->top > 0) ? Ri->d[0] : 0;
bn_mont.c:        mont->n0[1] = 0;
bn_mont.c:        mont->ri = BN_num_bits(&mont->N);
bn_mont.c:        if (!BN_set_bit(R, mont->ri))
bn_mont.c:        /* Ri = R^-1 mod N */
bn_mont.c:        if ((BN_mod_inverse(Ri, R, &mont->N, ctx)) == NULL)
bn_mont.c:        if (!BN_lshift(Ri, Ri, mont->ri))
bn_mont.c:         * Ni = (R*Ri-1) / N
bn_mont.c:        if (!BN_div(&(mont->Ni), NULL, Ri, &mont->N, ctx))
bn_mont.c:    BN_zero(&(mont->RR));
bn_mont.c:    if (!BN_set_bit(&(mont->RR), mont->ri * 2))
bn_mont.c:    if (!BN_mod(&(mont->RR), &(mont->RR), &(mont->N), ctx))
bn_mont.c:    for (i = mont->RR.top, ret = mont->N.top; i < ret; i++)
bn_mont.c:        mont->RR.d[i] = 0;
bn_mont.c:    mont->RR.top = ret;
bn_mont.c:    mont->RR.flags |= BN_FLG_FIXED_TOP;
bn_mont.c:    if (!BN_copy(&(to->RR), &(from->RR)))
bn_mont.c:    if (!BN_copy(&(to->N), &(from->N)))
bn_mont.c:    if (!BN_copy(&(to->Ni), &(from->Ni)))
bn_mont.c:    to->ri = from->ri;
bn_mont.c:    to->n0[0] = from->n0[0];
bn_mont.c:    to->n0[1] = from->n0[1];
bn_mont.c:     * We don't want to serialize globally while doing our lazy-init math in
bn_mont.c:     * lazy-init the same 'pmont', by having each do the lazy-init math work
bn_mont.c:    /* The locked compare-and-set, after the local work is done. */
bn_mpi.c: * Copyright 1995-2020 The OpenSSL Project Authors. All Rights Reserved.
bn_mpi.c:    if (a->neg)
bn_mpi.c:        a->neg = 0;
bn_mpi.c:        a->top = 0;
bn_mpi.c:    a->neg = neg;
bn_mpi.c:        BN_clear_bit(a, BN_num_bits(a) - 1);
bn_mul.c: * Copyright 1995-2018 The OpenSSL Project Authors. All Rights Reserved.
bn_mul.c: * between the two lengths, calculated as len(a)-len(b). All lengths are the
bn_mul.c:            r[0] = (0 - t - c) & BN_MASK2;
bn_mul.c:            r[1] = (0 - t - c) & BN_MASK2;
bn_mul.c:            r[2] = (0 - t - c) & BN_MASK2;
bn_mul.c:            r[3] = (0 - t - c) & BN_MASK2;
bn_mul.c:            r[0] = (t - c) & BN_MASK2;
bn_mul.c:            if (--dl <= 0)
bn_mul.c:            r[1] = (t - c) & BN_MASK2;
bn_mul.c:            if (--dl <= 0)
bn_mul.c:            r[2] = (t - c) & BN_MASK2;
bn_mul.c:            if (--dl <= 0)
bn_mul.c:            r[3] = (t - c) & BN_MASK2;
bn_mul.c:            if (--dl <= 0)
bn_mul.c:                switch (save_dl - dl) {
bn_mul.c:                    if (--dl <= 0)
bn_mul.c:                    if (--dl <= 0)
bn_mul.c:                    if (--dl <= 0)
bn_mul.c:                if (--dl <= 0)
bn_mul.c:                if (--dl <= 0)
bn_mul.c:                if (--dl <= 0)
bn_mul.c:                if (--dl <= 0)
bn_mul.c:/*-
bn_mul.c: * a[0]*b[0]+a[1]*b[1]+(a[0]-a[1])*(b[1]-b[0])
bn_mul.c:                   sizeof(BN_ULONG) * -(dna + dnb));
bn_mul.c:    /* r=(a[0]-a[1])*(b[1]-b[0]) */
bn_mul.c:    c1 = bn_cmp_part_words(a, &(a[n]), tna, n - tna);
bn_mul.c:    c2 = bn_cmp_part_words(&(b[n]), b, tnb, tnb - n);
bn_mul.c:    case -4:
bn_mul.c:        bn_sub_part_words(t, &(a[n]), a, tna, tna - n); /* - */
bn_mul.c:        bn_sub_part_words(&(t[n]), b, &(b[n]), tnb, n - tnb); /* - */
bn_mul.c:    case -3:
bn_mul.c:    case -2:
bn_mul.c:        bn_sub_part_words(t, &(a[n]), a, tna, tna - n); /* - */
bn_mul.c:        bn_sub_part_words(&(t[n]), &(b[n]), b, tnb, tnb - n); /* + */
bn_mul.c:    case -1:
bn_mul.c:        bn_sub_part_words(t, a, &(a[n]), tna, n - tna); /* + */
bn_mul.c:        bn_sub_part_words(&(t[n]), b, &(b[n]), tnb, n - tnb); /* - */
bn_mul.c:        bn_sub_part_words(t, a, &(a[n]), tna, n - tna);
bn_mul.c:        bn_sub_part_words(&(t[n]), &(b[n]), b, tnb, tnb - n);
bn_mul.c:    /*-
bn_mul.c:     * t[32] holds (a[0]-a[1])*(b[1]-b[0]), c1 is the sign
bn_mul.c:        c1 -= (int)(bn_sub_words(&(t[n2]), t, &(t[n2]), n2));
bn_mul.c:    /*-
bn_mul.c:     * t[32] holds (a[0]-a[1])*(b[1]-b[0])+(a[0]*b[0])+(a[1]*b[1])
bn_mul.c:    /* r=(a[0]-a[1])*(b[1]-b[0]) */
bn_mul.c:    c1 = bn_cmp_part_words(a, &(a[n]), tna, n - tna);
bn_mul.c:    c2 = bn_cmp_part_words(&(b[n]), b, tnb, tnb - n);
bn_mul.c:    case -4:
bn_mul.c:        bn_sub_part_words(t, &(a[n]), a, tna, tna - n); /* - */
bn_mul.c:        bn_sub_part_words(&(t[n]), b, &(b[n]), tnb, n - tnb); /* - */
bn_mul.c:    case -3:
bn_mul.c:    case -2:
bn_mul.c:        bn_sub_part_words(t, &(a[n]), a, tna, tna - n); /* - */
bn_mul.c:        bn_sub_part_words(&(t[n]), &(b[n]), b, tnb, tnb - n); /* + */
bn_mul.c:    case -1:
bn_mul.c:        bn_sub_part_words(t, a, &(a[n]), tna, n - tna); /* + */
bn_mul.c:        bn_sub_part_words(&(t[n]), b, &(b[n]), tnb, n - tnb); /* - */
bn_mul.c:        bn_sub_part_words(t, a, &(a[n]), tna, n - tna);
bn_mul.c:        bn_sub_part_words(&(t[n]), &(b[n]), b, tnb, tnb - n);
bn_mul.c:        memset(&r[n2 + tn * 2], 0, sizeof(*r) * (n2 - tn * 2));
bn_mul.c:        memset(&r[n2 + tna + tnb], 0, sizeof(*r) * (n2 - tna - tnb));
bn_mul.c:            j = tna - i;
bn_mul.c:            j = tnb - i;
bn_mul.c:                             i, tna - i, tnb - i, p);
bn_mul.c:            memset(&r[n2 + i * 2], 0, sizeof(*r) * (n2 - i * 2));
bn_mul.c:                                  i, tna - i, tnb - i, p);
bn_mul.c:                   sizeof(BN_ULONG) * (n2 - tna - tnb));
bn_mul.c:                                              i, tna - i, tnb - i, p);
bn_mul.c:                                         i, tna - i, tnb - i, p);
bn_mul.c:    /*-
bn_mul.c:     * t[32] holds (a[0]-a[1])*(b[1]-b[0]), c1 is the sign
bn_mul.c:        c1 -= (int)(bn_sub_words(&(t[n2]), t, &(t[n2]), n2));
bn_mul.c:    /*-
bn_mul.c:     * t[32] holds (a[0]-a[1])*(b[1]-b[0])+(a[0]*b[0])+(a[1]*b[1])
bn_mul.c:/*-
bn_mul.c:    al = a->top;
bn_mul.c:    bl = b->top;
bn_mul.c:    i = al - bl;
bn_mul.c:            rr->top = 8;
bn_mul.c:            bn_mul_comba4(rr->d, a->d, b->d);
bn_mul.c:            rr->top = 16;
bn_mul.c:            bn_mul_comba8(rr->d, a->d, b->d);
bn_mul.c:        if (i >= -1 && i <= 1) {
bn_mul.c:            if (i == -1) {
bn_mul.c:            j = 1 << (j - 1);
bn_mul.c:                bn_mul_part_recursive(rr->d, a->d, b->d,
bn_mul.c:                                      j, al - j, bl - j, t->d);
bn_mul.c:                bn_mul_recursive(rr->d, a->d, b->d, j, al - j, bl - j, t->d);
bn_mul.c:            rr->top = top;
bn_mul.c:    rr->top = top;
bn_mul.c:    bn_mul_normal(rr->d, a->d, al, b->d, bl);
bn_mul.c:    rr->neg = a->neg ^ b->neg;
bn_mul.c:    rr->flags |= BN_FLG_FIXED_TOP;
bn_mul.c:        if (--nb <= 0)
bn_mul.c:        if (--nb <= 0)
bn_mul.c:        if (--nb <= 0)
bn_mul.c:        if (--nb <= 0)
bn_mul.c:        if (--n <= 0)
bn_mul.c:        if (--n <= 0)
bn_mul.c:        if (--n <= 0)
bn_mul.c:        if (--n <= 0)
bn_nist.c: * Copyright 2002-2021 The OpenSSL Project Authors. All Rights Reserved.
bn_nist.c:#define BN_NIST_192_TOP (192+BN_BITS2-1)/BN_BITS2
bn_nist.c:#define BN_NIST_224_TOP (224+BN_BITS2-1)/BN_BITS2
bn_nist.c:#define BN_NIST_256_TOP (256+BN_BITS2-1)/BN_BITS2
bn_nist.c:#define BN_NIST_384_TOP (384+BN_BITS2-1)/BN_BITS2
bn_nist.c:#define BN_NIST_521_TOP (521+BN_BITS2-1)/BN_BITS2
bn_nist.c:/* pre-computed tables are "carry-less" values of modulus*(i+1) */
bn_nist.c:                                                    * "carry-full" */
bn_nist.c: * To avoid more recent compilers (specifically clang-14) from treating this
bn_nist.c: * diagnostics and llvm/llvm-project#55255 for the later discussions with the
bn_nist.c:        bn_cp_64(to, 0, from, (a3) - 3) \
bn_nist.c:        bn_cp_64(to, 1, from, (a2) - 3) \
bn_nist.c:        bn_cp_64(to, 2, from, (a1) - 3) \
bn_nist.c:    int top = a->top, i;
bn_nist.c:    register BN_ULONG *r_d, *a_d = a->d;
bn_nist.c:        r_d = r->d;
bn_nist.c:    nist_cp_bn_0(buf.bn, a_d + BN_NIST_192_TOP, top - BN_NIST_192_TOP,
bn_nist.c:        acc += bp[3 * 2 - 6];
bn_nist.c:        acc += bp[5 * 2 - 6];
bn_nist.c:        acc += bp[3 * 2 - 5];
bn_nist.c:        acc += bp[5 * 2 - 5];
bn_nist.c:        acc += bp[3 * 2 - 6];
bn_nist.c:        acc += bp[4 * 2 - 6];
bn_nist.c:        acc += bp[5 * 2 - 6];
bn_nist.c:        acc += bp[3 * 2 - 5];
bn_nist.c:        acc += bp[4 * 2 - 5];
bn_nist.c:        acc += bp[5 * 2 - 5];
bn_nist.c:        acc += bp[4 * 2 - 6];
bn_nist.c:        acc += bp[5 * 2 - 6];
bn_nist.c:        acc += bp[4 * 2 - 5];
bn_nist.c:        acc += bp[5 * 2 - 5];
bn_nist.c:            (int)bn_sub_words(r_d, r_d, _nist_p_192[carry - 1],
bn_nist.c:     * we need 'if (carry==0 || result>=modulus) result-=modulus;'
bn_nist.c:     * 'tmp=result-modulus; if (!carry || !borrow) result=tmp;'
bn_nist.c:     * this is what happens below, but without explicit if:-) a.
bn_nist.c:        0 - (PTR_SIZE_INT) bn_sub_words(c_d, r_d, _nist_p_192[0],
bn_nist.c:    mask &= 0 - (PTR_SIZE_INT) carry;
bn_nist.c:    r->top = BN_NIST_192_TOP;
bn_nist.c:        bn_cp_32(to, 0, from, (a7) - 7) \
bn_nist.c:        bn_cp_32(to, 1, from, (a6) - 7) \
bn_nist.c:        bn_cp_32(to, 2, from, (a5) - 7) \
bn_nist.c:        bn_cp_32(to, 3, from, (a4) - 7) \
bn_nist.c:        bn_cp_32(to, 4, from, (a3) - 7) \
bn_nist.c:        bn_cp_32(to, 5, from, (a2) - 7) \
bn_nist.c:        bn_cp_32(to, 6, from, (a1) - 7) \
bn_nist.c:    int top = a->top, i;
bn_nist.c:    BN_ULONG *r_d, *a_d = a->d;
bn_nist.c:        r_d = r->d;
bn_nist.c:    nist_cp_bn_0(c_d, a_d + (BN_NIST_224_TOP - 1),
bn_nist.c:                 top - (BN_NIST_224_TOP - 1), BN_NIST_224_TOP);
bn_nist.c:    r_d[BN_NIST_224_TOP - 1] &= BN_MASK2l;
bn_nist.c:    nist_cp_bn_0(buf.bn, a_d + BN_NIST_224_TOP, top - BN_NIST_224_TOP,
bn_nist.c:        acc -= bp[7 - 7];
bn_nist.c:        acc -= bp[11 - 7];
bn_nist.c:        acc -= bp[8 - 7];
bn_nist.c:        acc -= bp[12 - 7];
bn_nist.c:        acc -= bp[9 - 7];
bn_nist.c:        acc -= bp[13 - 7];
bn_nist.c:        acc += bp[7 - 7];
bn_nist.c:        acc += bp[11 - 7];
bn_nist.c:        acc -= bp[10 - 7];
bn_nist.c:        acc += bp[8 - 7];
bn_nist.c:        acc += bp[12 - 7];
bn_nist.c:        acc -= bp[11 - 7];
bn_nist.c:        acc += bp[9 - 7];
bn_nist.c:        acc += bp[13 - 7];
bn_nist.c:        acc -= bp[12 - 7];
bn_nist.c:        acc += bp[10 - 7];
bn_nist.c:        acc -= bp[13 - 7];
bn_nist.c:        carry -= (int)bn_sub_words(r_d, r_d, t_d, BN_NIST_224_TOP);
bn_nist.c:        carry -= (int)bn_sub_words(r_d, r_d, t_d, BN_NIST_224_TOP);
bn_nist.c:        carry = (int)(r_d[BN_NIST_224_TOP - 1] >> 32);
bn_nist.c:            (int)bn_sub_words(r_d, r_d, _nist_p_224[carry - 1],
bn_nist.c:        carry = (int)(~(r_d[BN_NIST_224_TOP - 1] >> 32)) & 1;
bn_nist.c:            (int)bn_add_words(r_d, r_d, _nist_p_224[-carry - 1],
bn_nist.c:        mask = 0 - (PTR_SIZE_INT) carry;
bn_nist.c:        0 - (PTR_SIZE_INT) (*u.f) (c_d, r_d, _nist_p_224[0], BN_NIST_224_TOP);
bn_nist.c:    mask &= 0 - (PTR_SIZE_INT) carry;
bn_nist.c:    r->top = BN_NIST_224_TOP;
bn_nist.c:        bn_cp_32(to, 0, from, (a8) - 8) \
bn_nist.c:        bn_cp_32(to, 1, from, (a7) - 8) \
bn_nist.c:        bn_cp_32(to, 2, from, (a6) - 8) \
bn_nist.c:        bn_cp_32(to, 3, from, (a5) - 8) \
bn_nist.c:        bn_cp_32(to, 4, from, (a4) - 8) \
bn_nist.c:        bn_cp_32(to, 5, from, (a3) - 8) \
bn_nist.c:        bn_cp_32(to, 6, from, (a2) - 8) \
bn_nist.c:        bn_cp_32(to, 7, from, (a1) - 8) \
bn_nist.c:    int i, top = a->top;
bn_nist.c:    register BN_ULONG *a_d = a->d, *r_d;
bn_nist.c:        r_d = r->d;
bn_nist.c:    nist_cp_bn_0(buf.bn, a_d + BN_NIST_256_TOP, top - BN_NIST_256_TOP,
bn_nist.c:        acc += bp[8 - 8];
bn_nist.c:        acc += bp[9 - 8];
bn_nist.c:        acc -= bp[11 - 8];
bn_nist.c:        acc -= bp[12 - 8];
bn_nist.c:        acc -= bp[13 - 8];
bn_nist.c:        acc -= bp[14 - 8];
bn_nist.c:        acc += bp[9 - 8];
bn_nist.c:        acc += bp[10 - 8];
bn_nist.c:        acc -= bp[12 - 8];
bn_nist.c:        acc -= bp[13 - 8];
bn_nist.c:        acc -= bp[14 - 8];
bn_nist.c:        acc -= bp[15 - 8];
bn_nist.c:        acc += bp[10 - 8];
bn_nist.c:        acc += bp[11 - 8];
bn_nist.c:        acc -= bp[13 - 8];
bn_nist.c:        acc -= bp[14 - 8];
bn_nist.c:        acc -= bp[15 - 8];
bn_nist.c:        acc += bp[11 - 8];
bn_nist.c:        acc += bp[11 - 8];
bn_nist.c:        acc += bp[12 - 8];
bn_nist.c:        acc += bp[12 - 8];
bn_nist.c:        acc += bp[13 - 8];
bn_nist.c:        acc -= bp[15 - 8];
bn_nist.c:        acc -= bp[8 - 8];
bn_nist.c:        acc -= bp[9 - 8];
bn_nist.c:        acc += bp[12 - 8];
bn_nist.c:        acc += bp[12 - 8];
bn_nist.c:        acc += bp[13 - 8];
bn_nist.c:        acc += bp[13 - 8];
bn_nist.c:        acc += bp[14 - 8];
bn_nist.c:        acc -= bp[9 - 8];
bn_nist.c:        acc -= bp[10 - 8];
bn_nist.c:        acc += bp[13 - 8];
bn_nist.c:        acc += bp[13 - 8];
bn_nist.c:        acc += bp[14 - 8];
bn_nist.c:        acc += bp[14 - 8];
bn_nist.c:        acc += bp[15 - 8];
bn_nist.c:        acc -= bp[10 - 8];
bn_nist.c:        acc -= bp[11 - 8];
bn_nist.c:        acc += bp[14 - 8];
bn_nist.c:        acc += bp[14 - 8];
bn_nist.c:        acc += bp[15 - 8];
bn_nist.c:        acc += bp[15 - 8];
bn_nist.c:        acc += bp[14 - 8];
bn_nist.c:        acc += bp[13 - 8];
bn_nist.c:        acc -= bp[8 - 8];
bn_nist.c:        acc -= bp[9 - 8];
bn_nist.c:        acc += bp[15 - 8];
bn_nist.c:        acc += bp[15 - 8];
bn_nist.c:        acc += bp[15 - 8];
bn_nist.c:        acc += bp[8 - 8];
bn_nist.c:        acc -= bp[10 - 8];
bn_nist.c:        acc -= bp[11 - 8];
bn_nist.c:        acc -= bp[12 - 8];
bn_nist.c:        acc -= bp[13 - 8];
bn_nist.c:            for (i = BN_NIST_256_TOP; i != 0; --i) {
bn_nist.c:        carry -= (int)bn_sub_words(r_d, r_d, t_d, BN_NIST_256_TOP);
bn_nist.c:        carry -= (int)bn_sub_words(r_d, r_d, t_d, BN_NIST_256_TOP);
bn_nist.c:        carry -= (int)bn_sub_words(r_d, r_d, t_d, BN_NIST_256_TOP);
bn_nist.c:        carry -= (int)bn_sub_words(r_d, r_d, t_d, BN_NIST_256_TOP);
bn_nist.c:            (int)bn_sub_words(r_d, r_d, _nist_p_256[carry - 1],
bn_nist.c:            (int)bn_add_words(r_d, r_d, _nist_p_256[-carry - 1],
bn_nist.c:        mask = 0 - (PTR_SIZE_INT) carry;
bn_nist.c:        0 - (PTR_SIZE_INT) (*u.f) (c_d, r_d, _nist_p_256[0], BN_NIST_256_TOP);
bn_nist.c:    mask &= 0 - (PTR_SIZE_INT) carry;
bn_nist.c:    r->top = BN_NIST_256_TOP;
bn_nist.c:        bn_cp_32(to, 0, from,  (a12) - 12) \
bn_nist.c:        bn_cp_32(to, 1, from,  (a11) - 12) \
bn_nist.c:        bn_cp_32(to, 2, from,  (a10) - 12) \
bn_nist.c:        bn_cp_32(to, 3, from,  (a9) - 12)  \
bn_nist.c:        bn_cp_32(to, 4, from,  (a8) - 12)  \
bn_nist.c:        bn_cp_32(to, 5, from,  (a7) - 12)  \
bn_nist.c:        bn_cp_32(to, 6, from,  (a6) - 12)  \
bn_nist.c:        bn_cp_32(to, 7, from,  (a5) - 12)  \
bn_nist.c:        bn_cp_32(to, 8, from,  (a4) - 12)  \
bn_nist.c:        bn_cp_32(to, 9, from,  (a3) - 12)  \
bn_nist.c:        bn_cp_32(to, 10, from, (a2) - 12)  \
bn_nist.c:        bn_cp_32(to, 11, from, (a1) - 12)  \
bn_nist.c:    int i, top = a->top;
bn_nist.c:    register BN_ULONG *r_d, *a_d = a->d;
bn_nist.c:        r_d = r->d;
bn_nist.c:    nist_cp_bn_0(buf.bn, a_d + BN_NIST_384_TOP, top - BN_NIST_384_TOP,
bn_nist.c:        acc += bp[12 - 12];
bn_nist.c:        acc += bp[21 - 12];
bn_nist.c:        acc += bp[20 - 12];
bn_nist.c:        acc -= bp[23 - 12];
bn_nist.c:        acc += bp[13 - 12];
bn_nist.c:        acc += bp[22 - 12];
bn_nist.c:        acc += bp[23 - 12];
bn_nist.c:        acc -= bp[12 - 12];
bn_nist.c:        acc -= bp[20 - 12];
bn_nist.c:        acc += bp[14 - 12];
bn_nist.c:        acc += bp[23 - 12];
bn_nist.c:        acc -= bp[13 - 12];
bn_nist.c:        acc -= bp[21 - 12];
bn_nist.c:        acc += bp[15 - 12];
bn_nist.c:        acc += bp[12 - 12];
bn_nist.c:        acc += bp[20 - 12];
bn_nist.c:        acc += bp[21 - 12];
bn_nist.c:        acc -= bp[14 - 12];
bn_nist.c:        acc -= bp[22 - 12];
bn_nist.c:        acc -= bp[23 - 12];
bn_nist.c:        acc += bp[21 - 12];
bn_nist.c:        acc += bp[21 - 12];
bn_nist.c:        acc += bp[16 - 12];
bn_nist.c:        acc += bp[13 - 12];
bn_nist.c:        acc += bp[12 - 12];
bn_nist.c:        acc += bp[20 - 12];
bn_nist.c:        acc += bp[22 - 12];
bn_nist.c:        acc -= bp[15 - 12];
bn_nist.c:        acc -= bp[23 - 12];
bn_nist.c:        acc -= bp[23 - 12];
bn_nist.c:        acc += bp[22 - 12];
bn_nist.c:        acc += bp[22 - 12];
bn_nist.c:        acc += bp[17 - 12];
bn_nist.c:        acc += bp[14 - 12];
bn_nist.c:        acc += bp[13 - 12];
bn_nist.c:        acc += bp[21 - 12];
bn_nist.c:        acc += bp[23 - 12];
bn_nist.c:        acc -= bp[16 - 12];
bn_nist.c:        acc += bp[23 - 12];
bn_nist.c:        acc += bp[23 - 12];
bn_nist.c:        acc += bp[18 - 12];
bn_nist.c:        acc += bp[15 - 12];
bn_nist.c:        acc += bp[14 - 12];
bn_nist.c:        acc += bp[22 - 12];
bn_nist.c:        acc -= bp[17 - 12];
bn_nist.c:        acc += bp[19 - 12];
bn_nist.c:        acc += bp[16 - 12];
bn_nist.c:        acc += bp[15 - 12];
bn_nist.c:        acc += bp[23 - 12];
bn_nist.c:        acc -= bp[18 - 12];
bn_nist.c:        acc += bp[20 - 12];
bn_nist.c:        acc += bp[17 - 12];
bn_nist.c:        acc += bp[16 - 12];
bn_nist.c:        acc -= bp[19 - 12];
bn_nist.c:        acc += bp[21 - 12];
bn_nist.c:        acc += bp[18 - 12];
bn_nist.c:        acc += bp[17 - 12];
bn_nist.c:        acc -= bp[20 - 12];
bn_nist.c:        acc += bp[22 - 12];
bn_nist.c:        acc += bp[19 - 12];
bn_nist.c:        acc += bp[18 - 12];
bn_nist.c:        acc -= bp[21 - 12];
bn_nist.c:        acc += bp[23 - 12];
bn_nist.c:        acc += bp[20 - 12];
bn_nist.c:        acc += bp[19 - 12];
bn_nist.c:        acc -= bp[22 - 12];
bn_nist.c:        nist_set_256(t_d, buf.bn, 0, 0, 0, 0, 0, 23 - 4, 22 - 4, 21 - 4);
bn_nist.c:            for (i = 3; i != 0; --i) {
bn_nist.c:        carry -= (int)bn_sub_words(r_d, r_d, t_d, BN_NIST_384_TOP);
bn_nist.c:        carry -= (int)bn_sub_words(r_d, r_d, t_d, BN_NIST_384_TOP);
bn_nist.c:        carry -= (int)bn_sub_words(r_d, r_d, t_d, BN_NIST_384_TOP);
bn_nist.c:            (int)bn_sub_words(r_d, r_d, _nist_p_384[carry - 1],
bn_nist.c:            (int)bn_add_words(r_d, r_d, _nist_p_384[-carry - 1],
bn_nist.c:        mask = 0 - (PTR_SIZE_INT) carry;
bn_nist.c:        0 - (PTR_SIZE_INT) (*u.f) (c_d, r_d, _nist_p_384[0], BN_NIST_384_TOP);
bn_nist.c:    mask &= 0 - (PTR_SIZE_INT) carry;
bn_nist.c:    r->top = BN_NIST_384_TOP;
bn_nist.c:#define BN_NIST_521_LSHIFT      (BN_BITS2-BN_NIST_521_RSHIFT)
bn_nist.c:    int top = a->top, i;
bn_nist.c:    BN_ULONG *r_d, *a_d = a->d, t_d[BN_NIST_521_TOP], val, tmp, *res;
bn_nist.c:        r_d = r->d;
bn_nist.c:    nist_cp_bn_0(t_d, a_d + (BN_NIST_521_TOP - 1),
bn_nist.c:                 top - (BN_NIST_521_TOP - 1), BN_NIST_521_TOP);
bn_nist.c:    for (val = t_d[0], i = 0; i < BN_NIST_521_TOP - 1; i++) {
bn_nist.c:        0 - (PTR_SIZE_INT) bn_sub_words(t_d, r_d, _nist_p_521,
bn_nist.c:    r->top = BN_NIST_521_TOP;
bn_ppc.c: * Copyright 2009-2022 The OpenSSL Project Authors. All Rights Reserved.
bn_prime.c: * Copyright 1995-2021 The OpenSSL Project Authors. All Rights Reserved.
bn_prime.c: * This includes 751 (which is not currently included in SP 800-89).
bn_prime.c: * combination with Miller-Rabin prime test, based on the sized of the prime.
bn_prime.c: * Use a minimum of 64 rounds of Miller-Rabin, which should give a false
bn_prime.c: * positive rate of 2^-128. If the size of the prime is larger than 2048
bn_prime.c: * to 128 rounds giving a false positive rate of 2^-256.
bn_prime.c:    switch (cb->ver) {
bn_prime.c:        /* Deprecated-style callbacks */
bn_prime.c:        if (!cb->cb.cb_1)
bn_prime.c:        cb->cb.cb_1(a, b, cb->arg);
bn_prime.c:        /* New-style callbacks */
bn_prime.c:        return cb->cb.cb_2(a, b, cb);
bn_prime.c:        if (i == -1)
bn_prime.c:         * for "safe prime" generation, check that (p-1)/2 is prime. Since a
bn_prime.c:            if (j == -1)
bn_prime.c:            if (j == -1)
bn_prime.c:            if (!BN_GENCB_call(cb, 2, c1 - 1))
bn_prime.c:    /* we have a prime :-) */
bn_prime.c: * See FIPS 186-4 C.3.1 Miller Rabin Probabilistic Primality Test.
bn_prime.c: * Returns 0 when composite, 1 when probable prime, -1 on error.
bn_prime.c:    int i, status, ret = -1;
bn_prime.c:        return -1;
bn_prime.c:            if (mod == (BN_ULONG)-1)
bn_prime.c:                return -1;
bn_prime.c:        if (!BN_GENCB_call(cb, 1, -1))
bn_prime.c:            return -1;
bn_prime.c: * Refer to FIPS 186-4 C.3.2 Enhanced Miller-Rabin Probabilistic Primality Test.
bn_prime.c: * OR C.3.1 Miller-Rabin Probabilistic Primality Test (if enhanced is zero).
bn_prime.c:            /* w1 := w - 1 */
bn_prime.c:            /* w3 := w - 3 */
bn_prime.c:    /* (Step 1) Calculate largest integer 'a' such that 2^a divides w-1 */
bn_prime.c:    /* (Step 2) m = (w-1) / 2^a */
bn_prime.c:        /* (Step 4.1) obtain a Random string of bits b where 1 < b < w-1 */
bn_prime.c:                || !BN_add_word(b, 2)) /* 1 < b < w-1 */
bn_prime.c:        /* (Step 4.6) if (z = 1 or z = w-1) */
bn_prime.c:        /* (Step 4.7) for j = 1 to a-1 */
bn_prime.c:            /* (Step 4.7.1 - 4.7.2) x = z. z = x^2 mod w */
bn_prime.c:        /* At this point z = b^((w-1)/2) mod w */
bn_prime.c:        /* (Steps 4.8 - 4.9) x = z, z = x^2 mod w */
bn_prime.c:        /* (Step 4.11) x = b^(w-1) mod w */
bn_prime.c:            /* (Step 4.1.2) g = GCD(x-1, w) */
bn_prime.c:            /* (Steps 4.1.3 - 4.1.4) */
bn_prime.c:    BN_ULONG maxdelta = BN_MASK2 - primes[trial_divisions - 1];
bn_prime.c:        if (mod == (BN_ULONG)-1)
bn_prime.c:         * gcd(rnd-1,primes) == 1 (except for 2)
bn_prime.c:    BN_ULONG maxdelta = BN_MASK2 - primes[trial_divisions - 1];
bn_prime.c:    if (maxdelta > BN_MASK2 - BN_get_word(add))
bn_prime.c:        maxdelta = BN_MASK2 - BN_get_word(add);
bn_prime.c:    /* we need ((rnd-rem) % add) == 0 */
bn_prime.c:        if (mod == (BN_ULONG)-1)
bn_prime.c:        /* rnd mod p == 1 implies q = (rnd-1)/2 is divisible by p */
bn_prime.h: * Copyright 1998-2021 The OpenSSL Project Authors. All Rights Reserved.
bn_prime.pl:# Copyright 1998-2021 The OpenSSL Project Authors. All Rights Reserved.
bn_prime.pl: * Copyright 1998-$YEAR The OpenSSL Project Authors. All Rights Reserved.
bn_prime.pl:loop: while ($#primes < $num-1) {
bn_print.c: * Copyright 1995-2017 The OpenSSL Project Authors. All Rights Reserved.
bn_print.c:    if ((a->neg) && BIO_write(bp, "-", 1) != 1)
bn_print.c:    for (i = a->top - 1; i >= 0; i--) {
bn_print.c:        for (j = BN_BITS2 - 4; j >= 0; j -= 4) {
bn_print.c:            v = (int)((a->d[i] >> j) & 0x0f);
bn_rand.c: * Copyright 1995-2021 The OpenSSL Project Authors. All Rights Reserved.
bn_rand.c:    bit = (bits - 1) % 8;
bn_rand.c:                buf[i] = buf[i - 1];
bn_rand.c:                buf[0] |= (3 << (bit - 1));
bn_rand.c:        buf[bytes - 1] |= 1;
bn_rand.c:    if (range->neg || BN_is_zero(range)) {
bn_rand.c:    /* BN_is_bit_set(range, n - 1) always holds */
bn_rand.c:    else if (!BN_is_bit_set(range, n - 2) && !BN_is_bit_set(range, n - 3)) {
bn_rand.c:             * If r < 3*range, use r := r MOD range (which is either r, r -
bn_rand.c:             * range, or r - 2*range). Otherwise, iterate once more. Since
bn_rand.c:            if (!--count) {
bn_rand.c:            if (!--count) {
bn_rand.c:        todo = num_k_bytes - done;
bn_recp.c: * Copyright 1995-2020 The OpenSSL Project Authors. All Rights Reserved.
bn_recp.c:    bn_init(&(recp->N));
bn_recp.c:    bn_init(&(recp->Nr));
bn_recp.c:    bn_init(&(ret->N));
bn_recp.c:    bn_init(&(ret->Nr));
bn_recp.c:    ret->flags = BN_FLG_MALLOCED;
bn_recp.c:    BN_free(&recp->N);
bn_recp.c:    BN_free(&recp->Nr);
bn_recp.c:    if (recp->flags & BN_FLG_MALLOCED)
bn_recp.c:    if (!BN_copy(&(recp->N), d))
bn_recp.c:    BN_zero(&(recp->Nr));
bn_recp.c:    recp->num_bits = BN_num_bits(d);
bn_recp.c:    recp->shift = 0;
bn_recp.c:    if (BN_ucmp(m, &(recp->N)) < 0) {
bn_recp.c:    j = recp->num_bits << 1;
bn_recp.c:    if (i != recp->shift)
bn_recp.c:        recp->shift = BN_reciprocal(&(recp->Nr), &(recp->N), i, ctx);
bn_recp.c:    /* BN_reciprocal could have returned -1 for an error */
bn_recp.c:    if (recp->shift == -1)
bn_recp.c:    /*-
bn_recp.c:     * d := |round(round(m / 2^BN_num_bits(N)) * recp->Nr / 2^(i - BN_num_bits(N)))|
bn_recp.c:     *    = |round(round(m / 2^BN_num_bits(N)) * round(2^i / N) / 2^(i - BN_num_bits(N)))|
bn_recp.c:    if (!BN_rshift(a, m, recp->num_bits))
bn_recp.c:    if (!BN_mul(b, a, &(recp->Nr), ctx))
bn_recp.c:    if (!BN_rshift(d, b, i - recp->num_bits))
bn_recp.c:    d->neg = 0;
bn_recp.c:    if (!BN_mul(b, &(recp->N), d, ctx))
bn_recp.c:    r->neg = 0;
bn_recp.c:    while (BN_ucmp(r, &(recp->N)) >= 0) {
bn_recp.c:        if (!BN_usub(r, r, &(recp->N)))
bn_recp.c:    r->neg = BN_is_zero(r) ? 0 : m->neg;
bn_recp.c:    d->neg = m->neg ^ recp->N.neg;
bn_recp.c:    int ret = -1;
bn_rsa_fips186_4.c: * Copyright 2018-2021 The OpenSSL Project Authors. All Rights Reserved.
bn_rsa_fips186_4.c: * Copyright (c) 2018-2019, Oracle and/or its affiliates.  All rights reserved.
bn_rsa_fips186_4.c: * According to NIST SP800-131A "Transitioning the use of cryptographic
bn_rsa_fips186_4.c: * FIPS 186-4 relies on the use of the auxiliary primes p1, p2, q1 and q2 that
bn_rsa_fips186_4.c: * Table B.1 in FIPS 186-4 specifies RSA modulus lengths of 2048 and
bn_rsa_fips186_4.c: * FIPS 186-5 Table A.1 includes an additional entry for 4096 which has been
bn_rsa_fips186_4.c: * FIPS 186-5 Table A.1. "Min length of auxiliary primes p1, p2, q1, q2".
bn_rsa_fips186_4.c: * (FIPS 186-5 has an entry for >= 4096 bits).
bn_rsa_fips186_4.c: * FIPS 186-5 Table A.1 "Max of len(p1) + len(p2) and
bn_rsa_fips186_4.c: * (FIPS 186-5 has an entry for >= 4096 bits).
bn_rsa_fips186_4.c: * See section FIPS 186-4 B.3.6 (Steps 4.2/5.2).
bn_rsa_fips186_4.c: * See FIPS 186-4 B.3.6 (Steps 4 & 5)
bn_rsa_fips186_4.c:    /* (Steps 4.2/5.2) - find first auxiliary probable primes */
bn_rsa_fips186_4.c:    /* (Steps 4.3/5.3) - generate prime */
bn_rsa_fips186_4.c: * See FIPS 186-4 C.9 "Compute a Probable Prime Factor Based on Auxiliary
bn_rsa_fips186_4.c: * Primes". Used by FIPS 186-4 B.3.6 Section (4.3) for p and Section (5.3) for q.
bn_rsa_fips186_4.c:     * range = ((2^(nlen/2))) - (1/sqrt(2) * 2^(nlen/2))
bn_rsa_fips186_4.c:                       bits - BN_num_bits(&ossl_bn_inv_sqrt_2))
bn_rsa_fips186_4.c:            /* (Step 2) R = ((r2^-1 mod 2r1) * r2) - ((2r1^-1 mod r2)*2r1) */
bn_rsa_fips186_4.c:            && BN_mul(R, R, r2, ctx) /* R = (r2^-1 mod 2r1) * r2 */
bn_rsa_fips186_4.c:            && BN_mul(tmp, tmp, r1x2, ctx) /* tmp = (2r1^-1 mod r2)*2r1 */
bn_rsa_fips186_4.c:     * In FIPS 186-4 imax was set to 5 * nlen/2.
bn_rsa_fips186_4.c:     * Analysis by Allen Roginsky (See https://csrc.nist.gov/CSRC/media/Publications/fips/186/4/final/documents/comments-received-fips186-4-december-2015.pdf
bn_rsa_fips186_4.c:     * FIPS186-5 Appendix B.9 Step 9.
bn_rsa_fips186_4.c:             *    sqrt(2) * 2^(nlen/2-1) <= Random X <= (2^(nlen/2)) - 1.
bn_rsa_fips186_4.c:        /* (Step 4) Y = X + ((R - X) mod 2r1r2) */
bn_rsa_fips186_4.c:            /* (Step 7) If GCD(Y-1) == 1 & Y is probably prime then return Y */
bn_rsa_fips186_4.c:            /* (Step 8-10) */
bn_shift.c: * Copyright 1995-2020 The OpenSSL Project Authors. All Rights Reserved.
bn_shift.c:        r->neg = a->neg;
bn_shift.c:        if (bn_wexpand(r, a->top + 1) == NULL)
bn_shift.c:        r->top = a->top;
bn_shift.c:        if (bn_wexpand(r, a->top + 1) == NULL)
bn_shift.c:    ap = a->d;
bn_shift.c:    rp = r->d;
bn_shift.c:    for (i = 0; i < a->top; i++) {
bn_shift.c:        c = t >> (BN_BITS2 - 1);
bn_shift.c:    r->top += c;
bn_shift.c:    i = a->top;
bn_shift.c:    ap = a->d;
bn_shift.c:        r->neg = a->neg;
bn_shift.c:    rp = r->d;
bn_shift.c:    r->top = i;
bn_shift.c:    t = ap[--i];
bn_shift.c:    c = t << (BN_BITS2 - 1);
bn_shift.c:    r->top -= (t == 1);
bn_shift.c:        t = ap[--i];
bn_shift.c:        c = t << (BN_BITS2 - 1);
bn_shift.c:    if (!r->top)
bn_shift.c:        r->neg = 0; /* don't allow negative zero */
bn_shift.c: * |n % BN_BITS2|, but not |n / BN_BITS2|. Or in other words pre-condition
bn_shift.c: * for constant-time-ness is |n < BN_BITS2| or |n / BN_BITS2| being
bn_shift.c: * non-secret.
bn_shift.c:    if (bn_wexpand(r, a->top + nw + 1) == NULL)
bn_shift.c:    if (a->top != 0) {
bn_shift.c:        rb = BN_BITS2 - lb;
bn_shift.c:        rmask = (BN_ULONG)0 - rb;  /* rmask = 0 - (rb != 0) */
bn_shift.c:        f = &(a->d[0]);
bn_shift.c:        t = &(r->d[nw]);
bn_shift.c:        l = f[a->top - 1];
bn_shift.c:        t[a->top] = (l >> rb) & rmask;
bn_shift.c:        for (i = a->top - 1; i > 0; i--) {
bn_shift.c:            l = f[i - 1];
bn_shift.c:        r->d[nw] = 0;
bn_shift.c:        memset(r->d, 0, sizeof(*t) * nw);
bn_shift.c:    r->neg = a->neg;
bn_shift.c:    r->top = a->top + nw + 1;
bn_shift.c:    r->flags |= BN_FLG_FIXED_TOP;
bn_shift.c: * |n % BN_BITS2|, but not |n / BN_BITS2|. Or in other words pre-condition
bn_shift.c: * for constant-time-ness for sufficiently[!] zero-padded inputs is
bn_shift.c: * |n < BN_BITS2| or |n / BN_BITS2| being non-secret.
bn_shift.c:    if (nw >= a->top) {
bn_shift.c:    lb = BN_BITS2 - rb;
bn_shift.c:    mask = (BN_ULONG)0 - lb;   /* mask = 0 - (lb != 0) */
bn_shift.c:    top = a->top - nw;
bn_shift.c:    t = &(r->d[0]);
bn_shift.c:    f = &(a->d[nw]);
bn_shift.c:    for (i = 0; i < top - 1; i++) {
bn_shift.c:    r->neg = a->neg;
bn_shift.c:    r->top = top;
bn_shift.c:    r->flags |= BN_FLG_FIXED_TOP;
bn_sparc.c: * Copyright 2005-2021 The OpenSSL Project Authors. All Rights Reserved.
bn_sparc.c:            bn_mul_mont_f worker = funcs[num / 16 - 1];
bn_sparc.c:                  * out because FMADD-capable processors where FPU
bn_sparc.c:                  * code path is undesirable are also VIS3-capable and
bn_sqr.c: * Copyright 1995-2018 The OpenSSL Project Authors. All Rights Reserved.
bn_sqr.c: * I've just gone over this and it is now %20 faster on x86 - eay - 27 Jun 96
bn_sqr.c:    al = a->top;
bn_sqr.c:        r->top = 0;
bn_sqr.c:        r->neg = 0;
bn_sqr.c:    max = 2 * al;               /* Non-zero (from above) */
bn_sqr.c:        bn_sqr_normal(rr->d, a->d, 4, t);
bn_sqr.c:        bn_sqr_comba4(rr->d, a->d);
bn_sqr.c:        bn_sqr_normal(rr->d, a->d, 8, t);
bn_sqr.c:        bn_sqr_comba8(rr->d, a->d);
bn_sqr.c:            bn_sqr_normal(rr->d, a->d, al, t);
bn_sqr.c:            j = 1 << (j - 1);
bn_sqr.c:                bn_sqr_recursive(rr->d, a->d, al, tmp->d);
bn_sqr.c:                bn_sqr_normal(rr->d, a->d, al, tmp->d);
bn_sqr.c:        bn_sqr_normal(rr->d, a->d, al, tmp->d);
bn_sqr.c:    rr->neg = 0;
bn_sqr.c:    rr->top = max;
bn_sqr.c:    rr->flags |= BN_FLG_FIXED_TOP;
bn_sqr.c:    rp[0] = rp[max - 1] = 0;
bn_sqr.c:    if (--j > 0) {
bn_sqr.c:        rp[j] = bn_mul_words(rp, ap, j, ap[-1]);
bn_sqr.c:    for (i = n - 2; i > 0; i--) {
bn_sqr.c:        j--;
bn_sqr.c:        rp[j] = bn_mul_add_words(rp, ap, j, ap[-1]);
bn_sqr.c:/*-
bn_sqr.c: * a[0]*b[0]+a[1]*b[1]+(a[0]-a[1])*(b[1]-b[0])
bn_sqr.c:    /* r=(a[0]-a[1])*(a[1]-a[0]) */
bn_sqr.c:    /*-
bn_sqr.c:     * t[32] holds (a[0]-a[1])*(a[1]-a[0]), it is negative or zero
bn_sqr.c:    c1 -= (int)(bn_sub_words(&(t[n2]), t, &(t[n2]), n2));
bn_sqr.c:    /*-
bn_sqr.c:     * t[32] holds (a[0]-a[1])*(a[1]-a[0])+(a[0]*a[0])+(a[1]*a[1])
bn_sqrt.c: * Copyright 2000-2022 The OpenSSL Project Authors. All Rights Reserved.
bn_sqrt.c:    /* now write  |p| - 1  as  2^e*q  where  q  is odd */
bn_sqrt.c:        /*-
bn_sqrt.c:         * The easy case:  (|p|-1)/2  is odd, so 2 has an inverse
bn_sqrt.c:         * modulo  (|p|-1)/2,  and square roots can be computed
bn_sqrt.c:         *     2 * (|p|+1)/4 == 1   (mod (|p|-1)/2),
bn_sqrt.c:         * so we can use exponent  (|p|+1)/4,  i.e.  (|p|-3)/4 + 1.
bn_sqrt.c:        q->neg = 0;
bn_sqrt.c:        /*-
bn_sqrt.c:         * In this case  2  is always a non-square since
bn_sqrt.c:         * Legendre(2,p) = (-1)^((p^2-1)/8)  for any odd prime.
bn_sqrt.c:         * So if  a  really is a square, then  2*a  is a non-square.
bn_sqrt.c:         *      b := (2*a)^((|p|-5)/8),
bn_sqrt.c:         *     i^2 = (2*a)^((1 + (|p|-5)/4)*2)
bn_sqrt.c:         *         = (2*a)^((p-1)/2)
bn_sqrt.c:         *         = -1;
bn_sqrt.c:         *      x := a*b*(i-1),
bn_sqrt.c:         *     x^2 = a^2 * b^2 * (i^2 - 2*i + 1)
bn_sqrt.c:         *         = a^2 * b^2 * (-2*i)
bn_sqrt.c:         *         = a*(-i)*(2*a*b^2)
bn_sqrt.c:         *         = a*(-i)*i
bn_sqrt.c:         * URL: https://listserv.nodak.edu/cgi-bin/wa.exe?A2=ind9211&L=NMBRTHRY&P=4026
bn_sqrt.c:        /* b := (2*a)^((|p|-5)/8) */
bn_sqrt.c:        q->neg = 0;
bn_sqrt.c:        /* t := (2*a)*b^2 - 1 */
bn_sqrt.c:    q->neg = 0;
bn_sqrt.c:                if (!(p->neg ? BN_add : BN_sub) (y, y, p))
bn_sqrt.c:        if (r < -1)
bn_sqrt.c:    if (r != -1) {
bn_sqrt.c:         * Many rounds and still no non-square -- this is more likely a bug
bn_sqrt.c:         * some y such that r == -1.
bn_sqrt.c:     * Now that we have some non-square, we can find an element of order 2^e
bn_sqrt.c:    /*-
bn_sqrt.c:    /* t := (q-1)/2  (note that  q  is odd) */
bn_sqrt.c:    /* x := a^((q-1)/2) */
bn_sqrt.c:        /*-
bn_sqrt.c:         *    y^2^(e-1) = -1,
bn_sqrt.c:         *    b^2^(e-1) = 1.
bn_sqrt.c:        /* t := y^2^(e - i - 1) */
bn_sqrt.c:        for (j = e - i - 1; j > 0; j--) {
bn_sqrt.c:         * verify the result -- the input might have been not a square (test
bn_srp.c: * Copyright 2014-2021 The OpenSSL Project Authors. All Rights Reserved.
bn_word.c: * Copyright 1995-2016 The OpenSSL Project Authors. All Rights Reserved.
bn_word.c:        return (BN_ULONG)-1;
bn_word.c:            return (BN_ULONG)-1;
bn_word.c:    for (i = a->top - 1; i >= 0; i--) {
bn_word.c:        ret = ((ret << BN_BITS4) | ((a->d[i] >> BN_BITS4) & BN_MASK2l)) % w;
bn_word.c:        ret = ((ret << BN_BITS4) | (a->d[i] & BN_MASK2l)) % w;
bn_word.c:        ret = (BN_ULLONG) (((ret << (BN_ULLONG) BN_BITS2) | a->d[i]) %
bn_word.c:        return (BN_ULONG)-1;
bn_word.c:    if (a->top == 0)
bn_word.c:    j = BN_BITS2 - BN_num_bits_word(w);
bn_word.c:        return (BN_ULONG)-1;
bn_word.c:    for (i = a->top - 1; i >= 0; i--) {
bn_word.c:        l = a->d[i];
bn_word.c:        ret = (l - ((d * w) & BN_MASK2)) & BN_MASK2;
bn_word.c:        a->d[i] = d;
bn_word.c:    if ((a->top > 0) && (a->d[a->top - 1] == 0))
bn_word.c:        a->top--;
bn_word.c:    if (!a->top)
bn_word.c:        a->neg = 0; /* don't allow negative zero */
bn_word.c:    if (a->neg) {
bn_word.c:        a->neg = 0;
bn_word.c:            a->neg = !(a->neg);
bn_word.c:    for (i = 0; w != 0 && i < a->top; i++) {
bn_word.c:        a->d[i] = l = (a->d[i] + w) & BN_MASK2;
bn_word.c:    if (w && i == a->top) {
bn_word.c:        if (bn_wexpand(a, a->top + 1) == NULL)
bn_word.c:        a->top++;
bn_word.c:        a->d[i] = w;
bn_word.c:    if (a->neg) {
bn_word.c:        a->neg = 0;
bn_word.c:        a->neg = 1;
bn_word.c:    if ((a->top == 1) && (a->d[0] < w)) {
bn_word.c:        a->d[0] = w - a->d[0];
bn_word.c:        a->neg = 1;
bn_word.c:        if (a->d[i] >= w) {
bn_word.c:            a->d[i] -= w;
bn_word.c:            a->d[i] = (a->d[i] - w) & BN_MASK2;
bn_word.c:    if ((a->d[i] == 0) && (i == (a->top - 1)))
bn_word.c:        a->top--;
bn_word.c:    if (a->top) {
bn_word.c:            ll = bn_mul_words(a->d, a->d, a->top, w);
bn_word.c:                if (bn_wexpand(a, a->top + 1) == NULL)
bn_word.c:                a->d[a->top++] = ll;
bn_x931p.c: * Copyright 2011-2021 The OpenSSL Project Authors. All Rights Reserved.
bn_x931p.c:    if (p->neg && !BN_add(p, p, p1p2))
bn_x931p.c:     * The random value Xp must be between sqrt(2) * 2^(nbits-1) and 2^nbits
bn_x931p.c:     * - 1. By setting the top two bits we ensure that the lower bound is
bn_x931p.c:        /* Check that |Xp - Xq| > 2^(nbits - 100) */
bn_x931p.c:        if (BN_num_bits(t) > (nbits - 100))
bn_x931p.c: * Xp2 only 'p' needs to be non-NULL. If any of the others are not NULL the
bn_x931p.c: * relevant parameter will be stored in it. Due to the fact that |Xp - Xq| >
bn_x931p.c: * 2^(nbits - 100) must be satisfied Xp and Xq are generated using the
build.info:IF[{- !$disabled{asm} -}]
build.info:  # OPENSSL_BN_ASM_PART_WORDS     For any collection with /-586/ file names
build.info:  # OPENSSL_BN_ASM_MONT           For any collection with /-mont/ file names
build.info:  # OPENSSL_BN_ASM_MONT5          For any collection with /-mont5/ file names
build.info:  # OPENSSL_BN_ASM_GF2m           For any collection with /-gf2m/ file names
build.info:  # BN_DIV3W                      For any collection with /-div3w/ file names
build.info:  $BNASM_x86=bn-586.S co-586.S x86-mont.S x86-gf2m.S
build.info:  # bn-586 is the only one implementing bn_*_part_words
build.info:          x86_64-mont.s x86_64-mont5.s x86_64-gf2m.s rsaz_exp.c rsaz-x86_64.s \
build.info:          rsaz-avx2.s rsaz_exp_x2.c rsaz-2k-avx512.s rsaz-3k-avx512.s rsaz-4k-avx512.s
build.info:  IF[{- $config{target} !~ /^VC/ -}]
build.info:    $BNASM_x86_64=asm/x86_64-gcc.c $BNASM_x86_64
build.info:  IF[{- $config{target} !~ /^VC/ -}]
build.info:    $BNASM_ia64=bn-ia64.s ia64-mont.s
build.info:    $BNASM_ia64=bn_asm.c ia64-mont.s
build.info:  $BNASM_sparcv9=asm/sparcv8plus.S sparcv9-mont.S sparcv9a-mont.S vis3-mont.S \
build.info:          sparct4-mont.S bn_sparc.c
build.info:  $BNASM_sparcv9_ec2m=sparcv9-gf2m.S
build.info:  $BNASM_alpha=bn_asm.c alpha-mont.S
build.info:  $BNASM_mips32=bn-mips.S mips-mont.S
build.info:  IF[{- ($target{perlasm_scheme} // '') eq '31' -}]
build.info:    $BNASM_s390x=bn_asm.c s390x-mont.S
build.info:    $BNASM_s390x=asm/s390x.S s390x-mont.S
build.info:  $BNASM_s390x_ec2m=s390x-gf2m.s
build.info:  $BNASM_armv4=bn_asm.c armv4-mont.S
build.info:  $BNASM_armv4_ec2m=armv4-gf2m.S
build.info:  $BNASM_aarch64=bn_asm.c armv8-mont.S
build.info:  $BNASM_parisc11=bn_asm.c parisc-mont.s
build.info:  $BNASM_ppc32=bn_ppc.c bn-ppc.s ppc-mont.s
build.info:  $BNASM_c64xplus=asm/bn-c64xplus.asm
build.info:  $BNASM_c64xplus_ec2m=c64xplus-gf2m.s
build.info:  IF[$BNASM_{- $target{asm_arch} -}]
build.info:    $BNASM=$BNASM_{- $target{asm_arch} -}
build.info:    $BNDEF=$BNDEF_{- $target{asm_arch} -}
build.info:    IF[{- !$disabled{ec2m} -}]
build.info:      $BNASM=$BNASM $BNASM_{- $target{asm_arch} -}_ec2m
build.info:      $BNDEF=$BNDEF $BNDEF_{- $target{asm_arch} -}_ec2m
build.info:    IF[{- !$disabled{sse2} -}]
build.info:      $BNDEF=$BNDEF $BNDEF_{- $target{asm_arch} -}_sse2
build.info:IF[{- !$disabled{'deprecated-0.9.8'} -}]
build.info:IF[{- !$disabled{'deprecated-3.0'} -}]
build.info:GENERATE[bn-586.S]=asm/bn-586.pl
build.info:DEPEND[bn-586.S]=../perlasm/x86asm.pl
build.info:GENERATE[co-586.S]=asm/co-586.pl
build.info:DEPEND[co-586.S]=../perlasm/x86asm.pl
build.info:GENERATE[x86-mont.S]=asm/x86-mont.pl
build.info:DEPEND[x86-mont.S]=../perlasm/x86asm.pl
build.info:GENERATE[x86-gf2m.S]=asm/x86-gf2m.pl
build.info:DEPEND[x86-gf2m.S]=../perlasm/x86asm.pl
build.info:GENERATE[sparcv9a-mont.S]=asm/sparcv9a-mont.pl
build.info:INCLUDE[sparcv9a-mont.o]=..
build.info:GENERATE[sparcv9-mont.S]=asm/sparcv9-mont.pl
build.info:INCLUDE[sparcv9-mont.o]=..
build.info:GENERATE[vis3-mont.S]=asm/vis3-mont.pl
build.info:INCLUDE[vis3-mont.o]=..
build.info:GENERATE[sparct4-mont.S]=asm/sparct4-mont.pl
build.info:INCLUDE[sparct4-mont.o]=..
build.info:GENERATE[sparcv9-gf2m.S]=asm/sparcv9-gf2m.pl
build.info:INCLUDE[sparcv9-gf2m.o]=..
build.info:GENERATE[bn-mips.S]=asm/mips.pl
build.info:INCLUDE[bn-mips.o]=..
build.info:GENERATE[mips-mont.S]=asm/mips-mont.pl
build.info:INCLUDE[mips-mont.o]=..
build.info:GENERATE[s390x-mont.S]=asm/s390x-mont.pl
build.info:GENERATE[s390x-gf2m.s]=asm/s390x-gf2m.pl
build.info:GENERATE[x86_64-mont.s]=asm/x86_64-mont.pl
build.info:GENERATE[x86_64-mont5.s]=asm/x86_64-mont5.pl
build.info:GENERATE[x86_64-gf2m.s]=asm/x86_64-gf2m.pl
build.info:GENERATE[rsaz-x86_64.s]=asm/rsaz-x86_64.pl
build.info:GENERATE[rsaz-avx2.s]=asm/rsaz-avx2.pl
build.info:GENERATE[rsaz-2k-avx512.s]=asm/rsaz-2k-avx512.pl
build.info:GENERATE[rsaz-3k-avx512.s]=asm/rsaz-3k-avx512.pl
build.info:GENERATE[rsaz-4k-avx512.s]=asm/rsaz-4k-avx512.pl
build.info:GENERATE[bn-ia64.s]=asm/ia64.S
build.info:GENERATE[ia64-mont.s]=asm/ia64-mont.pl
build.info:GENERATE[parisc-mont.s]=asm/parisc-mont.pl
build.info:# ppc - AIX, Linux, MacOS X...
build.info:GENERATE[bn-ppc.s]=asm/ppc.pl
build.info:GENERATE[ppc-mont.s]=asm/ppc-mont.pl
build.info:GENERATE[ppc64-mont.s]=asm/ppc64-mont.pl
build.info:GENERATE[alpha-mont.S]=asm/alpha-mont.pl
build.info:GENERATE[armv4-mont.S]=asm/armv4-mont.pl
build.info:INCLUDE[armv4-mont.o]=..
build.info:GENERATE[armv4-gf2m.S]=asm/armv4-gf2m.pl
build.info:INCLUDE[armv4-gf2m.o]=..
build.info:GENERATE[armv8-mont.S]=asm/armv8-mont.pl
build.info:INCLUDE[armv8-mont.o]=..
README.pod:bn_print, bn_dump, bn_set_max, bn_set_high, bn_set_low - BIGNUM
README.pod:=head2 Low-level arithmetic operations
README.pod:word-wise, and places the low and high bytes of the result in B<rp>.
README.pod:arrays B<ap>, B<bp> and B<rp>.  It computes B<ap> - B<bp>, places the
README.pod:and B<b>.  It returns 1, 0 and -1 if B<a> is greater than, equal and
README.pod:The bn_fix_top() macro reduces B<a-E<gt>top> to point to the most
README.pod:significant non-zero word plus one when B<a> has shrunk.
README.pod:bn_check_top() verifies that C<((a)-E<gt>top E<gt>= 0 && (a)-E<gt>top
README.pod:E<lt>= (a)-E<gt>dmax)>.  A violation will cause the program to abort.
README.pod:This is used by bn_set_low() and bn_set_high() to make B<r> a read-only
README.pod:Copyright 2000-2016 The OpenSSL Project Authors. All Rights Reserved.
rsaz_exp.c: * Copyright 2013-2016 The OpenSSL Project Authors. All Rights Reserved.
rsaz_exp.c: * See crypto/bn/asm/rsaz-avx2.pl for further details.
rsaz_exp.c:    unsigned char *p_str = storage + (64 - ((size_t)storage % 64));
rsaz_exp.c:    while (index > -1) {        /* loop for the remaining 127 windows */
rsaz_exp.c:        index -= 5;
rsaz_exp.c: * See crypto/bn/rsaz-x86_64.pl for further details.
rsaz_exp.c:    unsigned char *table = storage + (64 - ((size_t)storage % 64));
rsaz_exp.c:    temp[0] = 0 - m[0];
rsaz_exp.c:    for (index = 62; index >= 0; index--) {
rsaz_exp.h: * Copyright 2013-2021 The OpenSSL Project Authors. All Rights Reserved.
rsaz_exp.h:    carry -= bn_sub_words(tmp, r, m, num);
rsaz_exp_x2.c: * Copyright 2020-2021 The OpenSSL Project Authors. All Rights Reserved.
rsaz_exp_x2.c: * Copyright (c) 2020-2021, Intel Corporation. All Rights Reserved.
rsaz_exp_x2.c:    ((unsigned char *)(ptr) + (boundary - (((size_t)(ptr)) & (boundary - 1))))
rsaz_exp_x2.c:/* 52-bit mask */
rsaz_exp_x2.c:    (((digits_num) * 64 + (register_size) - 1) / (register_size))
rsaz_exp_x2.c:/* Number of |digit_size|-bit digits in |bitsize|-bit value */
rsaz_exp_x2.c:    return (bitsize + digit_size - 1) / digit_size;
rsaz_exp_x2.c: *    crypto/bn/asm/rsaz-avx512.pl
rsaz_exp_x2.c: *  52xZZ - data represented as array of ZZ digits in 52-bit radix
rsaz_exp_x2.c: *  _x1_/_x2_ - 1 or 2 independent inputs/outputs
rsaz_exp_x2.c: *  _ifma256 - uses 256-bit wide IFMA ISA (AVX512_IFMA256)
rsaz_exp_x2.c: *   - 2x1024
rsaz_exp_x2.c: *   - 2x1536
rsaz_exp_x2.c: *   - 2x2048
rsaz_exp_x2.c: *  [out] res|i|      - result of modular exponentiation: array of qword values
rsaz_exp_x2.c: *  [in]  base|i|     - base
rsaz_exp_x2.c: *  [in]  exp|i|      - exponent
rsaz_exp_x2.c: *  [in]  m|i|        - moduli
rsaz_exp_x2.c: *  [in]  rr|i|       - Montgomery parameter RR = R^2 mod m|i|
rsaz_exp_x2.c: *  [in]  k0_|i|      - Montgomery parameter k0 = -1/m|i| mod 2^64
rsaz_exp_x2.c: *  [in]  factor_size - moduli bit size
rsaz_exp_x2.c:     * Number of word-size (BN_ULONG) digits to store exponent in redundant
rsaz_exp_x2.c:    int coeff_pow = 4 * (DIGIT_SIZE * exp_digits - factor_size);
rsaz_exp_x2.c:    /* Convert base_i, m_i, rr_i, from regular to 52-bit radix */
rsaz_exp_x2.c:     * RR -> RR' transformation steps:
rsaz_exp_x2.c:     *  k = 4 * (52 * digits52 - modlen)
rsaz_exp_x2.c:    /* Dual (2-exps in parallel) exponentiation */
rsaz_exp_x2.c: * Dual {1024,1536,2048}-bit w-ary modular exponentiation using prime moduli of
rsaz_exp_x2.c: *  [out] res      - result of modular exponentiation: 2x{20,30,40} qword
rsaz_exp_x2.c: *  [in]  base     - base (2x{20,30,40} qword values in 2^52 radix)
rsaz_exp_x2.c: *  [in]  exp      - array of 2 pointers to {16,24,32} qword values in 2^64 radix.
rsaz_exp_x2.c: *  [in]  m        - moduli (2x{20,30,40} qword values in 2^52 radix)
rsaz_exp_x2.c: *  [in]  rr       - Montgomery parameter for 2 moduli:
rsaz_exp_x2.c: *  [in]  k0       - Montgomery parameter for 2 moduli: k0 = -1/m mod 2^64
rsaz_exp_x2.c:    int exp_win_mask = (1U << exp_win_size) - 1;
rsaz_exp_x2.c:    * Number of digits (64-bit words) in redundant representation to handle
rsaz_exp_x2.c:    /* Pre-computed table of base powers */
rsaz_exp_x2.c:     * Compute table of powers base^i, i = 0, ..., (2^EXP_WIN_SIZE) - 1
rsaz_exp_x2.c:    expz[1 * (exp_digits + 1) - 1] = 0;
rsaz_exp_x2.c:    expz[2 * (exp_digits + 1) - 1] = 0;
rsaz_exp_x2.c:        int exp_bit_no = modulus_bitsize - rem;
rsaz_exp_x2.c:         *      exp_bit_no = modulus_bitsize - exp_win_size
rsaz_exp_x2.c:        /* Process 1-st exp window - just init result */
rsaz_exp_x2.c:        for (exp_bit_no -= exp_win_size; exp_bit_no >= 0; exp_bit_no -= exp_win_size) {
rsaz_exp_x2.c:            /* Extract pre-computed multiplier from the table */
rsaz_exp_x2.c:                     * when 64-bit boundaries are crossed.
rsaz_exp_x2.c:                    if (exp_chunk_shift > 64 - exp_win_size) {
rsaz_exp_x2.c:                        T <<= (64 - exp_chunk_shift);
rsaz_exp_x2.c:                     * when 64-bit boundaries are crossed.
rsaz_exp_x2.c:                    if (exp_chunk_shift > 64 - exp_win_size) {
rsaz_exp_x2.c:                        T <<= (64 - exp_chunk_shift);
rsaz_exp_x2.c:     *     DOI: 10.1007/s13389-012-0031-5
rsaz_exp_x2.c:    for (; in_len > 0; in_len--) {
rsaz_exp_x2.c:        digit += (uint64_t)(in[in_len - 1]);
rsaz_exp_x2.c:    for (; in_bitsize >= (2 * DIGIT_SIZE); in_bitsize -= (2 * DIGIT_SIZE), out += 2) {
rsaz_exp_x2.c:        out_len -= 2;
rsaz_exp_x2.c:        in_bitsize -= DIGIT_SIZE;
rsaz_exp_x2.c:        out_len -= 2;
rsaz_exp_x2.c:        out_len--;
rsaz_exp_x2.c:        out_len--;
rsaz_exp_x2.c:    for (; out_len > 0; out_len--) {
rsaz_exp_x2.c:               out_bitsize -= (2 * DIGIT_SIZE), in += 2) {
rsaz_exp_x2.c:            out_bitsize -= DIGIT_SIZE;
